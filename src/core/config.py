#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Professional Statistics Suite - Hardware & AI Configuration Management
ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«çµ±è¨ˆã‚¹ã‚¤ãƒ¼ãƒˆ - ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ»AIè¨­å®šç®¡ç†

RTX 30/40/50 Series & Apple Silicon M2+ Optimized
SPSS-Grade Performance Enhancement
"""

import os
import platform
import subprocess
import sys
from pathlib import Path
from typing import Optional, Dict, Any, List, Tuple
import json
import yaml
from dotenv import load_dotenv
import threading
import time
from dataclasses import dataclass

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

@dataclass
class PerformanceProfile:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è¨­å®š"""
    name: str
    max_memory_usage: float  # GB
    cpu_threads: int
    gpu_memory_fraction: float
    batch_size_multiplier: float
    optimization_level: str
    parallel_processing: bool
    cache_enabled: bool
    description: str

class SPSSGradeConfig:
    """SPSSç´šè¨­å®šç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«å®šç¾©
        self.performance_profiles = {
            'ultra_high': PerformanceProfile(
                name='Ultra High Performance',
                max_memory_usage=64.0,
                cpu_threads=-1,  # All available
                gpu_memory_fraction=0.9,
                batch_size_multiplier=4.0,
                optimization_level='maximum',
                parallel_processing=True,
                cache_enabled=True,
                description='Maximum performance for large-scale analysis (64GB+ RAM)'
            ),
            'high': PerformanceProfile(
                name='High Performance',
                max_memory_usage=32.0,
                cpu_threads=-1,
                gpu_memory_fraction=0.8,
                batch_size_multiplier=2.0,
                optimization_level='high',
                parallel_processing=True,
                cache_enabled=True,
                description='High performance for medium to large datasets (32GB+ RAM)'
            ),
            'standard': PerformanceProfile(
                name='Standard Performance',
                max_memory_usage=16.0,
                cpu_threads=max(1, (os.cpu_count() or 4) // 2),
                gpu_memory_fraction=0.6,
                batch_size_multiplier=1.0,
                optimization_level='medium',
                parallel_processing=True,
                cache_enabled=True,
                description='Balanced performance for standard workloads (16GB+ RAM)'
            ),
            'conservative': PerformanceProfile(
                name='Conservative',
                max_memory_usage=8.0,
                cpu_threads=max(1, (os.cpu_count() or 4) // 4),
                gpu_memory_fraction=0.4,
                batch_size_multiplier=0.5,
                optimization_level='low',
                parallel_processing=False,
                cache_enabled=False,
                description='Conservative settings for limited resources (8GB+ RAM)'
            )
        }
        
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†è¨­å®š
        self.data_processing_config = {
            'chunk_size': 100000,  # pandas chunk size
            'use_polars': True,    # Use Polars for large datasets
            'use_vaex': True,      # Use Vaex for billion-row datasets
            'streaming_threshold': 1e6,  # Switch to streaming for >1M rows
            'compression': 'snappy',     # Default compression
            'parquet_engine': 'pyarrow', # Parquet engine
            'cache_directory': Path.home() / '.professional_stats_suite' / 'cache'
        }
        
        # çµ±è¨ˆè§£æè¨­å®š
        self.statistical_config = {
            'significance_level': 0.05,
            'confidence_interval': 0.95,
            'bootstrap_samples': 10000,
            'mcmc_samples': 5000,
            'permutation_tests': 10000,
            'cross_validation_folds': 10,
            'random_state': 42,
            'use_gpu_stats': True,  # GPU acceleration for statistics
            'parallel_bootstrap': True,
            'robust_methods': True  # Use robust statistical methods by default
        }
        
        # å¯è¦–åŒ–è¨­å®š
        self.visualization_config = {
            'dpi': 300,
            'figure_size': (12, 8),
            'color_palette': 'viridis',
            'style': 'seaborn-v0_8',
            'interactive': True,
            'save_format': 'png',
            'webgl': True,  # Use WebGL for faster rendering
            'max_points': 100000,  # Maximum points for scatter plots
            'use_datashader': True  # Use Datashader for big data visualization
        }

class HardwareDetector:
    """ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ¤œå‡ºãƒ»æœ€é©åŒ–ã‚¯ãƒ©ã‚¹ï¼ˆM1/M2 Macãƒ»ROCmãƒ»Intel GPUå¯¾å¿œï¼‰"""
    
    def __init__(self):
        self.platform = platform.system()
        self.architecture = platform.machine()
        self.python_version = platform.python_version()
        
        # Hardware information (Multi-platform)
        self.gpu_info = self._detect_gpu()
        self.cpu_info = self._detect_cpu()
        self.memory_info = self._detect_memory()
        
        # Platform-specific optimizations
        self.apple_silicon_info = self._detect_apple_silicon()
        self.amd_gpu_info = self._detect_amd_gpu()
        self.intel_gpu_info = self._detect_intel_gpu()
        
        # Advanced platform support
        self.metal_support = self._detect_metal_support()
        self.rocm_support = self._detect_rocm_support()
        self.mlx_support = self._detect_mlx_support()
        
        # SPSS-grade configuration
        self.spss_config = SPSSGradeConfig()
        
        # Optimization settings
        self.optimal_settings = self._determine_optimal_settings()
        
        # Performance monitoring
        self._start_performance_monitoring()
    
    def _start_performance_monitoring(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–é–‹å§‹"""
        self.monitoring_active = True
        self.performance_history = []
        
        def monitor():
            while self.monitoring_active:
                try:
                    import psutil
                    cpu_percent = psutil.cpu_percent(interval=1)
                    memory = psutil.virtual_memory()
                    
                    performance_data = {
                        'timestamp': time.time(),
                        'cpu_percent': cpu_percent,
                        'memory_percent': memory.percent,
                        'memory_available_gb': memory.available / (1024**3)
                    }
                    
                    # GPU monitoring
                    if self.gpu_info['nvidia']['available']:
                        try:
                            import torch
                            if torch.cuda.is_available():
                                for i in range(torch.cuda.device_count()):
                                    gpu_memory = torch.cuda.get_device_properties(i).total_memory
                                    gpu_allocated = torch.cuda.memory_allocated(i)
                                    performance_data[f'gpu_{i}_utilization'] = (gpu_allocated / gpu_memory) * 100
                        except Exception:
                            pass
                    
                    self.performance_history.append(performance_data)
                    
                    # Keep only last 1000 entries
                    if len(self.performance_history) > 1000:
                        self.performance_history = self.performance_history[-1000:]
                    
                    time.sleep(5)  # Monitor every 5 seconds
                except Exception:
                    time.sleep(5)
        
        self.monitor_thread = threading.Thread(target=monitor, daemon=True)
        self.monitor_thread.start()
    
    def get_optimal_profile(self) -> PerformanceProfile:
        """æœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
        total_memory_gb = self.memory_info.get('total_gb', 8)
        
        if total_memory_gb >= 64:
            return self.spss_config.performance_profiles['ultra_high']
        elif total_memory_gb >= 32:
            return self.spss_config.performance_profiles['high']
        elif total_memory_gb >= 16:
            return self.spss_config.performance_profiles['standard']
        else:
            return self.spss_config.performance_profiles['conservative']
    
    def configure_for_large_dataset(self, dataset_size_rows: int) -> Dict[str, Any]:
        """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨è¨­å®š"""
        config = {}
        
        if dataset_size_rows > 10e6:  # 10M+ rows
            config.update({
                'use_vaex': True,
                'use_polars': True,
                'streaming': True,
                'chunk_size': 500000,
                'compression': 'lz4',
                'parallel_processing': True,
                'memory_mapping': True
            })
        elif dataset_size_rows > 1e6:  # 1M+ rows
            config.update({
                'use_polars': True,
                'streaming': False,
                'chunk_size': 100000,
                'compression': 'snappy',
                'parallel_processing': True,
                'memory_mapping': False
            })
        else:
            config.update({
                'use_pandas': True,
                'streaming': False,
                'chunk_size': 50000,
                'parallel_processing': False,
                'memory_mapping': False
            })
        
        return config

    def _detect_gpu(self) -> Dict[str, Any]:
        """GPUæ¤œå‡ºï¼ˆCUDA/MPS/ROCmå¯¾å¿œï¼‰"""
        gpu_info = {
            'nvidia': {'available': False, 'devices': [], 'cuda_version': None},
            'amd': {'available': False, 'devices': [], 'rocm_version': None},
            'apple': {'available': False, 'devices': [], 'metal_version': None},
            'intel': {'available': False, 'devices': [], 'level_zero_version': None},
            'primary_platform': 'cpu',
            'recommended_backend': 'cpu'
        }
        
        # NVIDIA CUDA Detection
        try:
            import torch
            if torch.cuda.is_available():
                gpu_info['nvidia']['available'] = True
                try:
                    gpu_info['nvidia']['cuda_version'] = torch.version.cuda  # type: ignore
                except AttributeError:
                    gpu_info['nvidia']['cuda_version'] = "Unknown"
                gpu_info['primary_platform'] = 'cuda'
                gpu_info['recommended_backend'] = 'cuda'
                
                for i in range(torch.cuda.device_count()):
                    props = torch.cuda.get_device_properties(i)
                    device_info = {
                        'id': i,
                        'name': props.name,
                        'memory_gb': props.total_memory / (1024**3),
                        'compute_capability': f"{props.major}.{props.minor}",
                        'tensor_cores': self._has_tensor_cores(props.name),
                        'spss_rating': self._get_spss_performance_rating(props.name)
                    }
                    gpu_info['nvidia']['devices'].append(device_info)
        except Exception as e:
            print(f"CUDA detection failed: {e}")
        
        # Apple Metal Performance Shaders (MPS) Detection
        if self.platform == "Darwin":
            try:
                import torch
                if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    gpu_info['apple']['available'] = True
                    gpu_info['apple']['metal_version'] = self._get_metal_version()
                    
                    # Apple Silicon GPU detection
                    apple_gpu_info = self.apple_silicon_info
                    if apple_gpu_info['is_apple_silicon']:
                        gpu_info['apple']['devices'] = [{
                            'id': 0,
                            'name': apple_gpu_info['chip_name'],
                            'gpu_cores': apple_gpu_info['gpu_cores'],
                            'unified_memory': apple_gpu_info['unified_memory'],
                            'spss_rating': self._get_apple_spss_rating(apple_gpu_info['chip_name'])
                        }]
                        
                        if gpu_info['primary_platform'] == 'cpu':
                            gpu_info['primary_platform'] = 'mps'
                            gpu_info['recommended_backend'] = 'mps'
            except Exception as e:
                print(f"Apple MPS detection failed: {e}")
        
        # AMD ROCm Detection
        try:
            import torch
            # ROCmç’°å¢ƒã§ã®æ¤œå‡º
            if hasattr(torch, 'cuda') and torch.cuda.is_available():
                # Check if this is actually ROCm
                device_name = torch.cuda.get_device_name(0)
                if any(amd_identifier in device_name.lower() for amd_identifier in 
                       ['radeon', 'vega', 'navi', 'rdna', 'gfx']):
                    gpu_info['amd']['available'] = True
                    gpu_info['amd']['rocm_version'] = self._get_rocm_version()
                    
                    for i in range(torch.cuda.device_count()):
                        props = torch.cuda.get_device_properties(i)
                        if any(amd_id in props.name.lower() for amd_id in 
                               ['radeon', 'vega', 'navi', 'rdna']):
                            device_info = {
                                'id': i,
                                'name': props.name,
                                'memory_gb': props.total_memory / (1024**3),
                                'architecture': self._get_amd_architecture(props.name),
                                'spss_rating': self._get_amd_spss_rating(props.name)
                            }
                            gpu_info['amd']['devices'].append(device_info)
                    
                    if gpu_info['primary_platform'] == 'cpu':
                        gpu_info['primary_platform'] = 'rocm'
                        gpu_info['recommended_backend'] = 'rocm'
        except Exception as e:
            print(f"AMD ROCm detection failed: {e}")
        
        return gpu_info
    
    def _get_metal_version(self) -> str:
        """Metal Performance Shaders versionå–å¾—"""
        try:
            result = subprocess.run(['system_profiler', 'SPDisplaysDataType'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                # Parse Metal version from system profiler
                output = result.stdout
                if 'Metal Support' in output:
                    return "Available"
            return "Unknown"
        except Exception:
            return "Unknown"
    
    def _get_rocm_version(self) -> str:
        """ROCm versionå–å¾—"""
        try:
            result = subprocess.run(['rocm-smi', '--version'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                return result.stdout.strip()
            return "Unknown"
        except Exception:
            return "Unknown"
    
    def _get_amd_architecture(self, gpu_name: str) -> str:
        """AMD GPU architectureå–å¾—"""
        gpu_name_lower = gpu_name.lower()
        if 'rdna3' in gpu_name_lower or 'rx 7' in gpu_name_lower:
            return 'RDNA3'
        elif 'rdna2' in gpu_name_lower or 'rx 6' in gpu_name_lower:
            return 'RDNA2'
        elif 'rdna' in gpu_name_lower or 'rx 5' in gpu_name_lower:
            return 'RDNA'
        elif 'vega' in gpu_name_lower:
            return 'Vega'
        else:
            return 'Unknown'
    
    def _get_amd_spss_rating(self, gpu_name: str) -> str:
        """AMD GPU SPSSäº’æ›æ€§è©•ä¾¡"""
        gpu_name_lower = gpu_name.lower()
        if any(high_end in gpu_name_lower for high_end in ['rx 7900', 'rx 6900', 'vega 64']):
            return 'Excellent'
        elif any(mid_high in gpu_name_lower for mid_high in ['rx 7800', 'rx 6800', 'rx 6700']):
            return 'Very Good'
        elif any(mid in gpu_name_lower for mid in ['rx 7600', 'rx 6600', 'rx 5700']):
            return 'Good'
        else:
            return 'Basic'
    
    def _get_apple_spss_rating(self, chip_name: str) -> str:
        """Apple Silicon SPSSäº’æ›æ€§è©•ä¾¡"""
        chip_lower = chip_name.lower()
        if 'm2 ultra' in chip_lower or 'm1 ultra' in chip_lower:
            return 'Excellent'
        elif 'm2 max' in chip_lower or 'm1 max' in chip_lower:
            return 'Very Good'
        elif 'm2 pro' in chip_lower or 'm1 pro' in chip_lower:
            return 'Good'
        elif 'm2' in chip_lower or 'm1' in chip_lower:
            return 'Good'
        else:
            return 'Basic'

    def _detect_apple_silicon(self) -> Dict[str, Any]:
        """Apple Siliconæ¤œå‡º"""
        apple_info = {
            'is_apple_silicon': False,
            'chip_name': 'Unknown',
            'gpu_cores': 0,
            'unified_memory': 0,
            'neural_engine': False
        }
        
        if self.platform == "Darwin":
            try:
                # Check if running on Apple Silicon
                result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'], 
                                      capture_output=True, text=True)
                if result.returncode == 0:
                    cpu_brand = result.stdout.strip()
                    if 'Apple' in cpu_brand:
                        apple_info['is_apple_silicon'] = True
                        apple_info['chip_name'] = cpu_brand
                        apple_info['neural_engine'] = True
                        
                        # Get GPU core count (approximate based on known configurations)
                        if 'M1 Ultra' in cpu_brand:
                            apple_info['gpu_cores'] = 64
                        elif 'M1 Max' in cpu_brand:
                            apple_info['gpu_cores'] = 32
                        elif 'M1 Pro' in cpu_brand:
                            apple_info['gpu_cores'] = 16
                        elif 'M2 Ultra' in cpu_brand:
                            apple_info['gpu_cores'] = 76
                        elif 'M2 Max' in cpu_brand:
                            apple_info['gpu_cores'] = 38
                        elif 'M2 Pro' in cpu_brand:
                            apple_info['gpu_cores'] = 19
                        elif 'M2' in cpu_brand:
                            apple_info['gpu_cores'] = 10
                        elif 'M1' in cpu_brand:
                            apple_info['gpu_cores'] = 8
                        
                        # Get unified memory
                        memory_result = subprocess.run(['sysctl', '-n', 'hw.memsize'], 
                                                     capture_output=True, text=True)
                        if memory_result.returncode == 0:
                            apple_info['unified_memory'] = int(memory_result.stdout.strip()) // (1024**3)
            except Exception as e:
                print(f"Apple Silicon detection failed: {e}")
        
        return apple_info
    
    def _detect_amd_gpu(self) -> Dict[str, Any]:
        """AMD GPUè©³ç´°æ¤œå‡º"""
        amd_info = {
            'available': False,
            'rocm_installed': False,
            'devices': [],
            'opencl_support': False
        }
        
        try:
            # Check for ROCm installation
            rocm_result = subprocess.run(['which', 'rocm-smi'], 
                                       capture_output=True, text=True)
            if rocm_result.returncode == 0:
                amd_info['rocm_installed'] = True
                
                # Get device information
                smi_result = subprocess.run(['rocm-smi', '--showid'], 
                                          capture_output=True, text=True)
                if smi_result.returncode == 0:
                    amd_info['available'] = True
                    # Parse device information from rocm-smi output
                    # This is a simplified parsing - could be enhanced
                    amd_info['devices'] = ['AMD GPU Device']
        except Exception as e:
            print(f"AMD GPU detection failed: {e}")
        
        return amd_info
    
    def _detect_intel_gpu(self) -> Dict[str, Any]:
        """Intel GPUæ¤œå‡ºï¼ˆFuture supportï¼‰"""
        intel_info = {
            'available': False,
            'level_zero_installed': False,
            'devices': []
        }
        
        # Future implementation for Intel GPU support
        return intel_info
    
    def _detect_cpu(self) -> Dict[str, Any]:
        """CPUè©³ç´°æ¤œå‡º"""
        cpu_info = {
            'cores': os.cpu_count(),
            'architecture': self.architecture,
            'spss_performance_rating': 'Basic'
        }
        
        try:
            import cpuinfo  # type: ignore
            cpu_data = cpuinfo.get_cpu_info()
            cpu_info.update({
                'brand': cpu_data.get('brand_raw', 'Unknown'),
                'frequency': cpu_data.get('hz_actual_friendly', 'Unknown'),
                'features': cpu_data.get('flags', [])
            })
        except ImportError:
            # cpuinfo not available, use basic CPU info
            cpu_info['brand'] = platform.processor() or 'Unknown'
            cpu_info['frequency'] = 'Unknown'
            cpu_info['features'] = []
        
        # CPUæ€§èƒ½è©•ä¾¡
        cores = cpu_info['cores'] or 1
        if cores >= 16:
            cpu_info['spss_performance_rating'] = 'Superior to SPSS'
        elif cores >= 8:
            cpu_info['spss_performance_rating'] = 'SPSS-Grade'
        elif cores >= 4:
            cpu_info['spss_performance_rating'] = 'SPSS-Compatible'
        
        return cpu_info
    
    def _detect_memory(self) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªè©³ç´°æ¤œå‡º"""
        memory_info = {
            'total_gb': 8,
            'spss_performance_rating': 'Basic'
        }
        
        try:
            import psutil
            memory = psutil.virtual_memory()
            total_gb = memory.total / (1024**3)
            memory_info.update({
                'total_gb': total_gb,
                'available_gb': memory.available / (1024**3),
                'percent_used': memory.percent
            })
            
            # ãƒ¡ãƒ¢ãƒªæ€§èƒ½è©•ä¾¡
            if total_gb >= 64:
                memory_info['spss_performance_rating'] = 'Superior to SPSS'
            elif total_gb >= 32:
                memory_info['spss_performance_rating'] = 'SPSS-Grade'
            elif total_gb >= 16:
                memory_info['spss_performance_rating'] = 'SPSS-Compatible'
                
        except ImportError:
            pass
        
        return memory_info
    
    def _detect_metal_support(self) -> Dict[str, Any]:
        """Metal Performance Shadersã‚µãƒãƒ¼ãƒˆæ¤œå‡º"""
        metal_info = {
            'available': False,
            'version': None,
            'devices': [],
            'unified_memory': False
        }
        
        if self.platform == 'Darwin':
            try:
                # Check for Metal support
                import subprocess
                result = subprocess.run(['system_profiler', 'SPDisplaysDataType'], 
                                      capture_output=True, text=True)
                if result.returncode == 0 and 'Metal' in result.stdout:
                    metal_info['available'] = True
                    
                    # Apple Siliconç‰¹æœ‰ã®æ©Ÿèƒ½
                    if self.architecture == 'arm64':
                        metal_info['unified_memory'] = True
                        metal_info['neural_engine'] = True
                        
                        # MLXãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å¯¾å¿œç¢ºèª
                        try:
                            import mlx.core as mx  # type: ignore
                            metal_info['mlx_available'] = True
                        except ImportError:
                            metal_info['mlx_available'] = False
                            
            except Exception as e:
                print(f"Metal detection failed: {e}")
        
        return metal_info
    
    def _detect_rocm_support(self) -> Dict[str, Any]:
        """ROCmã‚µãƒãƒ¼ãƒˆæ¤œå‡º"""
        rocm_info = {
            'available': False,
            'version': None,
            'devices': [],
            'pytorch_rocm': False
        }
        
        if self.platform == 'Linux':
            try:
                # ROCmã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
                rocm_result = subprocess.run(['which', 'rocm-smi'], 
                                           capture_output=True, text=True)
                if rocm_result.returncode == 0:
                    rocm_info['available'] = True
                    
                    # ROCmãƒãƒ¼ã‚¸ãƒ§ãƒ³å–å¾—
                    version_result = subprocess.run(['rocm-smi', '--version'], 
                                                  capture_output=True, text=True)
                    if version_result.returncode == 0:
                        rocm_info['version'] = version_result.stdout.strip()
                    
                    # AMD GPU ãƒ‡ãƒã‚¤ã‚¹æ¤œå‡º
                    device_result = subprocess.run(['rocm-smi', '--showid'], 
                                                 capture_output=True, text=True)
                    if device_result.returncode == 0:
                        # Parse device information
                        lines = device_result.stdout.strip().split('\n')
                        devices = []
                        for line in lines:
                            if 'GPU' in line:
                                devices.append({'name': line.strip(), 'spss_rating': 'SPSS-Compatible'})
                        rocm_info['devices'] = devices
                    
                    # PyTorch ROCmå¯¾å¿œç¢ºèª
                    try:
                        import torch
                        if hasattr(torch, 'hip') and torch.hip.is_available():  # type: ignore
                            rocm_info['pytorch_rocm'] = True
                    except (ImportError, AttributeError):
                        pass
                        
            except Exception as e:
                print(f"ROCm detection failed: {e}")
        
        return rocm_info
    
    def _detect_mlx_support(self) -> Dict[str, Any]:
        """MLXãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å¯¾å¿œæ¤œå‡ºï¼ˆApple Siliconå°‚ç”¨ï¼‰"""
        mlx_info = {
            'available': False,
            'version': None,
            'neural_engine': False,
            'unified_memory': False
        }
        
        if self.platform == 'Darwin' and self.architecture == 'arm64':
            try:
                import mlx.core as mx  # type: ignore
                mlx_info['available'] = True
                mlx_info['version'] = mx.__version__
                mlx_info['neural_engine'] = True
                mlx_info['unified_memory'] = True
                
                # MLXæœ€é©åŒ–è¨­å®š
                mlx_info['optimization'] = {
                    'gpu_acceleration': True,
                    'neural_engine': True,
                    'mixed_precision': True,
                    'memory_efficient': True
                }
                
            except ImportError:
                pass
        
        return mlx_info
    
    def _has_tensor_cores(self, gpu_name: str) -> bool:
        """Tensor Coreså¯¾å¿œç¢ºèª"""
        tensor_core_gpus = ['RTX 20', 'RTX 30', 'RTX 40', 'RTX 50', 'A100', 'V100', 'T4']
        return any(gpu in gpu_name for gpu in tensor_core_gpus)
    
    def _get_spss_performance_rating(self, gpu_name: str) -> str:
        """SPSSæ€§èƒ½ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"""
        if any(series in gpu_name for series in ['RTX 50', 'RTX 51', 'A100', 'H100']):
            return "Superior to SPSS"  # SPSSä»¥ä¸Š
        elif any(series in gpu_name for series in ['RTX 40', 'RTX 41', 'RTX 30', 'RTX 31']):
            return "SPSS-Grade"       # SPSSç´š
        elif any(series in gpu_name for series in ['RTX 20', 'GTX 16']):
            return "SPSS-Compatible"  # SPSSäº’æ›
        else:
            return "Basic"            # åŸºæœ¬ãƒ¬ãƒ™ãƒ«
    
    def _determine_optimal_settings(self) -> Dict[str, Any]:
        """æœ€é©è¨­å®šæ±ºå®š"""
        profile = self.get_optimal_profile()
        
        settings = {
            'framework': 'auto',
            'device': 'auto',
            'precision': 'float32',
            'batch_size': 'auto',
            'num_workers': profile.cpu_threads,
            'memory_fraction': profile.gpu_memory_fraction,
            'optimization_level': profile.optimization_level,
            'performance_profile': profile.name,
            'spss_grade_features': True,
            'large_dataset_optimization': True,
            'enterprise_features': True
        }
        
        # GPU based optimization
        if self.gpu_info['nvidia']['available']:
            nvidia_device = self.gpu_info['nvidia']['devices'][0]
            optimization_level = nvidia_device.get('optimization_level', 'medium')
            if optimization_level == 'maximum':
                settings.update({
                    'precision': 'mixed',  # Mixed precision for RTX 50
                    'tensor_cores': True,
                    'gpu_acceleration': 'maximum'
                })
            elif optimization_level == 'high':
                settings.update({
                    'precision': 'float16',  # Half precision for RTX 30/40
                    'tensor_cores': nvidia_device.get('tensor_cores', False),
                    'gpu_acceleration': 'high'
                })
        
        # Apple Silicon optimization
        elif self.gpu_info['apple']['available']:
            apple_device = self.gpu_info['apple']['devices'][0]
            optimization_level = apple_device.get('optimization_level', 'medium')
            if optimization_level == 'maximum':
                settings.update({
                    'metal_acceleration': True,
                    'neural_engine': True,
                    'unified_memory': True,
                    'mlx_optimization': apple_device.get('mlx_compatible', False)
                })
        
        # ROCm (AMD GPU) optimization
        if self.rocm_support['available']:
            settings.update({
                'rocm_acceleration': True,
                'amd_gpu': True,
                'hip_support': self.rocm_support.get('pytorch_rocm', False),
                'amd_optimization_level': 'high'
            })
        
        # MLX framework optimization (Apple Siliconå°‚ç”¨)
        if self.mlx_support['available']:
            settings.update({
                'mlx_framework': True,
                'apple_neural_engine': True,
                'unified_memory_optimization': True,
                'mixed_precision_mlx': True
            })
        
        # Metal Performance Shaders optimization
        if self.metal_support['available']:
            settings.update({
                'metal_performance_shaders': True,
                'gpu_memory_unified': self.metal_support.get('unified_memory', False),
                'metal_optimization': 'maximum'
            })
        
        return settings

    def get_performance_summary(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¦‚è¦å–å¾—"""
        if not self.performance_history:
            return {}
        
        recent_data = self.performance_history[-60:]  # Last 5 minutes
        
        cpu_avg = sum(d['cpu_percent'] for d in recent_data) / len(recent_data)
        memory_avg = sum(d['memory_percent'] for d in recent_data) / len(recent_data)
        
        summary = {
            'cpu_utilization_avg': cpu_avg,
            'memory_utilization_avg': memory_avg,
            'performance_rating': self._calculate_performance_rating(),
            'recommendations': self.get_optimization_recommendations()
        }
        
        return summary
    
    def _calculate_performance_rating(self) -> str:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°è¨ˆç®—"""
        gpu_rating = "Basic"
        cpu_rating = self.cpu_info.get('spss_performance_rating', 'Basic')
        memory_rating = self.memory_info.get('spss_performance_rating', 'Basic')
        
        if self.gpu_info['nvidia']['available']:
            gpu_rating = self.gpu_info['nvidia']['devices'][0].get('spss_performance_rating', 'Basic')
        elif self.gpu_info['apple']['available']:
            gpu_rating = self.gpu_info['apple']['devices'][0].get('spss_performance_rating', 'Basic')
        
        ratings = [gpu_rating, cpu_rating, memory_rating]
        
        if all(r == "Superior to SPSS" for r in ratings):
            return "Superior to SPSS"
        elif any(r == "Superior to SPSS" for r in ratings) and all(r in ["Superior to SPSS", "SPSS-Grade"] for r in ratings):
            return "SPSS-Grade Plus"
        elif all(r in ["SPSS-Grade", "Superior to SPSS"] for r in ratings):
            return "SPSS-Grade"
        elif all(r in ["SPSS-Compatible", "SPSS-Grade", "Superior to SPSS"] for r in ratings):
            return "SPSS-Compatible"
        else:
            return "Basic"

    def get_optimization_recommendations(self) -> List[str]:
        """æœ€é©åŒ–æ¨å¥¨äº‹é …"""
        recommendations = []
        
        memory_gb = self.memory_info.get('total_gb', 0)
        if memory_gb < 16:
            recommendations.append("ğŸ’¾ ãƒ¡ãƒ¢ãƒªã‚’16GBä»¥ä¸Šã«å¢—è¨­ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ï¼ˆSPSSç´šæ€§èƒ½ã«ã¯32GBä»¥ä¸ŠãŒç†æƒ³ï¼‰")
        elif memory_gb < 32:
            recommendations.append("ğŸš€ ãƒ¡ãƒ¢ãƒªã‚’32GBä»¥ä¸Šã«å¢—è¨­ã™ã‚‹ã¨SPSSç´šæ€§èƒ½ãŒå®Ÿç¾ã§ãã¾ã™")
        
        if not self.gpu_info['nvidia']['available'] and not self.gpu_info['apple']['available']:
            recommendations.append("âš¡ GPUï¼ˆRTX 30/40/50ã‚·ãƒªãƒ¼ã‚ºã¾ãŸã¯Apple Silicon M2+ï¼‰ã®å°å…¥ã§å¤§å¹…ãªæ€§èƒ½å‘ä¸ŠãŒæœŸå¾…ã§ãã¾ã™")
        
        if self.gpu_info['nvidia']['available']:
            device = self.gpu_info['nvidia']['devices'][0]
            if device['optimization_level'] in ['standard', 'medium']:
                recommendations.append("ğŸ¯ æœ€æ–°ã®RTX 40/50ã‚·ãƒªãƒ¼ã‚ºã¸ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã§SPSSä»¥ä¸Šã®æ€§èƒ½ãŒå®Ÿç¾ã§ãã¾ã™")
        
        profile = self.get_optimal_profile()
        if profile.name == 'Conservative':
            recommendations.append("ğŸ“ˆ ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®å¢—å¼·ã«ã‚ˆã‚Šã€ã‚ˆã‚Šé«˜æ€§èƒ½ãªè§£æãŒå¯èƒ½ã«ãªã‚Šã¾ã™")
        
        recommendations.append("âœ¨ ç¾åœ¨ã®è¨­å®šã¯è‡ªå‹•æœ€é©åŒ–ã•ã‚Œã¦ãŠã‚Šã€SPSSãƒ¬ãƒ™ãƒ«ã®çµ±è¨ˆè§£ææ€§èƒ½ã‚’æä¾›ã—ã¾ã™")
        
        return recommendations

class AIConfig:
    """AIçµ±åˆè¨­å®šã‚¯ãƒ©ã‚¹ - Enhanced for SPSS-grade performance"""
    
    def __init__(self):
        self.hardware = HardwareDetector()
        self.config_file = Path.home() / '.professional_stats_suite' / 'ai_config.yaml'
        self.config_file.parent.mkdir(parents=True, exist_ok=True)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.google_api_key = os.getenv('GOOGLE_API_KEY')
        self.anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')
        
        # SPSS-grade AI settings
        self.ai_features = {
            'natural_language_queries': True,
            'automated_analysis': True,
            'intelligent_visualization': True,
            'statistical_interpretation': True,
            'report_generation': True,
            'code_generation': True,
            'data_quality_assessment': True,
            'advanced_modeling': True
        }
        
        # Model preferences
        self.model_preferences = {
            'primary_llm': 'gpt-4-turbo',
            'fallback_llm': 'claude-3-sonnet',
            'local_llm': None,  # For offline analysis
            'statistical_model': 'ensemble',  # Use ensemble methods
            'vision_model': 'gpt-4-vision-preview'
        }
        
        # Performance settings
        self.performance_settings = {
            'max_concurrent_requests': 5,
            'request_timeout': 300,
            'retry_attempts': 3,
            'cache_responses': True,
            'batch_processing': True
        }
        
        self._load_config()
    
    def _load_config(self):
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
        if self.config_file.exists():
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                    self._update_from_config(config)
            except Exception as e:
                print(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _update_from_config(self, config: Dict[str, Any]):
        """è¨­å®šæ›´æ–°"""
        for key, value in config.items():
            if hasattr(self, key):
                setattr(self, key, value)
    
    def save_config(self):
        """è¨­å®šä¿å­˜"""
        config = {
            'ai_features': self.ai_features,
            'model_preferences': self.model_preferences,
            'performance_settings': self.performance_settings
        }
        
        try:
            with open(self.config_file, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, 
                         allow_unicode=True, sort_keys=False)
        except Exception as e:
            print(f" ç·Šæ€¥ä¿å­˜å¤±æ•—: {e}")
    
    def get_spss_grade_features(self) -> Dict[str, bool]:
        """SPSSç´šæ©Ÿèƒ½ä¸€è¦§"""
        spss_features = {
            'descriptive_statistics': True,
            'hypothesis_testing': True,
            'regression_analysis': True,
            'anova': True,
            'chi_square_tests': True,
            'survival_analysis': True,
            'time_series_analysis': True,
            'multivariate_analysis': True,
            'bayesian_statistics': True,
            'machine_learning': True,
            'deep_learning': True,
            'big_data_processing': True,
            'gpu_acceleration': self.hardware.gpu_info['nvidia']['available'] or self.hardware.gpu_info['apple']['available'],
            'parallel_computing': True,
            'automated_reporting': True,
            'interactive_visualization': True,
            'data_mining': True,
            'predictive_analytics': True,
            'statistical_modeling': True,
            'advanced_graphics': True
        }
        return spss_features
    
    def get_hardware_status(self) -> Dict[str, Any]:
        """ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢çŠ¶æ…‹å–å¾—"""
        return {
            'gpu_info': self.hardware.gpu_info,
            'cpu_info': self.hardware.cpu_info,
            'memory_info': self.hardware.memory_info,
            'optimal_settings': self.hardware.optimal_settings,
            'performance_profile': self.hardware.get_optimal_profile(),
            'spss_compatibility': self.hardware._calculate_performance_rating()
        }
    
    def is_api_configured(self, provider: str) -> bool:
        """APIè¨­å®šç¢ºèª"""
        api_keys = {
            'openai': self.openai_api_key,
            'google': self.google_api_key,
            'anthropic': self.anthropic_api_key
        }
        return bool(api_keys.get(provider))
    
    def get_available_providers(self) -> list:
        """åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ä¸€è¦§"""
        providers = []
        if self.openai_api_key:
            providers.append('openai')
        if self.google_api_key:
            providers.append('google')
        if self.anthropic_api_key:
            providers.append('anthropic')
        return providers
    
    def get_enterprise_config(self) -> Dict[str, Any]:
        """ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºè¨­å®š"""
        return {
            'data_security': {
                'encryption_at_rest': True,
                'encryption_in_transit': True,
                'secure_api_calls': True,
                'audit_logging': True
            },
            'scalability': {
                'distributed_computing': True,
                'cloud_integration': True,
                'cluster_support': True,
                'load_balancing': True
            },
            'compliance': {
                'gdpr_compliant': True,
                'hipaa_ready': True,
                'sox_compatible': True,
                'data_governance': True
            },
            'performance': {
                'spss_grade': True,
                'real_time_analysis': True,
                'streaming_data': True,
                'big_data_support': True
            }
        }

# Global configuration instances
hardware_detector = HardwareDetector()
ai_config = AIConfig()
spss_config = SPSSGradeConfig()

def get_hardware_summary() -> str:
    """ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ¦‚è¦å–å¾—"""
    summary = []
    
    # Overall rating
    rating = hardware_detector._calculate_performance_rating()
    summary.append(f"ğŸ¯ ç·åˆæ€§èƒ½ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°: {rating}")
    
    # GPU info
    if hardware_detector.gpu_info['nvidia']['available']:
        device = hardware_detector.gpu_info['nvidia']['devices'][0]
        summary.append(f"ğŸš€ GPU: {device['name']} ({device['spss_rating']})")
    elif hardware_detector.gpu_info['apple']['available']:
        device = hardware_detector.gpu_info['apple']['devices'][0]
        summary.append(f"ğŸš€ Apple Silicon: {device['name']} ({device['spss_rating']})")
    else:
        summary.append("âš¡ GPU: æœªæ¤œå‡º (RTX 30/40/50ã¾ãŸã¯Apple Siliconæ¨å¥¨)")
    
    # Memory info
    memory_gb = hardware_detector.memory_info.get('total_gb', 0)
    memory_rating = hardware_detector.memory_info.get('spss_performance_rating', 'Basic')
    summary.append(f"ğŸ’¾ ãƒ¡ãƒ¢ãƒª: {memory_gb:.1f}GB ({memory_rating})")
    
    # CPU info
    cpu_cores = hardware_detector.cpu_info.get('physical_cores', hardware_detector.cpu_info.get('cores', 0))
    cpu_rating = hardware_detector.cpu_info.get('spss_performance_rating', 'Basic')
    summary.append(f"âš™ï¸ CPU: {cpu_cores}ã‚³ã‚¢ ({cpu_rating})")
    
    # Performance profile
    profile = hardware_detector.get_optimal_profile()
    summary.append(f"ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«: {profile.name}")
    
    return "\n".join(summary)

def initialize_spss_grade_environment():
    """SPSSç´šç’°å¢ƒåˆæœŸåŒ–"""
    print("ğŸš€ Professional Statistics Suite - SPSSç´šç’°å¢ƒã‚’åˆæœŸåŒ–ä¸­...")
    
    # Create necessary directories
    dirs_to_create = [
        Path.home() / '.professional_stats_suite',
        Path.home() / '.professional_stats_suite' / 'cache',
        Path.home() / '.professional_stats_suite' / 'temp',
        Path.home() / '.professional_stats_suite' / 'models',
        Path.home() / '.professional_stats_suite' / 'reports'
    ]
    
    for dir_path in dirs_to_create:
        dir_path.mkdir(parents=True, exist_ok=True)
    
    # Initialize hardware optimization
    hardware_detector._start_performance_monitoring()
    
    # Save configurations
    ai_config.save_config()
    
    print(" SPSSç´šç’°å¢ƒã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print(get_hardware_summary())

if __name__ == "__main__":
    initialize_spss_grade_environment() 