#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Test Performance Optimizer
ãƒ†ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ 

Author: Ryo Minegishi
Email: r.minegishi1987@gmail.com
License: MIT
"""

import time
import psutil
import gc
import threading
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from pathlib import Path
import json
import logging
from datetime import datetime
import asyncio
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing

@dataclass
class TestPerformanceMetrics:
    """ãƒ†ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    test_name: str
    execution_time: float
    memory_usage_mb: float
    cpu_usage_percent: float
    gc_collections: int
    gc_time: float
    timestamp: datetime = field(default_factory=datetime.now)
    
    def to_dict(self) -> Dict:
        return {
            "test_name": self.test_name,
            "execution_time": self.execution_time,
            "memory_usage_mb": self.memory_usage_mb,
            "cpu_usage_percent": self.cpu_usage_percent,
            "gc_collections": self.gc_collections,
            "gc_time": self.gc_time,
            "timestamp": self.timestamp.isoformat()
        }

class PerformanceProfiler:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼"""
    
    def __init__(self):
        self.metrics: List[TestPerformanceMetrics] = []
        self.process = psutil.Process()
        self.logger = logging.getLogger(__name__)
        
    def profile_test(self, test_func: Callable, test_name: str) -> TestPerformanceMetrics:
        """ãƒ†ã‚¹ãƒˆé–¢æ•°ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°"""
        # GCçµ±è¨ˆã‚’ãƒªã‚»ãƒƒãƒˆ
        gc.collect()
        gc_stats_before = gc.get_stats()
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨˜éŒ²
        memory_before = self.process.memory_info().rss / 1024 / 1024
        
        # CPUä½¿ç”¨é‡ã‚’è¨˜éŒ²
        cpu_before = self.process.cpu_percent()
        
        # å®Ÿè¡Œæ™‚é–“æ¸¬å®š
        start_time = time.time()
        
        try:
            # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            result = test_func()
            execution_time = time.time() - start_time
        except Exception as e:
            execution_time = time.time() - start_time
            self.logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            result = None
        
        # å®Ÿè¡Œå¾Œã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨˜éŒ²
        memory_after = self.process.memory_info().rss / 1024 / 1024
        cpu_after = self.process.cpu_percent()
        
        # GCçµ±è¨ˆã‚’å–å¾—
        gc_stats_after = gc.get_stats()
        gc_collections = sum(stats['collections'] for stats in gc_stats_after) - sum(stats['collections'] for stats in gc_stats_before)
        gc_time = sum(stats['collections_time'] for stats in gc_stats_after) - sum(stats['collections_time'] for stats in gc_stats_before)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä½œæˆ
        metrics = TestPerformanceMetrics(
            test_name=test_name,
            execution_time=execution_time,
            memory_usage_mb=memory_after - memory_before,
            cpu_usage_percent=(cpu_before + cpu_after) / 2,
            gc_collections=gc_collections,
            gc_time=gc_time
        )
        
        self.metrics.append(metrics)
        return metrics
    
    def get_slow_tests(self, threshold_seconds: float = 1.0) -> List[TestPerformanceMetrics]:
        """é…ã„ãƒ†ã‚¹ãƒˆã‚’ç‰¹å®š"""
        return [m for m in self.metrics if m.execution_time > threshold_seconds]
    
    def get_memory_intensive_tests(self, threshold_mb: float = 100.0) -> List[TestPerformanceMetrics]:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã„ãƒ†ã‚¹ãƒˆã‚’ç‰¹å®š"""
        return [m for m in self.metrics if m.memory_usage_mb > threshold_mb]
    
    def generate_performance_report(self) -> Dict:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if not self.metrics:
            return {"error": "ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“"}
        
        total_tests = len(self.metrics)
        total_time = sum(m.execution_time for m in self.metrics)
        total_memory = sum(m.memory_usage_mb for m in self.metrics)
        
        slow_tests = self.get_slow_tests()
        memory_intensive_tests = self.get_memory_intensive_tests()
        
        return {
            "summary": {
                "total_tests": total_tests,
                "total_execution_time": total_time,
                "average_execution_time": total_time / total_tests,
                "total_memory_usage_mb": total_memory,
                "average_memory_usage_mb": total_memory / total_tests
            },
            "slow_tests": [m.to_dict() for m in slow_tests],
            "memory_intensive_tests": [m.to_dict() for m in memory_intensive_tests],
            "all_metrics": [m.to_dict() for m in self.metrics]
        }

class TestOptimizer:
    """ãƒ†ã‚¹ãƒˆæœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.profiler = PerformanceProfiler()
        self.logger = logging.getLogger(__name__)
        self.optimization_suggestions: List[str] = []
        
    def optimize_test_collection(self, test_paths: List[str]) -> Dict:
        """ãƒ†ã‚¹ãƒˆåé›†ã®æœ€é©åŒ–"""
        suggestions = []
        
        # ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹ã®æœ€é©åŒ–
        if len(test_paths) > 10:
            suggestions.append("ãƒ†ã‚¹ãƒˆãƒ‘ã‚¹ãŒå¤šã™ãã¾ã™ã€‚testpathsè¨­å®šã§ç‰¹å®šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é™å®šã—ã¦ãã ã•ã„")
        
        # ä¸è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã®é™¤å¤–
        suggestions.append("__pycache__ã€.pycãƒ•ã‚¡ã‚¤ãƒ«ã‚’é™¤å¤–ã—ã¦ãã ã•ã„")
        suggestions.append("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã¯åˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã—ã¦ãã ã•ã„")
        
        return {
            "suggestions": suggestions,
            "optimized_paths": test_paths
        }
    
    def optimize_fixtures(self, fixture_usage: Dict) -> Dict:
        """ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ã®æœ€é©åŒ–"""
        suggestions = []
        
        for fixture_name, usage in fixture_usage.items():
            if usage.get("scope") == "function" and usage.get("setup_time", 0) > 1.0:
                suggestions.append(f"ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ '{fixture_name}' ã®ã‚¹ã‚³ãƒ¼ãƒ—ã‚’ 'module' ã¾ãŸã¯ 'session' ã«å¤‰æ›´ã—ã¦ãã ã•ã„")
            
            if usage.get("memory_usage", 0) > 50:
                suggestions.append(f"ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ '{fixture_name}' ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒé«˜ã„ã§ã™ã€‚è»½é‡åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        
        return {
            "suggestions": suggestions,
            "optimized_fixtures": fixture_usage
        }
    
    def optimize_parallel_execution(self, test_count: int, cpu_count: int = None) -> Dict:
        """ä¸¦åˆ—å®Ÿè¡Œã®æœ€é©åŒ–"""
        if cpu_count is None:
            cpu_count = multiprocessing.cpu_count()
        
        optimal_workers = min(cpu_count, test_count)
        
        return {
            "optimal_workers": optimal_workers,
            "cpu_count": cpu_count,
            "test_count": test_count,
            "estimated_improvement": f"{(test_count / optimal_workers) / test_count * 100:.1f}% ã®å®Ÿè¡Œæ™‚é–“çŸ­ç¸®ãŒæœŸå¾…ã§ãã¾ã™"
        }
    
    def generate_optimization_report(self) -> Dict:
        """æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        performance_report = self.profiler.generate_performance_report()
        
        return {
            "performance_metrics": performance_report,
            "optimization_suggestions": self.optimization_suggestions,
            "recommendations": {
                "parallel_execution": self.optimize_parallel_execution(len(self.profiler.metrics)),
                "memory_optimization": len(self.profiler.get_memory_intensive_tests()) > 0,
                "slow_test_optimization": len(self.profiler.get_slow_tests()) > 0
            }
        }

class CachingManager:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, cache_dir: str = "test_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.logger = logging.getLogger(__name__)
        
    def get_cache_key(self, test_name: str, test_data: Dict) -> str:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆ"""
        import hashlib
        data_str = json.dumps(test_data, sort_keys=True)
        return hashlib.md5(f"{test_name}_{data_str}".encode()).hexdigest()
    
    def get_cached_result(self, cache_key: str) -> Optional[Dict]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸçµæœã‚’å–å¾—"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                self.logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None
    
    def cache_result(self, cache_key: str, result: Dict):
        """çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        cache_file = self.cache_dir / f"{cache_key}.json"
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def clear_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        for cache_file in self.cache_dir.glob("*.json"):
            cache_file.unlink()
        self.logger.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸ")

class TestParallelExecutor:
    """ãƒ†ã‚¹ãƒˆä¸¦åˆ—å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or min(multiprocessing.cpu_count(), 8)
        self.logger = logging.getLogger(__name__)
        
    async def execute_tests_parallel(self, test_functions: List[Callable], test_names: List[str]) -> List[Dict]:
        """ãƒ†ã‚¹ãƒˆã‚’ä¸¦åˆ—å®Ÿè¡Œ"""
        results = []
        
        # ThreadPoolExecutorã‚’ä½¿ç”¨ã—ã¦ä¸¦åˆ—å®Ÿè¡Œ
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # ãƒ†ã‚¹ãƒˆé–¢æ•°ã‚’ä¸¦åˆ—å®Ÿè¡Œ
            future_to_test = {
                executor.submit(self._execute_single_test, func, name): name 
                for func, name in zip(test_functions, test_names)
            }
            
            for future in future_to_test:
                test_name = future_to_test[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    self.logger.error(f"ãƒ†ã‚¹ãƒˆ '{test_name}' å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
                    results.append({"test_name": test_name, "error": str(e)})
        
        return results
    
    def _execute_single_test(self, test_func: Callable, test_name: str) -> Dict:
        """å˜ä¸€ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        start_time = time.time()
        
        try:
            result = test_func()
            execution_time = time.time() - start_time
            
            return {
                "test_name": test_name,
                "success": True,
                "execution_time": execution_time,
                "result": result
            }
        except Exception as e:
            execution_time = time.time() - start_time
            
            return {
                "test_name": test_name,
                "success": False,
                "execution_time": execution_time,
                "error": str(e)
            }

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ ãƒ†ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•")
    
    # æœ€é©åŒ–ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    optimizer = TestOptimizer()
    profiler = PerformanceProfiler()
    cache_manager = CachingManager()
    parallel_executor = TestParallelExecutor()
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚¹ãƒˆé–¢æ•°
    def sample_test_1():
        time.sleep(0.5)
        return {"result": "test1"}
    
    def sample_test_2():
        time.sleep(1.0)
        return {"result": "test2"}
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
    print("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
    profiler.profile_test(sample_test_1, "sample_test_1")
    profiler.profile_test(sample_test_2, "sample_test_2")
    
    # æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print("ğŸ“ˆ æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
    optimization_report = optimizer.generate_optimization_report()
    
    # çµæœè¡¨ç¤º
    print("\n" + "="*50)
    print("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ")
    print("="*50)
    
    summary = optimization_report["performance_metrics"]["summary"]
    print(f"âœ… ç·ãƒ†ã‚¹ãƒˆæ•°: {summary['total_tests']}")
    print(f"â±ï¸ ç·å®Ÿè¡Œæ™‚é–“: {summary['total_execution_time']:.2f}ç§’")
    print(f"ğŸ“Š å¹³å‡å®Ÿè¡Œæ™‚é–“: {summary['average_execution_time']:.2f}ç§’")
    print(f"ğŸ’¾ å¹³å‡ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {summary['average_memory_usage_mb']:.1f}MB")
    
    # é…ã„ãƒ†ã‚¹ãƒˆã®è¡¨ç¤º
    slow_tests = optimization_report["performance_metrics"]["slow_tests"]
    if slow_tests:
        print(f"\nğŸŒ é…ã„ãƒ†ã‚¹ãƒˆ ({len(slow_tests)}ä»¶):")
        for test in slow_tests:
            print(f"  - {test['test_name']}: {test['execution_time']:.2f}ç§’")
    
    # æ¨å¥¨äº‹é …ã®è¡¨ç¤º
    recommendations = optimization_report["recommendations"]
    if recommendations["parallel_execution"]["optimal_workers"] > 1:
        print(f"\nâš¡ ä¸¦åˆ—å®Ÿè¡Œæ¨å¥¨: {recommendations['parallel_execution']['optimal_workers']}ãƒ¯ãƒ¼ã‚«ãƒ¼")
        print(f"   æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„: {recommendations['parallel_execution']['estimated_improvement']}")

if __name__ == "__main__":
    main() 