#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
CLI Test Runner
çµ±åˆCLIãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ„ãƒ¼ãƒ«

Author: Ryo Minegishi
Email: r.minegishi1987@gmail.com
License: MIT
"""

import argparse
import sys
import os
from pathlib import Path
from typing import Dict, List, Any, Optional
import logging
import json
from datetime import datetime
import asyncio
import subprocess

# ãƒ†ã‚¹ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src.tests.e2e_test_automation import E2ETestAutomation
from src.tests.gui_button_test_automation import GUIButtonTestAutomation
from src.tests.production_environment_test import ProductionEnvironmentTest
from src.tests.integrated_test_runner import IntegratedTestRunner
from src.tests.performance_optimizer import TestOptimizer, PerformanceProfiler
from src.tests.parallel_test_runner import ParallelTestRunner, PytestXdistRunner
from src.tests.coverage_analyzer import CoverageAnalyzer, TestCoverageGenerator
from src.tests.html_report_generator import ReportManager, TestResult, CoverageData, PerformanceMetrics
from src.tests.test_data_manager import TestDataManager, TestDataFactory, DataGenerationConfig

class CLITestRunner:
    """çµ±åˆCLIãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ„ãƒ¼ãƒ«"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.setup_logging()
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.e2e_runner = E2ETestAutomation()
        self.gui_runner = GUIButtonTestAutomation()
        self.production_runner = ProductionEnvironmentTest()
        self.integrated_runner = IntegratedTestRunner()
        self.performance_optimizer = TestOptimizer()
        self.parallel_runner = ParallelTestRunner()
        self.coverage_analyzer = CoverageAnalyzer()
        self.report_manager = ReportManager()
        self.data_manager = TestDataManager()
        
        # çµæœä¿å­˜ç”¨
        self.test_results: List[TestResult] = []
        self.coverage_data: List[CoverageData] = []
        self.performance_metrics: List[PerformanceMetrics] = []
    
    def setup_logging(self):
        """ãƒ­ã‚°è¨­å®š"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(sys.stdout),
                logging.FileHandler('test_runner.log', encoding='utf-8')
            ]
        )
    
    def run_e2e_tests(self, args) -> bool:
        """E2Eãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        print("ğŸš€ E2Eãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        try:
            result = self.e2e_runner.run_comprehensive_e2e_test()
            
            # çµæœã‚’è¨˜éŒ²
            test_result = TestResult(
                test_name="E2E Tests",
                status="success" if result.get("success", False) else "failure",
                execution_time=result.get("execution_time", 0),
                memory_usage=result.get("memory_usage", 0),
                cpu_usage=result.get("cpu_usage", 0),
                timestamp=datetime.now(),
                error_message=result.get("error_message")
            )
            self.test_results.append(test_result)
            
            print(f"âœ… E2Eãƒ†ã‚¹ãƒˆå®Œäº†: {result.get('success', False)}")
            return result.get("success", False)
            
        except Exception as e:
            self.logger.error(f"âŒ E2Eãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_gui_tests(self, args) -> bool:
        """GUIãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        print("ğŸ¨ GUIãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        try:
            result = self.gui_runner.run_all_gui_tests()
            
            # çµæœã‚’è¨˜éŒ²
            test_result = TestResult(
                test_name="GUI Tests",
                status="success" if result.get("success", False) else "failure",
                execution_time=result.get("execution_time", 0),
                memory_usage=result.get("memory_usage", 0),
                cpu_usage=result.get("cpu_usage", 0),
                timestamp=datetime.now(),
                error_message=result.get("error_message")
            )
            self.test_results.append(test_result)
            
            print(f"âœ… GUIãƒ†ã‚¹ãƒˆå®Œäº†: {result.get('success', False)}")
            return result.get("success", False)
            
        except Exception as e:
            self.logger.error(f"âŒ GUIãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_production_tests(self, args) -> bool:
        """æœ¬ç•ªç’°å¢ƒãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        print("ğŸ­ æœ¬ç•ªç’°å¢ƒãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        try:
            result = self.production_runner.run_all_production_tests()
            
            # çµæœã‚’è¨˜éŒ²
            test_result = TestResult(
                test_name="Production Tests",
                status="success" if result.get("success", False) else "failure",
                execution_time=result.get("execution_time", 0),
                memory_usage=result.get("memory_usage", 0),
                cpu_usage=result.get("cpu_usage", 0),
                timestamp=datetime.now(),
                error_message=result.get("error_message")
            )
            self.test_results.append(test_result)
            
            print(f"âœ… æœ¬ç•ªç’°å¢ƒãƒ†ã‚¹ãƒˆå®Œäº†: {result.get('success', False)}")
            return result.get("success", False)
            
        except Exception as e:
            self.logger.error(f"âŒ æœ¬ç•ªç’°å¢ƒãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_performance_tests(self, args) -> bool:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        print("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        try:
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ã‚’ä½¿ç”¨
            profiler = PerformanceProfiler()
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚¹ãƒˆé–¢æ•°
            def sample_test():
                import time
                time.sleep(1)
                return {"result": "test"}
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
            metrics = profiler.profile_test(sample_test, "performance_test")
            
            # çµæœã‚’è¨˜éŒ²
            performance_metric = PerformanceMetrics(
                test_name="Performance Test",
                execution_time=metrics.execution_time,
                memory_usage_mb=metrics.memory_usage_mb,
                cpu_usage_percent=metrics.cpu_usage_percent,
                gc_collections=metrics.gc_collections,
                gc_time=metrics.gc_time
            )
            self.performance_metrics.append(performance_metric)
            
            test_result = TestResult(
                test_name="Performance Tests",
                status="success",
                execution_time=metrics.execution_time,
                memory_usage=metrics.memory_usage_mb,
                cpu_usage=metrics.cpu_usage_percent,
                timestamp=datetime.now()
            )
            self.test_results.append(test_result)
            
            print(f"âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Œäº†: {metrics.execution_time:.2f}ç§’")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_parallel_tests(self, args) -> bool:
        """ä¸¦åˆ—ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        print("ğŸ”„ ä¸¦åˆ—ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        try:
            # éåŒæœŸã§ä¸¦åˆ—ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
            async def run_parallel():
                return await self.parallel_runner.run_parallel_tests()
            
            result = asyncio.run(run_parallel())
            
            # çµæœã‚’è¨˜éŒ²
            test_result = TestResult(
                test_name="Parallel Tests",
                status="success" if result.get("summary", {}).get("successful_tests", 0) > 0 else "failure",
                execution_time=result.get("summary", {}).get("parallel_execution_time", 0),
                memory_usage=0,  # ä¸¦åˆ—å®Ÿè¡Œã§ã¯å€‹åˆ¥ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—ã§ããªã„
                cpu_usage=0,
                timestamp=datetime.now()
            )
            self.test_results.append(test_result)
            
            summary = result.get("summary", {})
            print(f"âœ… ä¸¦åˆ—ãƒ†ã‚¹ãƒˆå®Œäº†: {summary.get('successful_tests', 0)}/{summary.get('total_tests', 0)} æˆåŠŸ")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ä¸¦åˆ—ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_coverage_analysis(self, args) -> bool:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã‚’å®Ÿè¡Œ"""
        print("ğŸ“Š ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã‚’å®Ÿè¡Œä¸­...")
        try:
            # ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®šé–‹å§‹
            self.coverage_analyzer.start_coverage_measurement()
            
            # ç°¡å˜ãªãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¦ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’æ¸¬å®š
            def sample_function():
                return "test"
            
            sample_function()
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸æ¸¬å®šåœæ­¢
            self.coverage_analyzer.stop_coverage_measurement()
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ
            coverage_data = self.coverage_analyzer.analyze_coverage()
            
            # çµæœã‚’è¨˜éŒ²
            for file_path, metrics in coverage_data.get("files", {}).items():
                coverage = CoverageData(
                    file_path=file_path,
                    total_lines=metrics.get("total_lines", 0),
                    covered_lines=metrics.get("covered_lines", 0),
                    coverage_percentage=metrics.get("coverage_percentage", 0),
                    uncovered_lines=metrics.get("uncovered_lines", [])
                )
                self.coverage_data.append(coverage)
            
            test_result = TestResult(
                test_name="Coverage Analysis",
                status="success",
                execution_time=0,
                memory_usage=0,
                cpu_usage=0,
                timestamp=datetime.now()
            )
            self.test_results.append(test_result)
            
            overall_coverage = coverage_data.get("coverage_percentage", 0)
            print(f"âœ… ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æå®Œäº†: {overall_coverage:.1f}%")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def generate_test_data(self, args) -> bool:
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        print("ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
        try:
            if args.data_type == "sample":
                config = TestDataFactory.create_sample_data_config()
            elif args.data_type == "performance":
                config = TestDataFactory.create_performance_test_config()
            else:
                # ã‚«ã‚¹ã‚¿ãƒ è¨­å®š
                config = DataGenerationConfig(
                    data_type=args.format,
                    size=args.size,
                    columns=args.columns.split(","),
                    data_types={col: "float" for col in args.columns.split(",")},
                    seed=args.seed
                )
            
            dataset = self.data_manager.generate_test_data(
                config,
                name=args.name,
                description=args.description,
                tags=args.tags.split(",") if args.tags else None
            )
            
            if dataset:
                print(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {dataset.name}")
                return True
            else:
                print("âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def generate_report(self, args) -> bool:
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        print("ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
        try:
            # ãƒ†ã‚¹ãƒˆçµæœã‚’ãƒ¬ãƒãƒ¼ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã«è¿½åŠ 
            for result in self.test_results:
                self.report_manager.generator.add_test_result(result)
            
            # ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            for coverage in self.coverage_data:
                self.report_manager.generator.add_coverage_data(coverage)
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¿½åŠ 
            for metrics in self.performance_metrics:
                self.report_manager.generator.add_performance_metrics(metrics)
            
            # HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
            report_path = self.report_manager.generate_comprehensive_report(args.output)
            
            if report_path:
                print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_path}")
                
                # ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
                summary = self.report_manager.generate_summary_report()
                print(f"\nğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆã‚µãƒãƒªãƒ¼:")
                print(f"  ç·ãƒ†ã‚¹ãƒˆæ•°: {summary.get('total_tests', 0)}")
                print(f"  æˆåŠŸç‡: {summary.get('success_rate', 0):.1f}%")
                print(f"  ã‚«ãƒãƒ¬ãƒƒã‚¸ç‡: {summary.get('coverage_percentage', 0):.1f}%")
                print(f"  ç·å®Ÿè¡Œæ™‚é–“: {summary.get('total_execution_time', 0):.2f}ç§’")
                
                return True
            else:
                print("âŒ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
                return False
                
        except Exception as e:
            self.logger.error(f"âŒ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def run_all_tests(self, args) -> bool:
        """ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ"""
        print("ğŸš€ åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œä¸­...")
        
        success_count = 0
        total_tests = 0
        
        # å„ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
        test_functions = [
            ("E2E Tests", self.run_e2e_tests),
            ("GUI Tests", self.run_gui_tests),
            ("Production Tests", self.run_production_tests),
            ("Performance Tests", self.run_performance_tests),
            ("Parallel Tests", self.run_parallel_tests),
            ("Coverage Analysis", self.run_coverage_analysis)
        ]
        
        for test_name, test_func in test_functions:
            total_tests += 1
            print(f"\n{'='*50}")
            print(f"å®Ÿè¡Œä¸­: {test_name}")
            print(f"{'='*50}")
            
            try:
                if test_func(args):
                    success_count += 1
                    print(f"âœ… {test_name} æˆåŠŸ")
                else:
                    print(f"âŒ {test_name} å¤±æ•—")
            except Exception as e:
                self.logger.error(f"âŒ {test_name} ã‚¨ãƒ©ãƒ¼: {e}")
                print(f"âŒ {test_name} ã‚¨ãƒ©ãƒ¼")
        
        # çµæœã‚µãƒãƒªãƒ¼
        print(f"\n{'='*50}")
        print(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå®Œäº†")
        print(f"{'='*50}")
        print(f"æˆåŠŸ: {success_count}/{total_tests}")
        print(f"æˆåŠŸç‡: {success_count/total_tests*100:.1f}%")
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        if args.report:
            print(f"\nğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­...")
            self.generate_report(args)
        
        return success_count == total_tests

def create_parser():
    """CLIãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä½œæˆ"""
    parser = argparse.ArgumentParser(
        description="Professional Statistics Suite - çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ„ãƒ¼ãƒ«",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
  python cli_test_runner.py run-all

  # ç‰¹å®šã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
  python cli_test_runner.py e2e
  python cli_test_runner.py gui
  python cli_test_runner.py production

  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
  python cli_test_runner.py performance

  # ä¸¦åˆ—ãƒ†ã‚¹ãƒˆ
  python cli_test_runner.py parallel

  # ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ
  python cli_test_runner.py coverage

  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
  python cli_test_runner.py generate-data --type sample --name employee_data

  # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
  python cli_test_runner.py report --output test_report.html
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰')
    
    # run-all ã‚³ãƒãƒ³ãƒ‰
    run_all_parser = subparsers.add_parser('run-all', help='ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ')
    run_all_parser.add_argument('--report', action='store_true', help='ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ')
    
    # e2e ã‚³ãƒãƒ³ãƒ‰
    e2e_parser = subparsers.add_parser('e2e', help='E2Eãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ')
    
    # gui ã‚³ãƒãƒ³ãƒ‰
    gui_parser = subparsers.add_parser('gui', help='GUIãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ')
    
    # production ã‚³ãƒãƒ³ãƒ‰
    production_parser = subparsers.add_parser('production', help='æœ¬ç•ªç’°å¢ƒãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ')
    
    # performance ã‚³ãƒãƒ³ãƒ‰
    performance_parser = subparsers.add_parser('performance', help='ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ')
    
    # parallel ã‚³ãƒãƒ³ãƒ‰
    parallel_parser = subparsers.add_parser('parallel', help='ä¸¦åˆ—ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ')
    
    # coverage ã‚³ãƒãƒ³ãƒ‰
    coverage_parser = subparsers.add_parser('coverage', help='ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æã‚’å®Ÿè¡Œ')
    
    # generate-data ã‚³ãƒãƒ³ãƒ‰
    generate_data_parser = subparsers.add_parser('generate-data', help='ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ')
    generate_data_parser.add_argument('--type', choices=['sample', 'performance', 'custom'], 
                                     default='sample', help='ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—')
    generate_data_parser.add_argument('--name', required=True, help='ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå')
    generate_data_parser.add_argument('--description', default='', help='èª¬æ˜')
    generate_data_parser.add_argument('--format', default='csv', choices=['csv', 'json', 'pickle'], 
                                     help='å‡ºåŠ›å½¢å¼')
    generate_data_parser.add_argument('--size', type=int, default=1000, help='ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º')
    generate_data_parser.add_argument('--columns', help='ã‚«ãƒ©ãƒ åï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰')
    generate_data_parser.add_argument('--seed', type=int, help='ä¹±æ•°ã‚·ãƒ¼ãƒ‰')
    generate_data_parser.add_argument('--tags', help='ã‚¿ã‚°ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰')
    
    # report ã‚³ãƒãƒ³ãƒ‰
    report_parser = subparsers.add_parser('report', help='ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ')
    report_parser.add_argument('--output', default='test_report.html', help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å')
    
    # list-data ã‚³ãƒãƒ³ãƒ‰
    list_data_parser = subparsers.add_parser('list-data', help='ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä¸€è¦§ã‚’è¡¨ç¤º')
    list_data_parser.add_argument('--tags', help='ã‚¿ã‚°ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°')
    
    # cleanup ã‚³ãƒãƒ³ãƒ‰
    cleanup_parser = subparsers.add_parser('cleanup', help='å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—')
    cleanup_parser.add_argument('--days', type=int, default=30, help='å‰Šé™¤ã™ã‚‹æ—¥æ•°')
    
    return parser

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = create_parser()
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # CLIãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼åˆæœŸåŒ–
    runner = CLITestRunner()
    
    try:
        if args.command == 'run-all':
            success = runner.run_all_tests(args)
            sys.exit(0 if success else 1)
            
        elif args.command == 'e2e':
            success = runner.run_e2e_tests(args)
            sys.exit(0 if success else 1)
            
        elif args.command == 'gui':
            success = runner.run_gui_tests(args)
            sys.exit(0 if success else 1)
            
        elif args.command == 'production':
            success = runner.run_production_tests(args)
            sys.exit(0 if success else 1)
            
        elif args.command == 'performance':
            success = runner.run_performance_tests(args)
            sys.exit(0 if success else 1)
            
        elif args.command == 'parallel':
            success = runner.run_parallel_tests(args)
            sys.exit(0 if success else 1)
            
        elif args.command == 'coverage':
            success = runner.run_coverage_analysis(args)
            sys.exit(0 if success else 1)
            
        elif args.command == 'generate-data':
            success = runner.generate_test_data(args)
            sys.exit(0 if success else 1)
            
        elif args.command == 'report':
            success = runner.generate_report(args)
            sys.exit(0 if success else 1)
            
        elif args.command == 'list-data':
            datasets = runner.data_manager.list_test_data(
                tags=args.tags.split(",") if args.tags else None
            )
            print(f"ğŸ“‹ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§ ({len(datasets)}ä»¶):")
            for dataset in datasets:
                print(f"  - {dataset.name}: {dataset.description}")
                print(f"    ã‚¿ã‚¤ãƒ—: {dataset.data_type}, ã‚µã‚¤ã‚º: {dataset.size_bytes} bytes")
                print(f"    ä½œæˆæ—¥: {dataset.created_at.strftime('%Y-%m-%d %H:%M:%S')}")
                if dataset.tags:
                    print(f"    ã‚¿ã‚°: {', '.join(dataset.tags)}")
                print()
            
        elif args.command == 'cleanup':
            deleted_count = runner.data_manager.cleanup_old_data(args.days)
            print(f"âœ… {deleted_count}ä»¶ã®å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            
    except KeyboardInterrupt:
        print("\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 