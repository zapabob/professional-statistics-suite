#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Test Latest AI Integration - 2025 July 25th Edition
æœ€æ–°AIçµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ - 2025å¹´7æœˆ25æ—¥ç‰ˆ
"""

import asyncio
import pandas as pd
import numpy as np
from pathlib import Path
import logging
from datetime import datetime
import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.ai.latest_ai_integration import LatestAIOrchestrator
from src.ai.gguf_model_manager import GGUFModelManager

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

class LatestAIIntegrationTester:
    """æœ€æ–°AIçµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.orchestrator = LatestAIOrchestrator()
        self.model_manager = GGUFModelManager()
        self.test_results = []
    
    async def test_provider_availability(self):
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ©ç”¨å¯èƒ½æ€§ãƒ†ã‚¹ãƒˆ"""
        print("ğŸ” ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ©ç”¨å¯èƒ½æ€§ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        providers = self.orchestrator.get_available_providers()
        print(f"åˆ©ç”¨å¯èƒ½ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {providers}")
        
        status = self.orchestrator.get_provider_status()
        print(f"ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼çŠ¶æ…‹: {status}")
        
        for provider, is_available in status.items():
            result = {
                'test': 'provider_availability',
                'provider': provider,
                'available': is_available,
                'timestamp': datetime.now().isoformat()
            }
            self.test_results.append(result)
            
            status_icon = "âœ…" if is_available else "âŒ"
            print(f"{status_icon} {provider}: {'åˆ©ç”¨å¯èƒ½' if is_available else 'åˆ©ç”¨ä¸å¯'}")
    
    async def test_basic_query(self, query: str = "tæ¤œå®šã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„"):
        """åŸºæœ¬ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ"""
        print(f"\nğŸ” åŸºæœ¬ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆé–‹å§‹: {query}")
        
        try:
            response = await self.orchestrator.analyze_query(query)
            
            result = {
                'test': 'basic_query',
                'query': query,
                'provider_used': response.provider_used,
                'success': response.confidence > 0.5,
                'processing_time': response.processing_time,
                'tokens_consumed': response.tokens_consumed,
                'timestamp': datetime.now().isoformat()
            }
            self.test_results.append(result)
            
            print(f"âœ… ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {response.provider_used}")
            print(f"â±ï¸ å‡¦ç†æ™‚é–“: {response.processing_time:.2f}ç§’")
            print(f"ğŸ”¢ ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {response.tokens_consumed}")
            print(f"ğŸ“Š ä¿¡é ¼åº¦: {response.confidence:.2f}")
            print(f"ğŸ“ å¿œç­”: {response.content[:200]}...")
            
        except Exception as e:
            print(f"âŒ åŸºæœ¬ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            result = {
                'test': 'basic_query',
                'query': query,
                'error': str(e),
                'success': False,
                'timestamp': datetime.now().isoformat()
            }
            self.test_results.append(result)
    
    async def test_statistical_analysis(self):
        """çµ±è¨ˆåˆ†æãƒ†ã‚¹ãƒˆ"""
        print("\nğŸ” çµ±è¨ˆåˆ†æãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
        np.random.seed(42)
        data = pd.DataFrame({
            'group_a': np.random.normal(100, 15, 30),
            'group_b': np.random.normal(105, 15, 30),
            'age': np.random.randint(20, 60, 30),
            'score': np.random.normal(75, 10, 30)
        })
        
        queries = [
            "ã“ã®ãƒ‡ãƒ¼ã‚¿ã§tæ¤œå®šã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„",
            "ç›¸é–¢åˆ†æã‚’è¡Œã£ã¦ãã ã•ã„",
            "å›å¸°åˆ†æã®æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„"
        ]
        
        for query in queries:
            try:
                response = await self.orchestrator.analyze_query(query, data)
                
                result = {
                    'test': 'statistical_analysis',
                    'query': query,
                    'provider_used': response.provider_used,
                    'success': response.confidence > 0.5,
                    'processing_time': response.processing_time,
                    'timestamp': datetime.now().isoformat()
                }
                self.test_results.append(result)
                
                print(f"âœ… {query}")
                print(f"   ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {response.provider_used}")
                print(f"   å‡¦ç†æ™‚é–“: {response.processing_time:.2f}ç§’")
                
            except Exception as e:
                print(f"âŒ çµ±è¨ˆåˆ†æãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                result = {
                    'test': 'statistical_analysis',
                    'query': query,
                    'error': str(e),
                    'success': False,
                    'timestamp': datetime.now().isoformat()
                }
                self.test_results.append(result)
    
    async def test_provider_fallback(self):
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆ"""
        print("\nğŸ” ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        # å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ãƒ†ã‚¹ãƒˆ
        providers = self.orchestrator.get_available_providers()
        
        for provider in providers:
            try:
                response = await self.orchestrator.analyze_query(
                    "ç°¡å˜ãªçµ±è¨ˆã®èª¬æ˜ã‚’ã—ã¦ãã ã•ã„",
                    preferred_provider=provider
                )
                
                result = {
                    'test': 'provider_fallback',
                    'preferred_provider': provider,
                    'actual_provider': response.provider_used,
                    'success': response.confidence > 0.5,
                    'processing_time': response.processing_time,
                    'timestamp': datetime.now().isoformat()
                }
                self.test_results.append(result)
                
                print(f"âœ… {provider} -> {response.provider_used}")
                
            except Exception as e:
                print(f"âŒ {provider} ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                result = {
                    'test': 'provider_fallback',
                    'preferred_provider': provider,
                    'error': str(e),
                    'success': False,
                    'timestamp': datetime.now().isoformat()
                }
                self.test_results.append(result)
    
    def test_gguf_model_manager(self):
        """GGUFãƒ¢ãƒ‡ãƒ«ç®¡ç†ãƒ†ã‚¹ãƒˆ"""
        print("\nğŸ” GGUFãƒ¢ãƒ‡ãƒ«ç®¡ç†ãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        # åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«ç¢ºèª
        available_models = self.model_manager.get_available_models()
        print(f"åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«æ•°: {len(available_models)}")
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ç¢ºèª
        downloaded_models = self.model_manager.list_downloaded_models()
        print(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«æ•°: {len(downloaded_models)}")
        
        # çµ±è¨ˆåˆ†ææ¨å¥¨ãƒ¢ãƒ‡ãƒ«
        recommended = self.model_manager.get_recommended_models("statistics")
        print(f"çµ±è¨ˆåˆ†ææ¨å¥¨ãƒ¢ãƒ‡ãƒ«: {recommended}")
        
        # ãƒ¢ãƒ‡ãƒ«çµ±è¨ˆ
        stats = self.model_manager.get_model_stats()
        print(f"ç·ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚µã‚¤ã‚º: {stats['total_size_gb']}GB")
        
        result = {
            'test': 'gguf_model_manager',
            'available_models': len(available_models),
            'downloaded_models': len(downloaded_models),
            'total_size_gb': stats['total_size_gb'],
            'recommended_models': recommended,
            'timestamp': datetime.now().isoformat()
        }
        self.test_results.append(result)
    
    def test_environment_configuration(self):
        """ç’°å¢ƒè¨­å®šãƒ†ã‚¹ãƒˆ"""
        print("\nğŸ” ç’°å¢ƒè¨­å®šãƒ†ã‚¹ãƒˆé–‹å§‹...")
        
        # ç’°å¢ƒå¤‰æ•°ç¢ºèª
        env_vars = {
            'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY'),
            'ANTHROPIC_API_KEY': os.getenv('ANTHROPIC_API_KEY'),
            'GOOGLE_API_KEY': os.getenv('GOOGLE_API_KEY'),
            'OLLAMA_BASE_URL': os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434'),
            'LMSTUDIO_BASE_URL': os.getenv('LMSTUDIO_BASE_URL', 'http://localhost:1234')
        }
        
        print("ç’°å¢ƒå¤‰æ•°è¨­å®šçŠ¶æ³:")
        for var, value in env_vars.items():
            if value:
                print(f"âœ… {var}: è¨­å®šæ¸ˆã¿")
            else:
                print(f"âŒ {var}: æœªè¨­å®š")
        
        result = {
            'test': 'environment_configuration',
            'env_vars': {k: bool(v) for k, v in env_vars.items()},
            'timestamp': datetime.now().isoformat()
        }
        self.test_results.append(result)
    
    def generate_test_report(self):
        """ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        print("\nğŸ“Š ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ...")
        
        # æˆåŠŸ/å¤±æ•—çµ±è¨ˆ
        total_tests = len(self.test_results)
        successful_tests = sum(1 for result in self.test_results if result.get('success', False))
        failed_tests = total_tests - successful_tests
        
        print(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {total_tests}")
        print(f"æˆåŠŸ: {successful_tests}")
        print(f"å¤±æ•—: {failed_tests}")
        print(f"æˆåŠŸç‡: {(successful_tests/total_tests*100):.1f}%")
        
        # ãƒ†ã‚¹ãƒˆçµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        report_file = Path("test_results") / f"latest_ai_integration_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_file.parent.mkdir(exist_ok=True)
        
        report_data = {
            'test_summary': {
                'total_tests': total_tests,
                'successful_tests': successful_tests,
                'failed_tests': failed_tests,
                'success_rate': successful_tests/total_tests*100 if total_tests > 0 else 0
            },
            'test_results': self.test_results,
            'timestamp': datetime.now().isoformat()
        }
        
        import json
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")
        
        return report_data
    
    async def run_all_tests(self):
        """å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""
        print("ğŸš€ æœ€æ–°AIçµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆé–‹å§‹")
        print("=" * 50)
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        await self.test_provider_availability()
        await self.test_basic_query()
        await self.test_statistical_analysis()
        await self.test_provider_fallback()
        self.test_gguf_model_manager()
        self.test_environment_configuration()
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = self.generate_test_report()
        
        print("\n" + "=" * 50)
        print("ğŸ‰ ãƒ†ã‚¹ãƒˆå®Œäº†ï¼")
        
        return report

async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    tester = LatestAIIntegrationTester()
    report = await tester.run_all_tests()
    
    # çµæœã‚µãƒãƒªãƒ¼
    summary = report['test_summary']
    print(f"\nğŸ“ˆ ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼:")
    print(f"æˆåŠŸç‡: {summary['success_rate']:.1f}%")
    print(f"æˆåŠŸ: {summary['successful_tests']}/{summary['total_tests']}")

if __name__ == "__main__":
    asyncio.run(main()) 