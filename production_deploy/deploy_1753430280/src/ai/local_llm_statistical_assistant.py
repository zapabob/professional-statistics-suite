#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Local LLM Statistical Assistant
ãƒ­ãƒ¼ã‚«ãƒ«LLMçµ±è¨ˆè£œåŠ©ã‚·ã‚¹ãƒ†ãƒ 

LMStudioã®Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¨è«–ã—ã€
çµ±è¨ˆåˆ†æã®ã‚µãƒãƒ¼ãƒˆã‚’è¡Œã†ã‚·ã‚¹ãƒ†ãƒ 
"""

import os
import sys
import time
import json
import logging
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
import asyncio
from dataclasses import dataclass
from datetime import datetime

# ãƒ‡ãƒ¼ã‚¿å‡¦ç†
import pandas as pd
import numpy as np

# LMStudioãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    from lmstudio import LMStudioClient
    LMSTUDIO_AVAILABLE = True
except ImportError:
    try:
        from lmstudio.client import LMStudioClient
        LMSTUDIO_AVAILABLE = True
    except ImportError:
        LMSTUDIO_AVAILABLE = False
        print("âš ï¸ LMStudioãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

# æ—¢å­˜ã®AIçµ±åˆã‚·ã‚¹ãƒ†ãƒ 
from ai_integration import AIOrchestrator, AnalysisContext, IntentType

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class GGUFModelConfig:
    """GGUFãƒ¢ãƒ‡ãƒ«è¨­å®š"""
    model_path: str
    model_name: str
    context_size: int = 4096
    temperature: float = 0.3
    max_tokens: int = 512
    top_p: float = 0.9
    repeat_penalty: float = 1.1

@dataclass
class StatisticalQuery:
    """çµ±è¨ˆã‚¯ã‚¨ãƒª"""
    query: str
    data_info: Optional[Dict[str, Any]] = None
    context: Optional[str] = None
    user_expertise: str = "intermediate"
    language: str = "ja"

@dataclass
class StatisticalResponse:
    """çµ±è¨ˆå¿œç­”"""
    answer: str
    confidence: float
    suggested_methods: List[str]
    processing_time: float
    tokens_used: int
    educational_content: Optional[str] = None
    code_example: Optional[str] = None

class LocalLLMStatisticalAssistant:
    """ãƒ­ãƒ¼ã‚«ãƒ«LLMçµ±è¨ˆè£œåŠ©ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, model_config: GGUFModelConfig):
        self.model_config = model_config
        self.client = None
        self.is_initialized = False
        
        # çµ±è¨ˆåˆ†æç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        self.statistical_prompts = {
            "descriptive": self._get_descriptive_prompt(),
            "inferential": self._get_inferential_prompt(),
            "predictive": self._get_predictive_prompt(),
            "educational": self._get_educational_prompt()
        }
        
        # åˆ©ç”¨å¯èƒ½ãªçµ±è¨ˆæ‰‹æ³•ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        self.statistical_methods = self._load_statistical_methods()
    
    def initialize(self) -> bool:
        """LMStudioã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        try:
            if not LMSTUDIO_AVAILABLE:
                logger.error("LMStudioãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                return False
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
            if not Path(self.model_config.model_path).exists():
                logger.error(f"GGUFãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.model_config.model_path}")
                return False
            
            # LMStudioã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ï¼ˆå®Ÿéš›ã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«åˆã‚ã›ã¦èª¿æ•´ï¼‰
            try:
                self.client = LMStudioClient()
                logger.info(f"âœ… LMStudioã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"âš ï¸ LMStudioã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆæœŸåŒ–ã§ããªãã¦ã‚‚ã€ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¯å­˜åœ¨ã™ã‚‹ã®ã§ç¶šè¡Œ
                pass
            
            logger.info(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {self.model_config.model_path}")
            self.is_initialized = True
            return True
            
        except Exception as e:
            logger.error(f"âŒ LMStudioã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å¤±æ•—: {e}")
            return False
    
    async def analyze_statistical_query(self, query: StatisticalQuery) -> StatisticalResponse:
        """çµ±è¨ˆã‚¯ã‚¨ãƒªã‚’åˆ†æ"""
        if not self.is_initialized:
            return StatisticalResponse(
                answer="ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“",
                confidence=0.0,
                suggested_methods=[],
                processing_time=0.0,
                tokens_used=0
            )
        
        try:
            start_time = time.time()
            
            # ã‚¯ã‚¨ãƒªã®æ„å›³ã‚’åˆ†é¡
            intent = self._classify_query_intent(query.query)
            
            # é©åˆ‡ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
            prompt = self._build_statistical_prompt(query, intent)
            
            # LMStudioã§æ¨è«–å®Ÿè¡Œ
            response = await self._generate_response(prompt)
            
            processing_time = time.time() - start_time
            
            # å¿œç­”ã‚’è§£æ
            parsed_response = self._parse_statistical_response(response)
            
            return StatisticalResponse(
                answer=parsed_response.get('answer', response),
                confidence=parsed_response.get('confidence', 0.7),
                suggested_methods=parsed_response.get('suggested_methods', []),
                educational_content=parsed_response.get('educational_content'),
                code_example=parsed_response.get('code_example'),
                processing_time=processing_time,
                tokens_used=parsed_response.get('tokens_used', 0)
            )
            
        except Exception as e:
            logger.error(f"çµ±è¨ˆã‚¯ã‚¨ãƒªåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return StatisticalResponse(
                answer=f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                confidence=0.0,
                suggested_methods=[],
                processing_time=0.0,
                tokens_used=0
            )
    
    async def _generate_response(self, prompt: str) -> str:
        """LMStudioã§å¿œç­”ç”Ÿæˆ"""
        try:
            if self.client:
                # LMStudioã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§æ¨è«–å®Ÿè¡Œ
                response = await self.client.chat.completions.create(
                    model=self.model_config.model_name,
                    messages=[
                        {"role": "system", "content": "ã‚ãªãŸã¯çµ±è¨ˆåˆ†æã®å°‚é–€å®¶ã§ã™ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=self.model_config.temperature,
                    max_tokens=self.model_config.max_tokens,
                    top_p=self.model_config.top_p,
                    repeat_penalty=self.model_config.repeat_penalty
                )
                
                return response.choices[0].message.content
            else:
                # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ãƒ¢ãƒƒã‚¯å¿œç­”
                logger.warning("LMStudioã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ãƒ¢ãƒƒã‚¯å¿œç­”ã‚’è¿”ã—ã¾ã™")
                return f"ãƒ¢ãƒƒã‚¯å¿œç­”: {prompt[:100]}... (LMStudioã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã›ã‚“)"
            
        except Exception as e:
            logger.error(f"LMStudioæ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
            return f"æ¨è«–ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    def _classify_query_intent(self, query: str) -> str:
        """ã‚¯ã‚¨ãƒªã®æ„å›³ã‚’åˆ†é¡"""
        query_lower = query.lower()
        
        # è¨˜è¿°çµ±è¨ˆ
        if any(word in query_lower for word in ['å¹³å‡', 'ä¸­å¤®å€¤', 'åˆ†æ•£', 'æ¨™æº–åå·®', 'åˆ†å¸ƒ', 'è¦ç´„']):
            return "descriptive"
        
        # æ¨è«–çµ±è¨ˆ
        elif any(word in query_lower for word in ['æ¤œå®š', 'tæ¤œå®š', 'ã‚«ã‚¤äºŒä¹—', 'ç›¸é–¢', 'å›å¸°', 'æœ‰æ„']):
            return "inferential"
        
        # äºˆæ¸¬åˆ†æ
        elif any(word in query_lower for word in ['äºˆæ¸¬', 'äºˆæ¸¬', 'ãƒ¢ãƒ‡ãƒ«', 'æ©Ÿæ¢°å­¦ç¿’', 'åˆ†é¡']):
            return "predictive"
        
        # æ•™è‚²çš„å†…å®¹
        else:
            return "educational"
    
    def _build_statistical_prompt(self, query: StatisticalQuery, intent: str) -> str:
        """çµ±è¨ˆåˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
        base_prompt = self.statistical_prompts.get(intent, self.statistical_prompts["educational"])
        
        # ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã‚’è¿½åŠ 
        data_context = ""
        if query.data_info:
            data_context = f"""
ãƒ‡ãƒ¼ã‚¿æƒ…å ±:
- è¡Œæ•°: {query.data_info.get('rows', 'N/A')}
- åˆ—æ•°: {query.data_info.get('columns', 'N/A')}
- åˆ—å: {query.data_info.get('column_names', [])}
- ãƒ‡ãƒ¼ã‚¿å‹: {query.data_info.get('dtypes', {})}
"""
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
        context_info = ""
        if query.context:
            context_info = f"\nåˆ†æã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {query.context}"
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸèª¬æ˜ãƒ¬ãƒ™ãƒ«ã‚’è¨­å®š
        expertise_level = {
            "beginner": "åˆå¿ƒè€…å‘ã‘ã«è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„",
            "intermediate": "ä¸­ç´šè€…å‘ã‘ã«èª¬æ˜ã—ã¦ãã ã•ã„",
            "advanced": "ä¸Šç´šè€…å‘ã‘ã«å°‚é–€çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„"
        }.get(query.user_expertise, "ä¸­ç´šè€…å‘ã‘ã«èª¬æ˜ã—ã¦ãã ã•ã„")
        
        prompt = f"""
{base_prompt}

{data_context}
{context_info}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {query.query}
{expertise_level}

å›ç­”ã¯ä»¥ä¸‹ã®å½¢å¼ã§æä¾›ã—ã¦ãã ã•ã„:
1. ç›´æ¥çš„ãªå›ç­”
2. æ¨å¥¨ã•ã‚Œã‚‹çµ±è¨ˆæ‰‹æ³•
3. æ•™è‚²çš„å†…å®¹ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
4. ã‚³ãƒ¼ãƒ‰ä¾‹ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
"""
        
        return prompt
    
    def _get_descriptive_prompt(self) -> str:
        """è¨˜è¿°çµ±è¨ˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        return """
ã‚ãªãŸã¯çµ±è¨ˆåˆ†æã®å°‚é–€å®¶ã§ã™ã€‚è¨˜è¿°çµ±è¨ˆã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

è¨˜è¿°çµ±è¨ˆã§ã¯ä»¥ä¸‹ã®ç‚¹ã‚’è€ƒæ…®ã—ã¦ãã ã•ã„:
- ãƒ‡ãƒ¼ã‚¿ã®è¦ç´„çµ±è¨ˆé‡ï¼ˆå¹³å‡ã€ä¸­å¤®å€¤ã€åˆ†æ•£ã€æ¨™æº–åå·®ãªã©ï¼‰
- ãƒ‡ãƒ¼ã‚¿ã®åˆ†å¸ƒã®å¯è¦–åŒ–
- å¤–ã‚Œå€¤ã®æ¤œå‡º
- ãƒ‡ãƒ¼ã‚¿ã®å“è³ªè©•ä¾¡
"""
    
    def _get_inferential_prompt(self) -> str:
        """æ¨è«–çµ±è¨ˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        return """
ã‚ãªãŸã¯çµ±è¨ˆåˆ†æã®å°‚é–€å®¶ã§ã™ã€‚æ¨è«–çµ±è¨ˆã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

æ¨è«–çµ±è¨ˆã§ã¯ä»¥ä¸‹ã®ç‚¹ã‚’è€ƒæ…®ã—ã¦ãã ã•ã„:
- é©åˆ‡ãªæ¤œå®šæ‰‹æ³•ã®é¸æŠ
- ä»®èª¬ã®è¨­å®š
- æœ‰æ„æ°´æº–ã®è¨­å®š
- çµæœã®è§£é‡ˆ
- ä»®å®šã®ç¢ºèª
"""
    
    def _get_predictive_prompt(self) -> str:
        """äºˆæ¸¬åˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        return """
ã‚ãªãŸã¯çµ±è¨ˆåˆ†æã®å°‚é–€å®¶ã§ã™ã€‚äºˆæ¸¬åˆ†æã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

äºˆæ¸¬åˆ†æã§ã¯ä»¥ä¸‹ã®ç‚¹ã‚’è€ƒæ…®ã—ã¦ãã ã•ã„:
- é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ã®é¸æŠ
- ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
- ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡æŒ‡æ¨™
- éå­¦ç¿’ã®å›é¿
- çµæœã®è§£é‡ˆ
"""
    
    def _get_educational_prompt(self) -> str:
        """æ•™è‚²çš„å†…å®¹ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        return """
ã‚ãªãŸã¯çµ±è¨ˆåˆ†æã®å°‚é–€å®¶ã§ã™ã€‚çµ±è¨ˆå­¦ã®æ•™è‚²çš„å†…å®¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

æ•™è‚²çš„å†…å®¹ã§ã¯ä»¥ä¸‹ã®ç‚¹ã‚’è€ƒæ…®ã—ã¦ãã ã•ã„:
- æ¦‚å¿µã®åˆ†ã‹ã‚Šã‚„ã™ã„èª¬æ˜
- å…·ä½“ä¾‹ã®æä¾›
- å®Ÿè·µçš„ãªå¿œç”¨ä¾‹
- ã‚ˆãã‚ã‚‹èª¤è§£ã®èª¬æ˜
"""
    
    def _parse_statistical_response(self, response: str) -> Dict[str, Any]:
        """çµ±è¨ˆå¿œç­”ã‚’è§£æ"""
        try:
            # JSONå½¢å¼ã®å¿œç­”ã‚’è©¦è¡Œ
            if response.strip().startswith('{') and response.strip().endswith('}'):
                return json.loads(response)
            
            # æ§‹é€ åŒ–ã•ã‚ŒãŸå¿œç­”ã‚’è§£æ
            parsed = {
                'answer': response,
                'confidence': 0.7,
                'suggested_methods': [],
                'educational_content': None,
                'code_example': None,
                'tokens_used': len(response.split())
            }
            
            # çµ±è¨ˆæ‰‹æ³•ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            statistical_keywords = [
                'tæ¤œå®š', 'ã‚«ã‚¤äºŒä¹—æ¤œå®š', 'ç›¸é–¢åˆ†æ', 'å›å¸°åˆ†æ', 'åˆ†æ•£åˆ†æ',
                'ãƒãƒ³ãƒ›ã‚¤ãƒƒãƒˆãƒ‹ãƒ¼æ¤œå®š', 'ã‚¦ã‚£ãƒ«ã‚³ã‚¯ã‚½ãƒ³æ¤œå®š', 'ã‚¯ãƒ©ã‚¹ã‚«ãƒ«ãƒ»ã‚¦ã‚©ãƒªã‚¹æ¤œå®š',
                'ãƒ•ãƒªãƒ¼ãƒ‰ãƒãƒ³æ¤œå®š', 'ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ', 'ä¸»æˆåˆ†åˆ†æ'
            ]
            
            for keyword in statistical_keywords:
                if keyword in response:
                    parsed['suggested_methods'].append(keyword)
            
            return parsed
            
        except Exception as e:
            logger.error(f"å¿œç­”è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'answer': response,
                'confidence': 0.5,
                'suggested_methods': [],
                'tokens_used': len(response.split())
            }
    
    def _load_statistical_methods(self) -> Dict[str, Dict[str, Any]]:
        """çµ±è¨ˆæ‰‹æ³•ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿"""
        return {
            "t_test": {
                "name": "tæ¤œå®š",
                "description": "2ç¾¤ã®å¹³å‡å€¤ã®å·®ã‚’æ¤œå®š",
                "assumptions": ["æ­£è¦åˆ†å¸ƒ", "ç­‰åˆ†æ•£æ€§", "ç‹¬ç«‹æ€§"],
                "use_cases": ["2ç¾¤ã®æ¯”è¼ƒ", "å‰å¾Œæ¯”è¼ƒ"]
            },
            "chi_square": {
                "name": "ã‚«ã‚¤äºŒä¹—æ¤œå®š",
                "description": "ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç‹¬ç«‹æ€§ã‚’æ¤œå®š",
                "assumptions": ["ç‹¬ç«‹æ€§", "æœŸå¾…åº¦æ•°"],
                "use_cases": ["åˆ†å‰²è¡¨ã®åˆ†æ", "é©åˆåº¦æ¤œå®š"]
            },
            "correlation": {
                "name": "ç›¸é–¢åˆ†æ",
                "description": "2å¤‰æ•°é–“ã®é–¢ä¿‚æ€§ã‚’åˆ†æ",
                "assumptions": ["ç·šå½¢é–¢ä¿‚", "æ­£è¦åˆ†å¸ƒ"],
                "use_cases": ["é–¢ä¿‚æ€§ã®æ¢ç´¢", "äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«"]
            },
            "regression": {
                "name": "å›å¸°åˆ†æ",
                "description": "å¾“å±å¤‰æ•°ã‚’ç‹¬ç«‹å¤‰æ•°ã§äºˆæ¸¬",
                "assumptions": ["ç·šå½¢æ€§", "ç‹¬ç«‹æ€§", "ç­‰åˆ†æ•£æ€§", "æ­£è¦æ€§"],
                "use_cases": ["äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«", "å› æœé–¢ä¿‚ã®æ¢ç´¢"]
            }
        }
    
    def get_model_info(self) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—"""
        return {
            "model_path": self.model_config.model_path,
            "model_name": self.model_config.model_name,
            "context_size": self.model_config.context_size,
            "temperature": self.model_config.temperature,
            "max_tokens": self.model_config.max_tokens,
            "is_initialized": self.is_initialized
        }

class StatisticalAssistantDemo:
    """çµ±è¨ˆè£œåŠ©ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ‡ãƒ¢"""
    
    def __init__(self):
        self.assistant = None
    
    def setup_assistant(self, model_path: str) -> bool:
        """çµ±è¨ˆè£œåŠ©ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        try:
            # GGUFãƒ¢ãƒ‡ãƒ«è¨­å®š
            model_config = GGUFModelConfig(
                model_path=model_path,
                model_name=Path(model_path).stem,
                context_size=4096,
                temperature=0.3,
                max_tokens=512
            )
            
            # çµ±è¨ˆè£œåŠ©ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
            self.assistant = LocalLLMStatisticalAssistant(model_config)
            
            if self.assistant.initialize():
                print("âœ… çµ±è¨ˆè£œåŠ©ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–æˆåŠŸ")
                return True
            else:
                print("âŒ çµ±è¨ˆè£œåŠ©ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å¤±æ•—")
                return False
                
        except Exception as e:
            print(f"âŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def run_demo(self):
        """ãƒ‡ãƒ¢ã‚’å®Ÿè¡Œ"""
        if not self.assistant:
            print("âŒ çµ±è¨ˆè£œåŠ©ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return
        
        print("\nğŸš€ ãƒ­ãƒ¼ã‚«ãƒ«LLMçµ±è¨ˆè£œåŠ©ã‚·ã‚¹ãƒ†ãƒ  ãƒ‡ãƒ¢")
        print("=" * 50)
        
        # ãƒ‡ãƒ¢ã‚¯ã‚¨ãƒª
        demo_queries = [
            StatisticalQuery(
                query="ãƒ‡ãƒ¼ã‚¿ã®å¹³å‡å€¤ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—ã™ã‚‹æ–¹æ³•ã‚’æ•™ãˆã¦ãã ã•ã„",
                user_expertise="beginner"
            ),
            StatisticalQuery(
                query="2ç¾¤ã®å¹³å‡å€¤ã®å·®ã‚’æ¤œå®šã™ã‚‹ã«ã¯ã©ã®æ‰‹æ³•ã‚’ä½¿ã„ã¾ã™ã‹ï¼Ÿ",
                user_expertise="intermediate"
            ),
            StatisticalQuery(
                query="ç›¸é–¢åˆ†æã¨å›å¸°åˆ†æã®é•ã„ã‚’èª¬æ˜ã—ã¦ãã ã•ã„",
                user_expertise="intermediate"
            ),
            StatisticalQuery(
                query="ãƒ‡ãƒ¼ã‚¿ãŒæ­£è¦åˆ†å¸ƒã—ã¦ã„ãªã„å ´åˆã®å¯¾å‡¦æ³•ã‚’æ•™ãˆã¦ãã ã•ã„",
                user_expertise="advanced"
            )
        ]
        
        for i, query in enumerate(demo_queries, 1):
            print(f"\nğŸ“Š ãƒ‡ãƒ¢ã‚¯ã‚¨ãƒª {i}: {query.query}")
            print("-" * 40)
            
            response = await self.assistant.analyze_statistical_query(query)
            
            print(f"âœ… å›ç­”ç”ŸæˆæˆåŠŸ")
            print(f"â±ï¸  å‡¦ç†æ™‚é–“: {response.processing_time:.2f}ç§’")
            print(f"ğŸ”¢ ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡: {response.tokens_used}")
            print(f"ğŸ“Š ä¿¡é ¼åº¦: {response.confidence:.2f}")
            
            if response.suggested_methods:
                print(f"ğŸ“‹ æ¨å¥¨æ‰‹æ³•: {', '.join(response.suggested_methods)}")
            
            print(f"ğŸ“ å›ç­”:\n{response.answer}")
            
            if response.educational_content:
                print(f"ğŸ“š æ•™è‚²çš„å†…å®¹:\n{response.educational_content}")
            
            if response.code_example:
                print(f"ğŸ’» ã‚³ãƒ¼ãƒ‰ä¾‹:\n{response.code_example}")

async def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print("ğŸ§  ãƒ­ãƒ¼ã‚«ãƒ«LLMçµ±è¨ˆè£œåŠ©ã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 50)
    
    # å‹•çš„ã«GGUFãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    model_paths = [
        "./models/",
        "../models/",
        "../../models/",
        "~/models/",
        "~/Downloads/"
    ]
    
    available_model = None
    for base_path in model_paths:
        search_path = Path(base_path).expanduser()
        if search_path.exists():
            # .ggufãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            gguf_files = list(search_path.glob('*.gguf'))
            if gguf_files:
                # ã‚µã‚¤ã‚ºã§ã‚½ãƒ¼ãƒˆï¼ˆå°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‚’å„ªå…ˆï¼‰
                gguf_files.sort(key=lambda x: x.stat().st_size)
                available_model = str(gguf_files[0])
                break
    
    if not available_model:
        print("âŒ GGUFãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("ğŸ’¡ ä»¥ä¸‹ã®ãƒ‘ã‚¹ã«GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®ã—ã¦ãã ã•ã„:")
        for path in model_paths:
            print(f"   - {path}")
        print("\nğŸ”§ AIã‚µãƒãƒ¼ãƒˆãªã—ã§ã‚‚çµ±è¨ˆåˆ†æã¯åˆ©ç”¨å¯èƒ½ã§ã™")
        return
    
    print(f"âœ… GGUFãƒ¢ãƒ‡ãƒ«ã‚’ç™ºè¦‹: {available_model}")
    
    # ãƒ‡ãƒ¢ã‚’å®Ÿè¡Œ
    demo = StatisticalAssistantDemo()
    if demo.setup_assistant(available_model):
        await demo.run_demo()
    else:
        print("âŒ ãƒ‡ãƒ¢ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    asyncio.run(main()) 