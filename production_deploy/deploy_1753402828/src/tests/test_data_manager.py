#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Test Data Manager
ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 

Author: Ryo Minegishi
Email: r.minegishi1987@gmail.com
License: MIT
"""

import json
import os
import sys
import shutil
import hashlib
import zipfile
import tempfile
from pathlib import Path
from typing import Dict, List, Any, Optional, Union, Callable
from datetime import datetime, timedelta
import logging
from dataclasses import dataclass, field
import pandas as pd
import numpy as np
import sqlite3
import pickle
import yaml
from abc import ABC, abstractmethod
import threading
import time
import random
import string

@dataclass
class TestDataSet:
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    name: str
    description: str
    data_type: str  # csv, json, sqlite, pickle, yaml
    file_path: str
    size_bytes: int
    created_at: datetime
    updated_at: datetime
    version: str
    tags: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)
    checksum: str = ""

@dataclass
class DataGenerationConfig:
    """ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆè¨­å®š"""
    data_type: str
    size: int
    columns: List[str]
    data_types: Dict[str, str]  # column_name -> data_type
    constraints: Dict[str, Any] = field(default_factory=dict)
    seed: Optional[int] = None

class DataGenerator:
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå™¨"""
    
    def __init__(self, seed: Optional[int] = None):
        self.seed = seed or int(time.time())
        np.random.seed(self.seed)
        random.seed(self.seed)
        self.logger = logging.getLogger(__name__)
    
    def generate_numeric_data(self, size: int, data_type: str = "float", **kwargs) -> List[Union[int, float]]:
        """æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        if data_type == "int":
            min_val = kwargs.get("min_val", 0)
            max_val = kwargs.get("max_val", 1000)
            return np.random.randint(min_val, max_val, size).tolist()
        elif data_type == "float":
            min_val = kwargs.get("min_val", 0.0)
            max_val = kwargs.get("max_val", 1000.0)
            return np.random.uniform(min_val, max_val, size).tolist()
        elif data_type == "normal":
            mean = kwargs.get("mean", 0.0)
            std = kwargs.get("std", 1.0)
            return np.random.normal(mean, std, size).tolist()
        else:
            raise ValueError(f"Unsupported numeric data type: {data_type}")
    
    def generate_categorical_data(self, size: int, categories: List[str], **kwargs) -> List[str]:
        """ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        weights = kwargs.get("weights", None)
        if weights and len(weights) == len(categories):
            return np.random.choice(categories, size, p=weights).tolist()
        else:
            return np.random.choice(categories, size).tolist()
    
    def generate_text_data(self, size: int, min_length: int = 5, max_length: int = 20, **kwargs) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        texts = []
        for _ in range(size):
            length = random.randint(min_length, max_length)
            text = ''.join(random.choices(string.ascii_letters + string.digits, k=length))
            texts.append(text)
        return texts
    
    def generate_datetime_data(self, size: int, start_date: str = "2020-01-01", end_date: str = "2024-12-31", **kwargs) -> List[str]:
        """æ—¥æ™‚ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        start = datetime.strptime(start_date, "%Y-%m-%d")
        end = datetime.strptime(end_date, "%Y-%m-%d")
        
        dates = []
        for _ in range(size):
            random_date = start + timedelta(days=random.randint(0, (end - start).days))
            dates.append(random_date.strftime("%Y-%m-%d"))
        return dates
    
    def generate_boolean_data(self, size: int, true_probability: float = 0.5) -> List[bool]:
        """ãƒ–ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        return np.random.choice([True, False], size, p=[true_probability, 1-true_probability]).tolist()
    
    def generate_dataframe(self, config: DataGenerationConfig) -> pd.DataFrame:
        """è¨­å®šã«åŸºã¥ã„ã¦DataFrameã‚’ç”Ÿæˆ"""
        data = {}
        
        for column in config.columns:
            data_type = config.data_types.get(column, "float")
            
            if data_type in ["int", "float", "normal"]:
                data[column] = self.generate_numeric_data(config.size, data_type, **config.constraints.get(column, {}))
            elif data_type == "categorical":
                categories = config.constraints.get(column, {}).get("categories", ["A", "B", "C"])
                data[column] = self.generate_categorical_data(config.size, categories, **config.constraints.get(column, {}))
            elif data_type == "text":
                data[column] = self.generate_text_data(config.size, **config.constraints.get(column, {}))
            elif data_type == "datetime":
                data[column] = self.generate_datetime_data(config.size, **config.constraints.get(column, {}))
            elif data_type == "boolean":
                data[column] = self.generate_boolean_data(config.size, **config.constraints.get(column, {}))
            else:
                raise ValueError(f"Unsupported data type: {data_type}")
        
        return pd.DataFrame(data)

class DataStorage:
    """ãƒ‡ãƒ¼ã‚¿ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç®¡ç†"""
    
    def __init__(self, base_dir: str = "test_data"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        self.logger = logging.getLogger(__name__)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        self.db_path = self.base_dir / "test_data.db"
        self._init_database()
    
    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS test_datasets (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE NOT NULL,
                description TEXT,
                data_type TEXT NOT NULL,
                file_path TEXT NOT NULL,
                size_bytes INTEGER,
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL,
                version TEXT NOT NULL,
                tags TEXT,
                metadata TEXT,
                checksum TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def save_dataset(self, dataset: TestDataSet) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO test_datasets 
                (name, description, data_type, file_path, size_bytes, created_at, updated_at, version, tags, metadata, checksum)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                dataset.name,
                dataset.description,
                dataset.data_type,
                dataset.file_path,
                dataset.size_bytes,
                dataset.created_at.isoformat(),
                dataset.updated_at.isoformat(),
                dataset.version,
                json.dumps(dataset.tags),
                json.dumps(dataset.metadata),
                dataset.checksum
            ))
            
            conn.commit()
            conn.close()
            
            self.logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜å®Œäº†: {dataset.name}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def load_dataset(self, name: str) -> Optional[TestDataSet]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('SELECT * FROM test_datasets WHERE name = ?', (name,))
            row = cursor.fetchone()
            
            conn.close()
            
            if row:
                return TestDataSet(
                    name=row[1],
                    description=row[2],
                    data_type=row[3],
                    file_path=row[4],
                    size_bytes=row[5],
                    created_at=datetime.fromisoformat(row[6]),
                    updated_at=datetime.fromisoformat(row[7]),
                    version=row[8],
                    tags=json.loads(row[9]) if row[9] else [],
                    metadata=json.loads(row[10]) if row[10] else {},
                    checksum=row[11]
                )
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def list_datasets(self, tags: Optional[List[str]] = None) -> List[TestDataSet]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§ã‚’å–å¾—"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            if tags:
                # ã‚¿ã‚°ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                placeholders = ','.join(['?' for _ in tags])
                cursor.execute(f'''
                    SELECT * FROM test_datasets 
                    WHERE tags LIKE '%{placeholders}%'
                ''', tags)
            else:
                cursor.execute('SELECT * FROM test_datasets')
            
            rows = cursor.fetchall()
            conn.close()
            
            datasets = []
            for row in rows:
                dataset = TestDataSet(
                    name=row[1],
                    description=row[2],
                    data_type=row[3],
                    file_path=row[4],
                    size_bytes=row[5],
                    created_at=datetime.fromisoformat(row[6]),
                    updated_at=datetime.fromisoformat(row[7]),
                    version=row[8],
                    tags=json.loads(row[9]) if row[9] else [],
                    metadata=json.loads(row[10]) if row[10] else {},
                    checksum=row[11]
                )
                datasets.append(dataset)
            
            return datasets
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def delete_dataset(self, name: str) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‰Šé™¤"""
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’å–å¾—
            dataset = self.load_dataset(name)
            if not dataset:
                return False
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            file_path = Path(dataset.file_path)
            if file_path.exists():
                file_path.unlink()
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å‰Šé™¤
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('DELETE FROM test_datasets WHERE name = ?', (name,))
            conn.commit()
            conn.close()
            
            self.logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‰Šé™¤å®Œäº†: {name}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
            return False

class DataSerializer:
    """ãƒ‡ãƒ¼ã‚¿ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¶ãƒ¼"""
    
    @staticmethod
    def save_dataframe(df: pd.DataFrame, file_path: str, data_type: str) -> bool:
        """DataFrameã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            if data_type == "csv":
                df.to_csv(file_path, index=False)
            elif data_type == "json":
                df.to_json(file_path, orient="records", indent=2)
            elif data_type == "pickle":
                df.to_pickle(file_path)
            elif data_type == "parquet":
                df.to_parquet(file_path, index=False)
            elif data_type == "excel":
                df.to_excel(file_path, index=False)
            else:
                raise ValueError(f"Unsupported data type: {data_type}")
            
            return True
            
        except Exception as e:
            logging.error(f"ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    @staticmethod
    def load_dataframe(file_path: str, data_type: str) -> Optional[pd.DataFrame]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰DataFrameã‚’èª­ã¿è¾¼ã¿"""
        try:
            if data_type == "csv":
                return pd.read_csv(file_path)
            elif data_type == "json":
                return pd.read_json(file_path)
            elif data_type == "pickle":
                return pd.read_pickle(file_path)
            elif data_type == "parquet":
                return pd.read_parquet(file_path)
            elif data_type == "excel":
                return pd.read_excel(file_path)
            else:
                raise ValueError(f"Unsupported data type: {data_type}")
            
        except Exception as e:
            logging.error(f"ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None

class TestDataManager:
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, base_dir: str = "test_data"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        self.logger = logging.getLogger(__name__)
        
        # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
        self.generator = DataGenerator()
        self.storage = DataStorage(base_dir)
        self.serializer = DataSerializer()
        
        # ãƒ­ãƒƒã‚¯ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ï¼‰
        self.lock = threading.Lock()
    
    def generate_test_data(self, config: DataGenerationConfig, name: str, description: str = "", tags: List[str] = None) -> Optional[TestDataSet]:
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"""
        try:
            with self.lock:
                # DataFrameã‚’ç”Ÿæˆ
                df = self.generator.generate_dataframe(config)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ±ºå®š
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"{name}_{timestamp}.{config.data_type}"
                file_path = self.base_dir / filename
                
                # ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                if not self.serializer.save_dataframe(df, str(file_path), config.data_type):
                    return None
                
                # ãƒã‚§ãƒƒã‚¯ã‚µãƒ ã‚’è¨ˆç®—
                checksum = self._calculate_checksum(file_path)
                
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’ä½œæˆ
                dataset = TestDataSet(
                    name=name,
                    description=description,
                    data_type=config.data_type,
                    file_path=str(file_path),
                    size_bytes=file_path.stat().st_size,
                    created_at=datetime.now(),
                    updated_at=datetime.now(),
                    version="1.0.0",
                    tags=tags or [],
                    metadata={
                        "size": config.size,
                        "columns": config.columns,
                        "data_types": config.data_types,
                        "constraints": config.constraints,
                        "seed": config.seed
                    },
                    checksum=checksum
                )
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
                if self.storage.save_dataset(dataset):
                    self.logger.info(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {name}")
                    return dataset
                else:
                    # å¤±æ•—ã—ãŸå ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
                    file_path.unlink()
                    return None
                
        except Exception as e:
            self.logger.error(f"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def load_test_data(self, name: str) -> Optional[pd.DataFrame]:
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            dataset = self.storage.load_dataset(name)
            if not dataset:
                return None
            
            # ãƒã‚§ãƒƒã‚¯ã‚µãƒ ã‚’æ¤œè¨¼
            current_checksum = self._calculate_checksum(Path(dataset.file_path))
            if current_checksum != dataset.checksum:
                self.logger.warning(f"ãƒã‚§ãƒƒã‚¯ã‚µãƒ ä¸ä¸€è‡´: {name}")
            
            return self.serializer.load_dataframe(dataset.file_path, dataset.data_type)
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def list_test_data(self, tags: Optional[List[str]] = None) -> List[TestDataSet]:
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä¸€è¦§ã‚’å–å¾—"""
        return self.storage.list_datasets(tags)
    
    def delete_test_data(self, name: str) -> bool:
        """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤"""
        return self.storage.delete_dataset(name)
    
    def cleanup_old_data(self, days: int = 30) -> int:
        """å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days)
            datasets = self.storage.list_datasets()
            
            deleted_count = 0
            for dataset in datasets:
                if dataset.created_at < cutoff_date:
                    if self.storage.delete_dataset(dataset.name):
                        deleted_count += 1
            
            self.logger.info(f"âœ… {deleted_count}ä»¶ã®å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
            return deleted_count
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0
    
    def export_data(self, name: str, export_path: str, export_format: str = "zip") -> bool:
        """ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        try:
            dataset = self.storage.load_dataset(name)
            if not dataset:
                return False
            
            if export_format == "zip":
                with zipfile.ZipFile(export_path, 'w') as zipf:
                    zipf.write(dataset.file_path, os.path.basename(dataset.file_path))
                    
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚å«ã‚ã‚‹
                    metadata = {
                        "dataset": dataset.__dict__,
                        "export_date": datetime.now().isoformat()
                    }
                    zipf.writestr("metadata.json", json.dumps(metadata, indent=2, default=str))
            
            self.logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {name} -> {export_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def import_data(self, import_path: str, name: str = None) -> Optional[TestDataSet]:
        """ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ"""
        try:
            if import_path.endswith('.zip'):
                with zipfile.ZipFile(import_path, 'r') as zipf:
                    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
                    metadata_content = zipf.read("metadata.json")
                    metadata = json.loads(metadata_content)
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡º
                    data_files = [f for f in zipf.namelist() if not f.endswith('.json')]
                    if not data_files:
                        return None
                    
                    data_file = data_files[0]
                    temp_dir = tempfile.mkdtemp()
                    zipf.extract(data_file, temp_dir)
                    
                    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’å¾©å…ƒ
                    dataset_dict = metadata["dataset"]
                    dataset_dict["created_at"] = datetime.fromisoformat(dataset_dict["created_at"])
                    dataset_dict["updated_at"] = datetime.fromisoformat(dataset_dict["updated_at"])
                    
                    dataset = TestDataSet(**dataset_dict)
                    
                    # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¨­å®š
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    new_filename = f"{dataset.name}_{timestamp}.{dataset.data_type}"
                    new_file_path = self.base_dir / new_filename
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç§»å‹•
                    shutil.move(os.path.join(temp_dir, data_file), new_file_path)
                    shutil.rmtree(temp_dir)
                    
                    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’æ›´æ–°
                    dataset.file_path = str(new_file_path)
                    dataset.size_bytes = new_file_path.stat().st_size
                    dataset.updated_at = datetime.now()
                    dataset.checksum = self._calculate_checksum(new_file_path)
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
                    if self.storage.save_dataset(dataset):
                        self.logger.info(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†: {dataset.name}")
                        return dataset
            
            return None
            
        except Exception as e:
            self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def _calculate_checksum(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚§ãƒƒã‚¯ã‚µãƒ ã‚’è¨ˆç®—"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception:
            return ""

class TestDataFactory:
    """ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼"""
    
    @staticmethod
    def create_sample_data_config() -> DataGenerationConfig:
        """ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿è¨­å®šã‚’ä½œæˆ"""
        return DataGenerationConfig(
            data_type="csv",
            size=1000,
            columns=["id", "name", "age", "salary", "department", "hire_date", "is_active"],
            data_types={
                "id": "int",
                "name": "text",
                "age": "int",
                "salary": "float",
                "department": "categorical",
                "hire_date": "datetime",
                "is_active": "boolean"
            },
            constraints={
                "id": {"min_val": 1, "max_val": 10000},
                "age": {"min_val": 18, "max_val": 65},
                "salary": {"min_val": 30000, "max_val": 150000},
                "department": {"categories": ["Engineering", "Sales", "Marketing", "HR", "Finance"]},
                "name": {"min_length": 5, "max_length": 15}
            },
            seed=42
        )
    
    @staticmethod
    def create_performance_test_config() -> DataGenerationConfig:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿è¨­å®šã‚’ä½œæˆ"""
        return DataGenerationConfig(
            data_type="csv",
            size=100000,
            columns=["id", "value", "category", "timestamp"],
            data_types={
                "id": "int",
                "value": "normal",
                "category": "categorical",
                "timestamp": "datetime"
            },
            constraints={
                "id": {"min_val": 1, "max_val": 1000000},
                "value": {"mean": 100, "std": 20},
                "category": {"categories": ["A", "B", "C", "D", "E"]},
                "timestamp": {"start_date": "2020-01-01", "end_date": "2024-12-31"}
            },
            seed=123
        )

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ èµ·å‹•")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
    manager = TestDataManager()
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    print("ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
    sample_config = TestDataFactory.create_sample_data_config()
    sample_dataset = manager.generate_test_data(
        sample_config,
        name="sample_employee_data",
        description="å¾“æ¥­å“¡ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«",
        tags=["sample", "employee", "hr"]
    )
    
    if sample_dataset:
        print(f"âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {sample_dataset.name}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        df = manager.load_test_data(sample_dataset.name)
        if df is not None:
            print(f"ğŸ“‹ ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
            print(f"ğŸ“‹ ã‚«ãƒ©ãƒ : {list(df.columns)}")
            print(f"ğŸ“‹ æœ€åˆã®5è¡Œ:")
            print(df.head())
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    print("\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆä¸­...")
    perf_config = TestDataFactory.create_performance_test_config()
    perf_dataset = manager.generate_test_data(
        perf_config,
        name="performance_test_data",
        description="ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿",
        tags=["performance", "large", "test"]
    )
    
    if perf_dataset:
        print(f"âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {perf_dataset.name}")
    
    # ãƒ‡ãƒ¼ã‚¿ä¸€è¦§ã‚’è¡¨ç¤º
    print("\nğŸ“‹ ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§:")
    datasets = manager.list_test_data()
    for dataset in datasets:
        print(f"  - {dataset.name}: {dataset.description} ({dataset.data_type}, {dataset.size_bytes} bytes)")
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆå¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ï¼‰
    print("\nğŸ§¹ å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­...")
    deleted_count = manager.cleanup_old_data(days=1)  # 1æ—¥ä»¥ä¸Šå¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
    print(f"âœ… {deleted_count}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    main() 