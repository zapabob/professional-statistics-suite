#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Automated Assumption Validation System
è‡ªå‹•çµ±è¨ˆçš„ä»®å®šæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™:
- çµ±è¨ˆæ‰‹æ³•ã®ä»®å®šã®è‡ªå‹•æ¤œè¨¼
- ä»®å®šé•åã®é‡è¦åº¦è©•ä¾¡
- ä»£æ›¿æ‰‹æ³•ã®ææ¡ˆ
- è©³ç´°ãªè¨ºæ–­ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import json
import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Optional
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime

# çµ±è¨ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from scipy import stats
from scipy.stats import (
    shapiro, normaltest, levene, chi2_contingency
)

# statsmodelsã‹ã‚‰Durbin-Watsonæ¤œå®šã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from statsmodels.stats.diagnostic import durbin_watson
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…
    def durbin_watson(residuals):
        """Durbin-Watsonçµ±è¨ˆé‡ã®ç°¡æ˜“è¨ˆç®—"""
        diff = np.diff(residuals)
        return np.sum(diff**2) / np.sum(residuals**2)
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
# ãƒ—ãƒ­ãƒƒãƒˆé–¢é€£ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    PLOTTING_AVAILABLE = True
except ImportError:
    PLOTTING_AVAILABLE = False
    plt = None
    sns = None

# è¨­å®šã¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
try:
    from config import check_feature_permission
    if not check_feature_permission('advanced_ai'):
        raise ImportError("Advanced AI features require Professional edition or higher")
except ImportError:
    def check_feature_permission(feature):
        return True

# çµ±è¨ˆæ‰‹æ³•ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼
try:
    from statistical_method_advisor import StatisticalMethod, DataType
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®šç¾©
    class StatisticalMethod(Enum):
        T_TEST_TWO_SAMPLE = "t_test_two_sample"
        ANOVA_ONE_WAY = "anova_one_way"
        LINEAR_REGRESSION = "linear_regression"
    
    class DataType(Enum):
        CONTINUOUS = "continuous"
        CATEGORICAL = "categorical"

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AssumptionType(Enum):
    """çµ±è¨ˆçš„ä»®å®šã®ç¨®é¡"""
    NORMALITY = "normality"
    HOMOSCEDASTICITY = "homoscedasticity"
    INDEPENDENCE = "independence"
    LINEARITY = "linearity"
    NO_MULTICOLLINEARITY = "no_multicollinearity"
    EXPECTED_FREQUENCY = "expected_frequency"
    RANDOM_SAMPLING = "random_sampling"
    ADDITIVITY = "additivity"

class ViolationSeverity(Enum):
    """ä»®å®šé•åã®é‡è¦åº¦"""
    NONE = "none"
    MILD = "mild"
    MODERATE = "moderate"
    SEVERE = "severe"
    CRITICAL = "critical"

@dataclass
class AssumptionTest:
    """ä»®å®šæ¤œè¨¼ãƒ†ã‚¹ãƒˆã®çµæœ"""
    assumption: AssumptionType
    test_name: str
    statistic: float
    p_value: float
    critical_value: Optional[float] = None
    is_violated: bool = False
    severity: ViolationSeverity = ViolationSeverity.NONE
    interpretation: str = ""
    recommendation: str = ""

@dataclass
class ValidationResult:
    """ä»®å®šæ¤œè¨¼ã®ç·åˆçµæœ"""
    method: StatisticalMethod
    assumptions_tested: List[AssumptionTest]
    overall_validity: bool
    severity_summary: Dict[ViolationSeverity, int]
    alternative_methods: List[StatisticalMethod] = field(default_factory=list)
    diagnostic_plots: List[str] = field(default_factory=list)
    detailed_report: str = ""

class AssumptionValidator:
    """è‡ªå‹•çµ±è¨ˆçš„ä»®å®šæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, alpha: float = 0.05, plot_diagnostics: bool = True):
        self.logger = logging.getLogger(f"{__name__}.AssumptionValidator")
        self.alpha = alpha  # æœ‰æ„æ°´æº–
        self.plot_diagnostics = plot_diagnostics
        
        # ä»®å®šæ¤œè¨¼ãƒ«ãƒ¼ãƒ«ã®åˆæœŸåŒ–
        self.validation_rules = self._initialize_validation_rules()
        
        # é‡è¦åº¦åˆ¤å®šåŸºæº–
        self.severity_thresholds = {
            ViolationSeverity.MILD: 0.01,
            ViolationSeverity.MODERATE: 0.001,
            ViolationSeverity.SEVERE: 0.0001,
            ViolationSeverity.CRITICAL: 0.00001
        }
        
        self.logger.info("AssumptionValidatoråˆæœŸåŒ–å®Œäº†")
    
    def _initialize_validation_rules(self) -> Dict[StatisticalMethod, List[AssumptionType]]:
        """çµ±è¨ˆæ‰‹æ³•ã”ã¨ã®ä»®å®šæ¤œè¨¼ãƒ«ãƒ¼ãƒ«"""
        return {
            StatisticalMethod.T_TEST_TWO_SAMPLE: [
                AssumptionType.NORMALITY,
                AssumptionType.HOMOSCEDASTICITY,
                AssumptionType.INDEPENDENCE
            ],
            StatisticalMethod.ANOVA_ONE_WAY: [
                AssumptionType.NORMALITY,
                AssumptionType.HOMOSCEDASTICITY,
                AssumptionType.INDEPENDENCE
            ],
            StatisticalMethod.LINEAR_REGRESSION: [
                AssumptionType.LINEARITY,
                AssumptionType.INDEPENDENCE,
                AssumptionType.HOMOSCEDASTICITY,
                AssumptionType.NORMALITY,
                AssumptionType.NO_MULTICOLLINEARITY
            ]
        }
    
    def validate_assumptions(self, data: pd.DataFrame, method: StatisticalMethod,
                           target_variable: str, predictor_variables: List[str] = None,
                           group_variable: str = None) -> ValidationResult:
        """çµ±è¨ˆæ‰‹æ³•ã®ä»®å®šã‚’åŒ…æ‹¬çš„ã«æ¤œè¨¼"""
        self.logger.info(f"ä»®å®šæ¤œè¨¼é–‹å§‹: {method.value}")
        
        # å¿…è¦ãªä»®å®šã‚’å–å¾—
        required_assumptions = self.validation_rules.get(method, [])
        
        # å„ä»®å®šã‚’ãƒ†ã‚¹ãƒˆ
        assumption_tests = []
        for assumption in required_assumptions:
            test_result = self._test_assumption(
                assumption, data, target_variable, 
                predictor_variables, group_variable, method
            )
            assumption_tests.append(test_result)
        
        # ç·åˆè©•ä¾¡
        overall_validity = all(not test.is_violated for test in assumption_tests)
        
        # é‡è¦åº¦ã‚µãƒãƒªãƒ¼
        severity_summary = {severity: 0 for severity in ViolationSeverity}
        for test in assumption_tests:
            severity_summary[test.severity] += 1
        
        # ä»£æ›¿æ‰‹æ³•ã®ææ¡ˆ
        alternative_methods = self._suggest_alternatives(method, assumption_tests)
        
        # è¨ºæ–­ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆ
        diagnostic_plots = []
        if self.plot_diagnostics:
            diagnostic_plots = self._generate_diagnostic_plots(
                data, method, target_variable, predictor_variables, group_variable
            )
        
        # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        detailed_report = self._generate_detailed_report(
            method, assumption_tests, overall_validity
        )
        
        result = ValidationResult(
            method=method,
            assumptions_tested=assumption_tests,
            overall_validity=overall_validity,
            severity_summary=severity_summary,
            alternative_methods=alternative_methods,
            diagnostic_plots=diagnostic_plots,
            detailed_report=detailed_report
        )
        
        self.logger.info(f"ä»®å®šæ¤œè¨¼å®Œäº†: æœ‰åŠ¹æ€§={overall_validity}")
        return result
    
    def _test_assumption(self, assumption: AssumptionType, data: pd.DataFrame,
                        target_variable: str, predictor_variables: List[str] = None,
                        group_variable: str = None, method: StatisticalMethod = None) -> AssumptionTest:
        """å€‹åˆ¥ã®ä»®å®šã‚’ãƒ†ã‚¹ãƒˆ"""
        
        if assumption == AssumptionType.NORMALITY:
            return self._test_normality(data, target_variable, group_variable)
        
        elif assumption == AssumptionType.HOMOSCEDASTICITY:
            return self._test_homoscedasticity(data, target_variable, group_variable, predictor_variables)
        
        elif assumption == AssumptionType.INDEPENDENCE:
            return self._test_independence(data, target_variable)
        
        elif assumption == AssumptionType.LINEARITY:
            return self._test_linearity(data, target_variable, predictor_variables)
        
        elif assumption == AssumptionType.NO_MULTICOLLINEARITY:
            return self._test_multicollinearity(data, predictor_variables)
        
        elif assumption == AssumptionType.EXPECTED_FREQUENCY:
            return self._test_expected_frequency(data, target_variable, group_variable)
        
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ä»®å®šãƒ†ã‚¹ãƒˆ
            return AssumptionTest(
                assumption=assumption,
                test_name="Not Implemented",
                statistic=0.0,
                p_value=1.0,
                is_violated=False,
                severity=ViolationSeverity.NONE,
                interpretation="ã“ã®ãƒ†ã‚¹ãƒˆã¯å®Ÿè£…ã•ã‚Œã¦ã„ã¾ã›ã‚“",
                recommendation="æ‰‹å‹•ã§ç¢ºèªã—ã¦ãã ã•ã„"
            )
    
    def _test_normality(self, data: pd.DataFrame, target_variable: str, 
                       group_variable: str = None) -> AssumptionTest:
        """æ­£è¦æ€§ã®æ¤œå®š"""
        try:
            if group_variable:
                # ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã®æ­£è¦æ€§æ¤œå®š
                groups = data.groupby(group_variable)[target_variable]
                p_values = []
                statistics = []
                
                for name, group in groups:
                    clean_group = group.dropna()
                    if len(clean_group) >= 8:
                        if len(clean_group) <= 5000:
                            stat, p_val = shapiro(clean_group)
                        else:
                            stat, p_val = normaltest(clean_group)
                        statistics.append(stat)
                        p_values.append(p_val)
                
                if p_values:
                    # æœ€å°på€¤ã‚’ä½¿ç”¨ï¼ˆæœ€ã‚‚å³ã—ã„çµæœï¼‰
                    min_p_value = min(p_values)
                    avg_statistic = np.mean(statistics)
                    test_name = "Shapiro-Wilk (grouped)"
                else:
                    min_p_value = 1.0
                    avg_statistic = 0.0
                    test_name = "Normality test (insufficient data)"
            
            else:
                # å˜ä¸€å¤‰æ•°ã®æ­£è¦æ€§æ¤œå®š
                clean_data = data[target_variable].dropna()
                
                if len(clean_data) < 8:
                    return AssumptionTest(
                        assumption=AssumptionType.NORMALITY,
                        test_name="Normality test (insufficient data)",
                        statistic=0.0,
                        p_value=1.0,
                        is_violated=False,
                        severity=ViolationSeverity.NONE,
                        interpretation="ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒä¸è¶³ã—ã¦ã„ã¾ã™",
                        recommendation="ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦ãã ã•ã„"
                    )
                
                if len(clean_data) <= 5000:
                    avg_statistic, min_p_value = shapiro(clean_data)
                    test_name = "Shapiro-Wilk"
                else:
                    avg_statistic, min_p_value = normaltest(clean_data)
                    test_name = "D'Agostino-Pearson"
            
            # ä»®å®šé•åã®åˆ¤å®š
            is_violated = min_p_value < self.alpha
            severity = self._determine_severity(min_p_value)
            
            # è§£é‡ˆã¨æ¨å¥¨äº‹é …
            if is_violated:
                interpretation = f"æ­£è¦æ€§ãŒæ£„å´ã•ã‚Œã¾ã—ãŸ (p={min_p_value:.4f})"
                recommendation = "ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®šã¾ãŸã¯å¤‰æ•°å¤‰æ›ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
            else:
                interpretation = f"æ­£è¦æ€§ãŒæ”¯æŒã•ã‚Œã¾ã—ãŸ (p={min_p_value:.4f})"
                recommendation = "ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®šã‚’ä½¿ç”¨ã§ãã¾ã™"
            
            return AssumptionTest(
                assumption=AssumptionType.NORMALITY,
                test_name=test_name,
                statistic=avg_statistic,
                p_value=min_p_value,
                is_violated=is_violated,
                severity=severity,
                interpretation=interpretation,
                recommendation=recommendation
            )
            
        except Exception as e:
            self.logger.error(f"æ­£è¦æ€§æ¤œå®šã‚¨ãƒ©ãƒ¼: {e}")
            return AssumptionTest(
                assumption=AssumptionType.NORMALITY,
                test_name="Normality test (error)",
                statistic=0.0,
                p_value=1.0,
                is_violated=False,
                severity=ViolationSeverity.NONE,
                interpretation=f"æ¤œå®šå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}",
                recommendation="ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
            )
    
    def _test_homoscedasticity(self, data: pd.DataFrame, target_variable: str,
                              group_variable: str = None, predictor_variables: List[str] = None) -> AssumptionTest:
        """ç­‰åˆ†æ•£æ€§ã®æ¤œå®š"""
        try:
            if group_variable:
                # ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®ç­‰åˆ†æ•£æ€§æ¤œå®š
                groups = [group[target_variable].dropna() for name, group in data.groupby(group_variable)]
                
                if len(groups) < 2:
                    return AssumptionTest(
                        assumption=AssumptionType.HOMOSCEDASTICITY,
                        test_name="Homoscedasticity test (insufficient groups)",
                        statistic=0.0,
                        p_value=1.0,
                        is_violated=False,
                        severity=ViolationSeverity.NONE,
                        interpretation="ã‚°ãƒ«ãƒ¼ãƒ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™",
                        recommendation="è¤‡æ•°ã®ã‚°ãƒ«ãƒ¼ãƒ—ãŒå¿…è¦ã§ã™"
                    )
                
                # Leveneæ¤œå®šã‚’ä½¿ç”¨
                statistic, p_value = levene(*groups)
                test_name = "Levene's test"
                
            elif predictor_variables:
                # å›å¸°åˆ†æã§ã®ç­‰åˆ†æ•£æ€§æ¤œå®šï¼ˆæ®‹å·®åˆ†æï¼‰
                X = data[predictor_variables].dropna()
                y = data[target_variable].dropna()
                
                # å…±é€šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
                common_index = X.index.intersection(y.index)
                X = X.loc[common_index]
                y = y.loc[common_index]
                
                if len(X) < 10:
                    return AssumptionTest(
                        assumption=AssumptionType.HOMOSCEDASTICITY,
                        test_name="Homoscedasticity test (insufficient data)",
                        statistic=0.0,
                        p_value=1.0,
                        is_violated=False,
                        severity=ViolationSeverity.NONE,
                        interpretation="ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™",
                        recommendation="ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™"
                    )
                
                # ç·šå½¢å›å¸°ã‚’å®Ÿè¡Œ
                model = LinearRegression()
                model.fit(X, y)
                residuals = y - model.predict(X)
                
                # Breusch-Paganæ¤œå®šã®ç°¡æ˜“ç‰ˆ
                # æ®‹å·®ã®äºŒä¹—ã‚’äºˆæ¸¬å€¤ã§å›å¸°
                fitted_values = model.predict(X)
                residuals_squared = residuals ** 2
                
                bp_model = LinearRegression()
                bp_model.fit(fitted_values.reshape(-1, 1), residuals_squared)
                bp_predictions = bp_model.predict(fitted_values.reshape(-1, 1))
                
                # Fçµ±è¨ˆé‡ã®è¨ˆç®—
                ss_reg = np.sum((bp_predictions - np.mean(residuals_squared)) ** 2)
                ss_res = np.sum((residuals_squared - bp_predictions) ** 2)
                
                df_reg = 1
                df_res = len(residuals_squared) - 2
                
                if ss_res > 0:
                    f_statistic = (ss_reg / df_reg) / (ss_res / df_res)
                    p_value = 1 - stats.f.cdf(f_statistic, df_reg, df_res)
                else:
                    f_statistic = 0.0
                    p_value = 1.0
                
                statistic = f_statistic
                test_name = "Breusch-Pagan test (simplified)"
                
            else:
                return AssumptionTest(
                    assumption=AssumptionType.HOMOSCEDASTICITY,
                    test_name="Homoscedasticity test (no groups or predictors)",
                    statistic=0.0,
                    p_value=1.0,
                    is_violated=False,
                    severity=ViolationSeverity.NONE,
                    interpretation="ã‚°ãƒ«ãƒ¼ãƒ—å¤‰æ•°ã¾ãŸã¯äºˆæ¸¬å¤‰æ•°ãŒå¿…è¦ã§ã™",
                    recommendation="é©åˆ‡ãªå¤‰æ•°ã‚’æŒ‡å®šã—ã¦ãã ã•ã„"
                )
            
            # ä»®å®šé•åã®åˆ¤å®š
            is_violated = p_value < self.alpha
            severity = self._determine_severity(p_value)
            
            # è§£é‡ˆã¨æ¨å¥¨äº‹é …
            if is_violated:
                interpretation = f"ç­‰åˆ†æ•£æ€§ãŒæ£„å´ã•ã‚Œã¾ã—ãŸ (p={p_value:.4f})"
                recommendation = "Welchã®tæ¤œå®šã‚„é‡ã¿ä»˜ãæœ€å°äºŒä¹—æ³•ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
            else:
                interpretation = f"ç­‰åˆ†æ•£æ€§ãŒæ”¯æŒã•ã‚Œã¾ã—ãŸ (p={p_value:.4f})"
                recommendation = "æ¨™æº–çš„ãªæ¤œå®šã‚’ä½¿ç”¨ã§ãã¾ã™"
            
            return AssumptionTest(
                assumption=AssumptionType.HOMOSCEDASTICITY,
                test_name=test_name,
                statistic=statistic,
                p_value=p_value,
                is_violated=is_violated,
                severity=severity,
                interpretation=interpretation,
                recommendation=recommendation
            )
            
        except Exception as e:
            self.logger.error(f"ç­‰åˆ†æ•£æ€§æ¤œå®šã‚¨ãƒ©ãƒ¼: {e}")
            return AssumptionTest(
                assumption=AssumptionType.HOMOSCEDASTICITY,
                test_name="Homoscedasticity test (error)",
                statistic=0.0,
                p_value=1.0,
                is_violated=False,
                severity=ViolationSeverity.NONE,
                interpretation=f"æ¤œå®šå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}",
                recommendation="ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
            )
    
    def _test_independence(self, data: pd.DataFrame, target_variable: str) -> AssumptionTest:
        """ç‹¬ç«‹æ€§ã®æ¤œå®šï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        try:
            clean_data = data[target_variable].dropna()
            
            if len(clean_data) < 20:
                return AssumptionTest(
                    assumption=AssumptionType.INDEPENDENCE,
                    test_name="Independence test (insufficient data)",
                    statistic=0.0,
                    p_value=1.0,
                    is_violated=False,
                    severity=ViolationSeverity.NONE,
                    interpretation="ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™",
                    recommendation="ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™"
                )
            
            # Durbin-Watsonæ¤œå®šï¼ˆæ™‚ç³»åˆ—ã®è‡ªå·±ç›¸é–¢ï¼‰
            # ãƒ‡ãƒ¼ã‚¿ãŒæ™‚ç³»åˆ—é †ã«ä¸¦ã‚“ã§ã„ã‚‹ã¨ä»®å®š
            dw_statistic = durbin_watson(clean_data)
            
            # DWçµ±è¨ˆé‡ã®è§£é‡ˆï¼ˆ2ã«è¿‘ã„ã»ã©ç‹¬ç«‹æ€§ãŒé«˜ã„ï¼‰
            if 1.5 <= dw_statistic <= 2.5:
                is_violated = False
                severity = ViolationSeverity.NONE
                interpretation = f"ç‹¬ç«‹æ€§ãŒæ”¯æŒã•ã‚Œã¾ã—ãŸ (DW={dw_statistic:.3f})"
                recommendation = "ãƒ‡ãƒ¼ã‚¿ã¯ç‹¬ç«‹ã—ã¦ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™"
            else:
                is_violated = True
                if dw_statistic < 1.5:
                    severity = ViolationSeverity.MODERATE
                    interpretation = f"æ­£ã®è‡ªå·±ç›¸é–¢ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ (DW={dw_statistic:.3f})"
                else:
                    severity = ViolationSeverity.MILD
                    interpretation = f"è² ã®è‡ªå·±ç›¸é–¢ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ (DW={dw_statistic:.3f})"
                recommendation = "æ™‚ç³»åˆ—åˆ†ææ‰‹æ³•ã‚„ä¸€èˆ¬åŒ–æœ€å°äºŒä¹—æ³•ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
            
            # på€¤ã®è¿‘ä¼¼è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            p_value = 2 * min(abs(dw_statistic - 2), 2 - abs(dw_statistic - 2)) / 2
            
            return AssumptionTest(
                assumption=AssumptionType.INDEPENDENCE,
                test_name="Durbin-Watson test",
                statistic=dw_statistic,
                p_value=p_value,
                is_violated=is_violated,
                severity=severity,
                interpretation=interpretation,
                recommendation=recommendation
            )
            
        except Exception as e:
            self.logger.error(f"ç‹¬ç«‹æ€§æ¤œå®šã‚¨ãƒ©ãƒ¼: {e}")
            return AssumptionTest(
                assumption=AssumptionType.INDEPENDENCE,
                test_name="Independence test (error)",
                statistic=0.0,
                p_value=1.0,
                is_violated=False,
                severity=ViolationSeverity.NONE,
                interpretation=f"æ¤œå®šå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}",
                recommendation="ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
            )
    
    def _test_linearity(self, data: pd.DataFrame, target_variable: str,
                       predictor_variables: List[str]) -> AssumptionTest:
        """ç·šå½¢æ€§ã®æ¤œå®š"""
        try:
            if not predictor_variables:
                return AssumptionTest(
                    assumption=AssumptionType.LINEARITY,
                    test_name="Linearity test (no predictors)",
                    statistic=0.0,
                    p_value=1.0,
                    is_violated=False,
                    severity=ViolationSeverity.NONE,
                    interpretation="äºˆæ¸¬å¤‰æ•°ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“",
                    recommendation="äºˆæ¸¬å¤‰æ•°ã‚’æŒ‡å®šã—ã¦ãã ã•ã„"
                )
            
            X = data[predictor_variables].dropna()
            y = data[target_variable].dropna()
            
            # å…±é€šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å–å¾—
            common_index = X.index.intersection(y.index)
            X = X.loc[common_index]
            y = y.loc[common_index]
            
            if len(X) < 10:
                return AssumptionTest(
                    assumption=AssumptionType.LINEARITY,
                    test_name="Linearity test (insufficient data)",
                    statistic=0.0,
                    p_value=1.0,
                    is_violated=False,
                    severity=ViolationSeverity.NONE,
                    interpretation="ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™",
                    recommendation="ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™"
                )
            
            # ç·šå½¢å›å¸°ãƒ¢ãƒ‡ãƒ«
            linear_model = LinearRegression()
            linear_model.fit(X, y)
            linear_predictions = linear_model.predict(X)
            linear_mse = mean_squared_error(y, linear_predictions)
            
            # å¤šé …å¼å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆ2æ¬¡ï¼‰
            from sklearn.preprocessing import PolynomialFeatures
            poly_features = PolynomialFeatures(degree=2)
            X_poly = poly_features.fit_transform(X)
            
            poly_model = LinearRegression()
            poly_model.fit(X_poly, y)
            poly_predictions = poly_model.predict(X_poly)
            poly_mse = mean_squared_error(y, poly_predictions)
            
            # Fæ¤œå®šã«ã‚ˆã‚‹ç·šå½¢æ€§ã®æ¤œå®š
            n = len(y)
            p_linear = X.shape[1]
            p_poly = X_poly.shape[1]
            
            if poly_mse > 0 and p_poly > p_linear:
                f_statistic = ((linear_mse - poly_mse) / (p_poly - p_linear)) / (poly_mse / (n - p_poly))
                p_value = 1 - stats.f.cdf(f_statistic, p_poly - p_linear, n - p_poly)
            else:
                f_statistic = 0.0
                p_value = 1.0
            
            # ä»®å®šé•åã®åˆ¤å®š
            is_violated = p_value < self.alpha
            severity = self._determine_severity(p_value)
            
            # è§£é‡ˆã¨æ¨å¥¨äº‹é …
            if is_violated:
                interpretation = f"éç·šå½¢æ€§ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ (p={p_value:.4f})"
                recommendation = "å¤šé …å¼å›å¸°ã‚„éç·šå½¢å›å¸°ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
            else:
                interpretation = f"ç·šå½¢æ€§ãŒæ”¯æŒã•ã‚Œã¾ã—ãŸ (p={p_value:.4f})"
                recommendation = "ç·šå½¢å›å¸°ã‚’ä½¿ç”¨ã§ãã¾ã™"
            
            return AssumptionTest(
                assumption=AssumptionType.LINEARITY,
                test_name="Linearity F-test",
                statistic=f_statistic,
                p_value=p_value,
                is_violated=is_violated,
                severity=severity,
                interpretation=interpretation,
                recommendation=recommendation
            )
            
        except Exception as e:
            self.logger.error(f"ç·šå½¢æ€§æ¤œå®šã‚¨ãƒ©ãƒ¼: {e}")
            return AssumptionTest(
                assumption=AssumptionType.LINEARITY,
                test_name="Linearity test (error)",
                statistic=0.0,
                p_value=1.0,
                is_violated=False,
                severity=ViolationSeverity.NONE,
                interpretation=f"æ¤œå®šå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}",
                recommendation="ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
            )
    
    def _test_multicollinearity(self, data: pd.DataFrame, predictor_variables: List[str]) -> AssumptionTest:
        """å¤šé‡å…±ç·šæ€§ã®æ¤œå®š"""
        try:
            if not predictor_variables or len(predictor_variables) < 2:
                return AssumptionTest(
                    assumption=AssumptionType.NO_MULTICOLLINEARITY,
                    test_name="Multicollinearity test (insufficient predictors)",
                    statistic=0.0,
                    p_value=1.0,
                    is_violated=False,
                    severity=ViolationSeverity.NONE,
                    interpretation="äºˆæ¸¬å¤‰æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™",
                    recommendation="è¤‡æ•°ã®äºˆæ¸¬å¤‰æ•°ãŒå¿…è¦ã§ã™"
                )
            
            X = data[predictor_variables].dropna()
            
            if len(X) < len(predictor_variables) + 5:
                return AssumptionTest(
                    assumption=AssumptionType.NO_MULTICOLLINEARITY,
                    test_name="Multicollinearity test (insufficient data)",
                    statistic=0.0,
                    p_value=1.0,
                    is_violated=False,
                    severity=ViolationSeverity.NONE,
                    interpretation="ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™",
                    recommendation="ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™"
                )
            
            # ç›¸é–¢è¡Œåˆ—ã®è¨ˆç®—
            corr_matrix = X.corr()
            
            # æœ€å¤§ç›¸é–¢ä¿‚æ•°ã‚’å–å¾—ï¼ˆå¯¾è§’æˆåˆ†ã‚’é™¤ãï¼‰
            corr_values = corr_matrix.values
            np.fill_diagonal(corr_values, 0)
            max_correlation = np.max(np.abs(corr_values))
            
            # VIFï¼ˆåˆ†æ•£æ‹¡å¤§è¦å› ï¼‰ã®è¨ˆç®—
            from sklearn.linear_model import LinearRegression
            vif_values = []
            
            for i, var in enumerate(predictor_variables):
                # ä»–ã®å¤‰æ•°ã§ã“ã®å¤‰æ•°ã‚’äºˆæ¸¬
                other_vars = [v for j, v in enumerate(predictor_variables) if j != i]
                if len(other_vars) > 0:
                    X_others = X[other_vars]
                    y_var = X[var]
                    
                    model = LinearRegression()
                    model.fit(X_others, y_var)
                    r_squared = model.score(X_others, y_var)
                    
                    if r_squared < 0.999:  # å®Œå…¨ãªå¤šé‡å…±ç·šæ€§ã‚’é¿ã‘ã‚‹
                        vif = 1 / (1 - r_squared)
                    else:
                        vif = float('inf')
                    
                    vif_values.append(vif)
            
            max_vif = max(vif_values) if vif_values else 1.0
            
            # å¤šé‡å…±ç·šæ€§ã®åˆ¤å®š
            # VIF > 10 ã¾ãŸã¯ ç›¸é–¢ä¿‚æ•° > 0.8 ã§å•é¡Œã¨ã™ã‚‹
            is_violated = max_vif > 10 or max_correlation > 0.8
            
            if max_vif > 100:
                severity = ViolationSeverity.CRITICAL
            elif max_vif > 50:
                severity = ViolationSeverity.SEVERE
            elif max_vif > 10:
                severity = ViolationSeverity.MODERATE
            elif max_correlation > 0.8:
                severity = ViolationSeverity.MILD
            else:
                severity = ViolationSeverity.NONE
            
            # è§£é‡ˆã¨æ¨å¥¨äº‹é …
            if is_violated:
                interpretation = f"å¤šé‡å…±ç·šæ€§ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ (æœ€å¤§VIF={max_vif:.2f}, æœ€å¤§ç›¸é–¢={max_correlation:.3f})"
                recommendation = "å¤‰æ•°é¸æŠã€ä¸»æˆåˆ†åˆ†æã€ã¾ãŸã¯ãƒªãƒƒã‚¸å›å¸°ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
            else:
                interpretation = f"å¤šé‡å…±ç·šæ€§ã¯å•é¡Œã‚ã‚Šã¾ã›ã‚“ (æœ€å¤§VIF={max_vif:.2f}, æœ€å¤§ç›¸é–¢={max_correlation:.3f})"
                recommendation = "ç¾åœ¨ã®å¤‰æ•°ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã§ãã¾ã™"
            
            # på€¤ã®è¿‘ä¼¼ï¼ˆVIFãƒ™ãƒ¼ã‚¹ï¼‰
            p_value = max(0.001, 1 / max_vif) if max_vif > 1 else 1.0
            
            return AssumptionTest(
                assumption=AssumptionType.NO_MULTICOLLINEARITY,
                test_name="VIF and Correlation test",
                statistic=max_vif,
                p_value=p_value,
                is_violated=is_violated,
                severity=severity,
                interpretation=interpretation,
                recommendation=recommendation
            )
            
        except Exception as e:
            self.logger.error(f"å¤šé‡å…±ç·šæ€§æ¤œå®šã‚¨ãƒ©ãƒ¼: {e}")
            return AssumptionTest(
                assumption=AssumptionType.NO_MULTICOLLINEARITY,
                test_name="Multicollinearity test (error)",
                statistic=0.0,
                p_value=1.0,
                is_violated=False,
                severity=ViolationSeverity.NONE,
                interpretation=f"æ¤œå®šå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}",
                recommendation="ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
            )
    
    def _test_expected_frequency(self, data: pd.DataFrame, target_variable: str,
                                group_variable: str) -> AssumptionTest:
        """æœŸå¾…åº¦æ•°ã®æ¤œå®šï¼ˆã‚«ã‚¤äºŒä¹—æ¤œå®šç”¨ï¼‰"""
        try:
            if not group_variable:
                return AssumptionTest(
                    assumption=AssumptionType.EXPECTED_FREQUENCY,
                    test_name="Expected frequency test (no group variable)",
                    statistic=0.0,
                    p_value=1.0,
                    is_violated=False,
                    severity=ViolationSeverity.NONE,
                    interpretation="ã‚°ãƒ«ãƒ¼ãƒ—å¤‰æ•°ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“",
                    recommendation="ã‚°ãƒ«ãƒ¼ãƒ—å¤‰æ•°ã‚’æŒ‡å®šã—ã¦ãã ã•ã„"
                )
            
            # ã‚¯ãƒ­ã‚¹é›†è¨ˆè¡¨ã®ä½œæˆ
            contingency_table = pd.crosstab(data[target_variable], data[group_variable])
            
            # ã‚«ã‚¤äºŒä¹—æ¤œå®šã®å®Ÿè¡Œ
            chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)
            
            # æœŸå¾…åº¦æ•°ã®æœ€å°å€¤
            min_expected = np.min(expected)
            
            # æœŸå¾…åº¦æ•°ãŒ5æœªæº€ã®ã‚»ãƒ«ã®å‰²åˆ
            cells_below_5 = np.sum(expected < 5) / expected.size
            
            # ä»®å®šé•åã®åˆ¤å®š
            # æœŸå¾…åº¦æ•°ãŒ5æœªæº€ã®ã‚»ãƒ«ãŒ20%ã‚’è¶…ãˆã‚‹å ´åˆã¯å•é¡Œ
            is_violated = cells_below_5 > 0.2 or min_expected < 1
            
            if min_expected < 1:
                severity = ViolationSeverity.SEVERE
            elif cells_below_5 > 0.5:
                severity = ViolationSeverity.MODERATE
            elif cells_below_5 > 0.2:
                severity = ViolationSeverity.MILD
            else:
                severity = ViolationSeverity.NONE
            
            # è§£é‡ˆã¨æ¨å¥¨äº‹é …
            if is_violated:
                interpretation = f"æœŸå¾…åº¦æ•°ãŒä¸ååˆ†ã§ã™ (æœ€å°æœŸå¾…åº¦æ•°={min_expected:.2f}, 5æœªæº€ã®å‰²åˆ={cells_below_5:.1%})"
                recommendation = "Fisherã®æ­£ç¢ºæ¤œå®šã‚„ã‚«ãƒ†ã‚´ãƒªã®çµ±åˆã‚’æ¤œè¨ã—ã¦ãã ã•ã„"
            else:
                interpretation = f"æœŸå¾…åº¦æ•°ã¯ååˆ†ã§ã™ (æœ€å°æœŸå¾…åº¦æ•°={min_expected:.2f}, 5æœªæº€ã®å‰²åˆ={cells_below_5:.1%})"
                recommendation = "ã‚«ã‚¤äºŒä¹—æ¤œå®šã‚’ä½¿ç”¨ã§ãã¾ã™"
            
            return AssumptionTest(
                assumption=AssumptionType.EXPECTED_FREQUENCY,
                test_name="Expected frequency test",
                statistic=min_expected,
                p_value=cells_below_5,  # é•åå‰²åˆã‚’på€¤ã¨ã—ã¦ä½¿ç”¨
                is_violated=is_violated,
                severity=severity,
                interpretation=interpretation,
                recommendation=recommendation
            )
            
        except Exception as e:
            self.logger.error(f"æœŸå¾…åº¦æ•°æ¤œå®šã‚¨ãƒ©ãƒ¼: {e}")
            return AssumptionTest(
                assumption=AssumptionType.EXPECTED_FREQUENCY,
                test_name="Expected frequency test (error)",
                statistic=0.0,
                p_value=1.0,
                is_violated=False,
                severity=ViolationSeverity.NONE,
                interpretation=f"æ¤œå®šå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}",
                recommendation="ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
            )
    
    def _determine_severity(self, p_value: float) -> ViolationSeverity:
        """på€¤ã«åŸºã¥ãé•åé‡è¦åº¦ã®åˆ¤å®š"""
        if p_value >= self.alpha:
            return ViolationSeverity.NONE
        
        for severity, threshold in self.severity_thresholds.items():
            if p_value >= threshold:
                return severity
        
        return ViolationSeverity.CRITICAL
    
    def _suggest_alternatives(self, method: StatisticalMethod, 
                            assumption_tests: List[AssumptionTest]) -> List[StatisticalMethod]:
        """ä»®å®šé•åã«åŸºã¥ãä»£æ›¿æ‰‹æ³•ã®ææ¡ˆ"""
        alternatives = []
        
        # é•åã—ãŸä»®å®šã‚’ç‰¹å®š
        violated_assumptions = [test.assumption for test in assumption_tests if test.is_violated]
        
        if method == StatisticalMethod.T_TEST_TWO_SAMPLE:
            if AssumptionType.NORMALITY in violated_assumptions:
                alternatives.append("mann_whitney")
            if AssumptionType.HOMOSCEDASTICITY in violated_assumptions:
                alternatives.append("welch_t_test")
        
        elif method == StatisticalMethod.ANOVA_ONE_WAY:
            if AssumptionType.NORMALITY in violated_assumptions:
                alternatives.append("kruskal_wallis")
            if AssumptionType.HOMOSCEDASTICITY in violated_assumptions:
                alternatives.append("welch_anova")
        
        elif method == StatisticalMethod.LINEAR_REGRESSION:
            if AssumptionType.LINEARITY in violated_assumptions:
                alternatives.append("polynomial_regression")
                alternatives.append("gam")
            if AssumptionType.HOMOSCEDASTICITY in violated_assumptions:
                alternatives.append("weighted_least_squares")
            if AssumptionType.NO_MULTICOLLINEARITY in violated_assumptions:
                alternatives.append("ridge_regression")
                alternatives.append("lasso_regression")
        
        return alternatives
    
    def _generate_diagnostic_plots(self, data: pd.DataFrame, method: StatisticalMethod,
                                  target_variable: str, predictor_variables: List[str] = None,
                                  group_variable: str = None) -> List[str]:
        """è¨ºæ–­ãƒ—ãƒ­ãƒƒãƒˆã®ç”Ÿæˆ"""
        plot_files = []
        
        try:
            if not self.plot_diagnostics or not PLOTTING_AVAILABLE:
                return plot_files
            
            # ãƒ—ãƒ­ãƒƒãƒˆã®ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            plot_dir = Path("diagnostic_plots")
            plot_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # æ­£è¦æ€§è¨ºæ–­ãƒ—ãƒ­ãƒƒãƒˆ
            if method in [StatisticalMethod.T_TEST_TWO_SAMPLE, StatisticalMethod.ANOVA_ONE_WAY]:
                fig, axes = plt.subplots(1, 2, figsize=(12, 5))
                
                # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
                data[target_variable].hist(bins=30, ax=axes[0])
                axes[0].set_title('Distribution of Target Variable')
                axes[0].set_xlabel(target_variable)
                
                # Q-Qãƒ—ãƒ­ãƒƒãƒˆ
                stats.probplot(data[target_variable].dropna(), dist="norm", plot=axes[1])
                axes[1].set_title('Q-Q Plot')
                
                plot_file = plot_dir / f"normality_diagnostic_{timestamp}.png"
                plt.savefig(plot_file)
                plt.close()
                plot_files.append(str(plot_file))
            
            # å›å¸°è¨ºæ–­ãƒ—ãƒ­ãƒƒãƒˆ
            elif method == StatisticalMethod.LINEAR_REGRESSION and predictor_variables:
                X = data[predictor_variables].dropna()
                y = data[target_variable].dropna()
                
                common_index = X.index.intersection(y.index)
                X = X.loc[common_index]
                y = y.loc[common_index]
                
                if len(X) > 5:
                    model = LinearRegression()
                    model.fit(X, y)
                    predictions = model.predict(X)
                    residuals = y - predictions
                    
                    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
                    
                    # æ®‹å·®ãƒ—ãƒ­ãƒƒãƒˆ
                    axes[0, 0].scatter(predictions, residuals)
                    axes[0, 0].axhline(y=0, color='r', linestyle='--')
                    axes[0, 0].set_title('Residuals vs Fitted')
                    axes[0, 0].set_xlabel('Fitted Values')
                    axes[0, 0].set_ylabel('Residuals')
                    
                    # æ­£è¦Q-Qãƒ—ãƒ­ãƒƒãƒˆ
                    stats.probplot(residuals, dist="norm", plot=axes[0, 1])
                    axes[0, 1].set_title('Normal Q-Q Plot of Residuals')
                    
                    # ã‚¹ã‚±ãƒ¼ãƒ«-ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ—ãƒ­ãƒƒãƒˆ
                    standardized_residuals = np.sqrt(np.abs(residuals / np.std(residuals)))
                    axes[1, 0].scatter(predictions, standardized_residuals)
                    axes[1, 0].set_title('Scale-Location Plot')
                    axes[1, 0].set_xlabel('Fitted Values')
                    axes[1, 0].set_ylabel('âˆš|Standardized Residuals|')
                    
                    # æ®‹å·®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
                    axes[1, 1].hist(residuals, bins=20)
                    axes[1, 1].set_title('Residuals Distribution')
                    axes[1, 1].set_xlabel('Residuals')
                    
                    plt.tight_layout()
                    plot_file = plot_dir / f"regression_diagnostic_{timestamp}.png"
                    plt.savefig(plot_file)
                    plt.close()
                    plot_files.append(str(plot_file))
            
        except Exception as e:
            self.logger.error(f"è¨ºæ–­ãƒ—ãƒ­ãƒƒãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        
        return plot_files
    
    def _generate_detailed_report(self, method: StatisticalMethod, 
                                assumption_tests: List[AssumptionTest],
                                overall_validity: bool) -> str:
        """è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        report_lines = []
        
        report_lines.append("çµ±è¨ˆæ‰‹æ³•ä»®å®šæ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ")
        report_lines.append("=" * 50)
        report_lines.append(f"æ¤œè¨¼å¯¾è±¡æ‰‹æ³•: {method.value}")
        report_lines.append(f"æ¤œè¨¼æ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"ç·åˆåˆ¤å®š: {'æœ‰åŠ¹' if overall_validity else 'è¦æ³¨æ„'}")
        report_lines.append("")
        
        report_lines.append("å€‹åˆ¥ä»®å®šæ¤œè¨¼çµæœ:")
        report_lines.append("-" * 30)
        
        for test in assumption_tests:
            report_lines.append(f"ä»®å®š: {test.assumption.value}")
            report_lines.append(f"  æ¤œå®š: {test.test_name}")
            report_lines.append(f"  çµ±è¨ˆé‡: {test.statistic:.4f}")
            report_lines.append(f"  på€¤: {test.p_value:.4f}")
            report_lines.append(f"  é•å: {'ã¯ã„' if test.is_violated else 'ã„ã„ãˆ'}")
            report_lines.append(f"  é‡è¦åº¦: {test.severity.value}")
            report_lines.append(f"  è§£é‡ˆ: {test.interpretation}")
            report_lines.append(f"  æ¨å¥¨: {test.recommendation}")
            report_lines.append("")
        
        return "\n".join(report_lines)
    
    def export_validation_report(self, result: ValidationResult, filepath: str) -> bool:
        """æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        try:
            export_data = {
                "method": result.method.value,
                "validation_timestamp": datetime.now().isoformat(),
                "overall_validity": bool(result.overall_validity),
                "severity_summary": {k.value: int(v) for k, v in result.severity_summary.items()},
                "assumptions_tested": [
                    {
                        "assumption": test.assumption.value,
                        "test_name": test.test_name,
                        "statistic": float(test.statistic),
                        "p_value": float(test.p_value),
                        "is_violated": bool(test.is_violated),
                        "severity": test.severity.value,
                        "interpretation": test.interpretation,
                        "recommendation": test.recommendation
                    }
                    for test in result.assumptions_tested
                ],
                "alternative_methods": result.alternative_methods,
                "diagnostic_plots": result.diagnostic_plots,
                "detailed_report": result.detailed_report
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ: {filepath}")
            return True
            
        except Exception as e:
            self.logger.error(f"ãƒ¬ãƒãƒ¼ãƒˆã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False

def create_sample_data_for_assumption_testing() -> pd.DataFrame:
    """ä»®å®šæ¤œè¨¼ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
    np.random.seed(42)
    
    n = 100
    
    # æ­£è¦åˆ†å¸ƒãƒ‡ãƒ¼ã‚¿
    normal_data = np.random.normal(50, 10, n)
    
    # éæ­£è¦åˆ†å¸ƒãƒ‡ãƒ¼ã‚¿ï¼ˆæŒ‡æ•°åˆ†å¸ƒï¼‰
    non_normal_data = np.random.exponential(2, n)
    
    # ã‚°ãƒ«ãƒ¼ãƒ—å¤‰æ•°
    groups = np.random.choice(['A', 'B', 'C'], n)
    
    # äºˆæ¸¬å¤‰æ•°ï¼ˆå¤šé‡å…±ç·šæ€§ã‚ã‚Šï¼‰
    x1 = np.random.normal(0, 1, n)
    x2 = 2 * x1 + np.random.normal(0, 0.1, n)  # x1ã¨é«˜ã„ç›¸é–¢
    x3 = np.random.normal(0, 1, n)
    
    # ç›®çš„å¤‰æ•°ï¼ˆç·šå½¢é–¢ä¿‚ï¼‰
    y_linear = 2 * x1 + 3 * x3 + np.random.normal(0, 1, n)
    
    # ç›®çš„å¤‰æ•°ï¼ˆéç·šå½¢é–¢ä¿‚ï¼‰
    y_nonlinear = x1**2 + x3 + np.random.normal(0, 1, n)
    
    return pd.DataFrame({
        'normal_var': normal_data,
        'non_normal_var': non_normal_data,
        'group': groups,
        'x1': x1,
        'x2': x2,
        'x3': x3,
        'y_linear': y_linear,
        'y_nonlinear': y_nonlinear
    })

if __name__ == '__main__':
    # ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    print("ğŸ” Assumption Validator ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("=" * 60)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
    print("\nğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ...")
    sample_data = create_sample_data_for_assumption_testing()
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {sample_data.shape}")
    print(f"å¤‰æ•°: {list(sample_data.columns)}")
    
    # ãƒãƒªãƒ‡ãƒ¼ã‚¿ãƒ¼ã®åˆæœŸåŒ–
    print("\nğŸ”§ AssumptionValidatoråˆæœŸåŒ–...")
    validator = AssumptionValidator(alpha=0.05, plot_diagnostics=False)
    
    # tæ¤œå®šã®ä»®å®šæ¤œè¨¼
    print("\nğŸ§ª tæ¤œå®šã®ä»®å®šæ¤œè¨¼...")
    t_test_result = validator.validate_assumptions(
        data=sample_data,
        method=StatisticalMethod.T_TEST_TWO_SAMPLE,
        target_variable='normal_var',
        group_variable='group'
    )
    
    print(f"ç·åˆæœ‰åŠ¹æ€§: {t_test_result.overall_validity}")
    print(f"é‡è¦åº¦ã‚µãƒãƒªãƒ¼: {t_test_result.severity_summary}")
    
    for test in t_test_result.assumptions_tested:
        print(f"\nä»®å®š: {test.assumption.value}")
        print(f"  æ¤œå®š: {test.test_name}")
        print(f"  på€¤: {test.p_value:.4f}")
        print(f"  é•å: {test.is_violated}")
        print(f"  é‡è¦åº¦: {test.severity.value}")
    
    # ç·šå½¢å›å¸°ã®ä»®å®šæ¤œè¨¼
    print("\nğŸ“ˆ ç·šå½¢å›å¸°ã®ä»®å®šæ¤œè¨¼...")
    regression_result = validator.validate_assumptions(
        data=sample_data,
        method=StatisticalMethod.LINEAR_REGRESSION,
        target_variable='y_linear',
        predictor_variables=['x1', 'x2', 'x3']
    )
    
    print(f"ç·åˆæœ‰åŠ¹æ€§: {regression_result.overall_validity}")
    print(f"ä»£æ›¿æ‰‹æ³•: {regression_result.alternative_methods}")
    
    for test in regression_result.assumptions_tested:
        print(f"\nä»®å®š: {test.assumption.value}")
        print(f"  æ¤œå®š: {test.test_name}")
        print(f"  çµ±è¨ˆé‡: {test.statistic:.4f}")
        print(f"  é•å: {test.is_violated}")
        print(f"  æ¨å¥¨: {test.recommendation}")
    
    print("\nâœ… ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†ï¼")