#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
AI Integration Module - 2025 Latest Edition
AIçµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« - 2025å¹´æœ€æ–°ç‰ˆï¼ˆãƒãƒ«ãƒLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã€è‡ªå·±ä¿®æ­£ã€RAGã€ãƒ­ãƒ¼ã‚«ãƒ«LLMå¯¾å¿œï¼‰
"""

import asyncio
import base64
import re
import json
import os
import platform
import time
from typing import Optional, Dict, Any, List, Union, Literal, Tuple
from pathlib import Path
import traceback
import logging
from datetime import datetime
import io
from contextlib import redirect_stdout
from enum import Enum
import uuid
import glob
import signal
import sys

# Data processing
import pandas as pd
import numpy as np
import requests

# AI API clients
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    from lmstudio import LMStudioClient
    LMSTUDIO_AVAILABLE = True
except ImportError:
    LMSTUDIO_AVAILABLE = False

# GGUF direct support
try:
    import llama_cpp
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    import google.generativeai as genai
    GOOGLE_AI_AVAILABLE = True
except ImportError:
    GOOGLE_AI_AVAILABLE = False

# RAG and Vector DB
try:
    import faiss
    from sentence_transformers import SentenceTransformer
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False

# Image processing
try:
    from PIL import Image
    import cv2
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

# OCR engines
PYTESSERACT_AVAILABLE = False
EASYOCR_AVAILABLE = False
try:
    import pytesseract
    PYTESSERACT_AVAILABLE = True
except ImportError:
    pass
try:
    import easyocr
    EASYOCR_AVAILABLE = True
except ImportError:
    pass

# IntentType Enum for AI Orchestrator
class IntentType(Enum):
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼æ„å›³ã®åˆ†é¡"""
    DESCRIPTIVE = "descriptive"
    INFERENTIAL = "inferential"
    PREDICTIVE = "predictive"
    EXPLORATORY = "exploratory"
    EDUCATIONAL = "educational"

# Configuration
class AIConfig:
    """AIè¨­å®šã‚¯ãƒ©ã‚¹ï¼ˆ2025å¹´ç‰ˆï¼‰"""
    def __init__(self):
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.anthropic_api_key = os.getenv("ANTHROPIC_API_KEY")
        self.google_api_key = os.getenv("GOOGLE_API_KEY")
        self.together_api_key = os.getenv("TOGETHER_API_KEY")
        self.ollama_base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        self.lmstudio_base_url = os.getenv("LMSTUDIO_BASE_URL", "http://localhost:1234/v1")
        self.koboldcpp_base_url = os.getenv("KOBOLDCPP_BASE_URL", "http://localhost:5001/v1")
        self.ocr_languages = ["eng", "jpn"]
        self.tesseract_cmd = None
        self.default_provider = "openai"
        self.default_model = "gpt-4o"
        self.max_tokens = 4096
        self.temperature = 0.1
        self.max_correction_attempts = 2
        self._load_from_config()

    def _load_from_config(self):
        try:
            from config import ai_config as external_config
            for key in self.__dict__:
                if hasattr(external_config, key):
                    setattr(self, key, getattr(external_config, key) or getattr(self, key))
        except (ImportError, AttributeError):
            pass

    def is_api_configured(self, provider: str) -> bool:
        key_map = {
            "openai": self.openai_api_key,
            "anthropic": self.anthropic_api_key,
            "google": self.google_api_key,
            "together": self.together_api_key,
            "ollama": True,
            "lmstudio": True,
            "koboldcpp": True
        }
        return bool(key_map.get(provider))

ai_config = AIConfig()

# Model Definitions
LLM_MODELS = {
    "openai": {"gpt-4o": {}, "o3": {}, "gpt-4-turbo": {}},
    "anthropic": {"claude-3-7-sonnet-20250219": {}, "claude-3-5-sonnet-20240620": {}, "claude-3-opus-20240229": {}},
    "google": {"gemini-2.5-pro-latest": {}, "gemini-2.5-flash-latest": {}},
    "together": {"meta-llama/Llama-3.1-70b-chat-hf": {}, "meta-llama/Llama-3.1-8b-chat-hf": {}},
    "ollama": {"llama3.1": {}, "gemma2": {}},
    "lmstudio": {"local-model/gguf-model": {}},
    "koboldcpp": {"local-model/gguf-model": {}} # Models depend on what is loaded
}

# ... (Prompt Templates and KnowledgeBase class remain the same) ...

class LLMProvider:
    """LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    def __init__(self, provider_name: str, api_key: Optional[str] = None):
        self.provider_name = provider_name
        self.api_key = api_key
        self.logger = logging.getLogger(f"{__name__}.{provider_name}")
    
    async def generate_response(self, prompt: str, model: str, **kwargs) -> Dict[str, Any]:
        raise NotImplementedError

class OpenAIProvider(LLMProvider):
    """OpenAI GPT ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""
    def __init__(self, api_key: str):
        super().__init__("openai", api_key)
        if OPENAI_AVAILABLE:
            self.client = openai.OpenAI(api_key=api_key)
    
    async def generate_response(self, prompt: str, model: str = "gpt-3.5-turbo", **kwargs) -> Dict[str, Any]:
        try:
            if not OPENAI_AVAILABLE:
                return {"success": False, "error": "OpenAI not available"}
            
            response = self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                **kwargs
            )
            
            return {
                "success": True,
                "content": response.choices[0].message.content,
                "tokens": response.usage.total_tokens if response.usage else 0,
                "provider": "openai"
            }
        except Exception as e:
            return {"success": False, "error": str(e), "provider": "openai"}

class GoogleProvider(LLMProvider):
    """Google Gemini ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆAPIè¨­å®šè‡ªå‹•æ¤œå‡ºæ©Ÿèƒ½å¼·åŒ–ç‰ˆï¼‰"""
    def __init__(self, api_key: str = None):
        super().__init__("google", api_key)
        self.api_key = self._get_api_key(api_key)
        self.logger = logging.getLogger(f"{__name__}.GoogleProvider")
        
        if self.api_key and GOOGLE_AI_AVAILABLE:
            try:
                genai.configure(api_key=self.api_key)
                self.logger.info("Google AI APIè¨­å®šå®Œäº†")
            except Exception as e:
                self.logger.error(f"Google AI APIè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
    
    def _get_api_key(self, api_key: str = None) -> str:
        """API keyã®è‡ªå‹•æ¤œå‡º"""
        # 1. å¼•æ•°ã§æŒ‡å®šã•ã‚ŒãŸAPI key
        if api_key:
            return api_key
        
        # 2. ç’°å¢ƒå¤‰æ•°ã‹ã‚‰æ¤œç´¢
        env_vars = [
            'GOOGLE_API_KEY',
            'GOOGLE_AI_API_KEY', 
            'GEMINI_API_KEY',
            'GOOGLE_API_KEY_STATISTICS'
        ]
        
        for env_var in env_vars:
            api_key = os.getenv(env_var)
            if api_key:
                self.logger.info(f"ç’°å¢ƒå¤‰æ•° {env_var} ã‹ã‚‰API keyã‚’æ¤œå‡º")
                return api_key
        
        # 3. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ¤œç´¢
        config_files = [
            'config.json',
            'ai_config.json',
            'google_config.json',
            '.env'
        ]
        
        for config_file in config_files:
            try:
                if os.path.exists(config_file):
                    with open(config_file, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                        if 'google_api_key' in config:
                            self.logger.info(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« {config_file} ã‹ã‚‰API keyã‚’æ¤œå‡º")
                            return config['google_api_key']
                        elif 'api_key' in config:
                            self.logger.info(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« {config_file} ã‹ã‚‰API keyã‚’æ¤œå‡º")
                            return config['api_key']
            except Exception as e:
                self.logger.debug(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« {config_file} ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # 4. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®API keyï¼ˆé–‹ç™ºç”¨ï¼‰
        default_key = "AIzaSyBxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"  # ãƒ€ãƒŸãƒ¼ã‚­ãƒ¼
        self.logger.warning("API keyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ­ãƒ¼ã‚«ãƒ«LLMã¸ã®åˆ‡ã‚Šæ›¿ãˆã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        return default_key
    
    def is_available(self) -> bool:
        """åˆ©ç”¨å¯èƒ½æ€§ã®ç¢ºèª"""
        try:
            if not self.api_key or self.api_key == "AIzaSyBxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx":
                return False
            
            if not GOOGLE_AI_AVAILABLE:
                return False
            
            # API keyã®å½¢å¼ãƒã‚§ãƒƒã‚¯
            if not self.api_key.startswith("AIza"):
                return False
            
            return True
        except Exception as e:
            self.logger.error(f"Google Provideråˆ©ç”¨å¯èƒ½æ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    async def generate_response(self, prompt: str, model: str = "gemini-1.5-pro-latest", **kwargs) -> Dict[str, Any]:
        try:
            if not self.is_available():
                return {
                    "success": False, 
                    "error": "Google AI APIãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ­ãƒ¼ã‚«ãƒ«LLMã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚", 
                    "provider": "google",
                    "suggested_fallback": "ollama"
                }
            
            # éåŒæœŸå®Ÿè¡Œã®ãŸã‚ã«asyncioã‚’ä½¿ç”¨
            import asyncio
            
            def _generate_sync():
                try:
                    model_instance = genai.GenerativeModel(model)
                    response = model_instance.generate_content(prompt)
                    return response
                except Exception as e:
                    self.logger.error(f"Google AI APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
                    raise
            
            # åŒæœŸé–¢æ•°ã‚’éåŒæœŸã§å®Ÿè¡Œ
            response = await asyncio.get_event_loop().run_in_executor(None, _generate_sync)
            
            return {
                "success": True,
                "content": response.text,
                "tokens": len(response.text.split()),  # æ¦‚ç®—
                "provider": "google",
                "model_used": model
            }
        except Exception as e:
            error_msg = str(e)
            self.logger.error(f"Google AI API ã‚¨ãƒ©ãƒ¼: {error_msg}")
            
            # ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡ã«å¿œã˜ãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ææ¡ˆ
            if "API key not valid" in error_msg or "API_KEY_INVALID" in error_msg:
                fallback_suggestion = "ollama"
                error_msg += " â†’ ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆOllamaï¼‰ã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™"
            elif "quota" in error_msg.lower():
                fallback_suggestion = "lmstudio"
                error_msg += " â†’ ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆLM Studioï¼‰ã®ä½¿ç”¨ã‚’æ¨å¥¨ã—ã¾ã™"
            else:
                fallback_suggestion = "ollama"
                error_msg += " â†’ ãƒ­ãƒ¼ã‚«ãƒ«LLMã¸ã®åˆ‡ã‚Šæ›¿ãˆã‚’æ¨å¥¨ã—ã¾ã™"
            
            return {
                "success": False, 
                "error": error_msg, 
                "provider": "google",
                "suggested_fallback": fallback_suggestion
            }

class AnthropicProvider(LLMProvider):
    """Anthropic Claude ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""
    def __init__(self, api_key: str):
        super().__init__("anthropic", api_key)
        if ANTHROPIC_AVAILABLE:
            self.client = anthropic.Anthropic(api_key=api_key)
    
    async def generate_response(self, prompt: str, model: str = "claude-3-sonnet-20240229", **kwargs) -> Dict[str, Any]:
        try:
            if not ANTHROPIC_AVAILABLE:
                return {"success": False, "error": "Anthropic not available"}
            
            response = self.client.messages.create(
                model=model,
                max_tokens=kwargs.get('max_tokens', 1000),
                messages=[{"role": "user", "content": prompt}]
            )
            
            return {
                "success": True,
                "content": response.content[0].text,
                "tokens": response.usage.input_tokens + response.usage.output_tokens,
                "provider": "anthropic"
            }
        except Exception as e:
            return {"success": False, "error": str(e), "provider": "anthropic"}

class TogetherProvider(LLMProvider):
    """Together AI ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""
    def __init__(self, api_key: str):
        super().__init__("together", api_key)
        self.api_key = api_key
    
    async def generate_response(self, prompt: str, model: str = "meta-llama/Llama-2-7b-chat-hf", **kwargs) -> Dict[str, Any]:
        try:
            headers = {"Authorization": f"Bearer {self.api_key}"}
            data = {
                "model": model,
                "prompt": prompt,
                "max_tokens": kwargs.get('max_tokens', 1000)
            }
            
            response = requests.post("https://api.together.xyz/inference", headers=headers, json=data)
            result = response.json()
            
            return {
                "success": True,
                "content": result.get("output", {}).get("choices", [{}])[0].get("text", ""),
                "tokens": result.get("output", {}).get("usage", {}).get("total_tokens", 0),
                "provider": "together"
            }
        except Exception as e:
            return {"success": False, "error": str(e), "provider": "together"}

class OllamaProvider(LLMProvider):
    """Ollama ãƒ­ãƒ¼ã‚«ãƒ«LLM ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""
    def __init__(self, base_url: str = "http://localhost:11434"):
        super().__init__("ollama")
        self.base_url = base_url
    
    async def generate_response(self, prompt: str, model: str = "llama2", **kwargs) -> Dict[str, Any]:
        try:
            data = {
                "model": model,
                "prompt": prompt,
                "stream": False
            }
            
            response = requests.post(f"{self.base_url}/api/generate", json=data)
            result = response.json()
            
            return {
                "success": True,
                "content": result.get("response", ""),
                "tokens": len(result.get("response", "").split()),
                "provider": "ollama"
            }
        except Exception as e:
            return {"success": False, "error": str(e), "provider": "ollama"}

class LMStudioProvider(LLMProvider):
    """LM Studio ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆGGUFå¯¾å¿œå¼·åŒ–ç‰ˆ + çµ±è¨ˆè£œåŠ©æ©Ÿèƒ½çµ±åˆï¼‰"""
    def __init__(self, base_url: str = "http://localhost:1234", models_dir: str = "./models"):
        super().__init__("lmstudio")
        self.base_url = base_url
        self.models_dir = Path(models_dir)
        self.available_models = []
        self.current_model = None
        self.logger = logging.getLogger(__name__)
        
        # çµ±è¨ˆè£œåŠ©æ©Ÿèƒ½ã®åˆæœŸåŒ–
        self.statistical_assistant = None
        self.statistical_methods_db = self._load_statistical_methods_db()
        
        # modelsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        self._scan_gguf_models()
    
    def _scan_gguf_models(self) -> List[str]:
        """modelsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³"""
        gguf_files = []
        
        if self.models_dir.exists():
            # .ggufãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢
            gguf_files = list(self.models_dir.rglob('*.gguf'))
            self.available_models = [str(f.relative_to(self.models_dir)) for f in gguf_files]
            
            if self.available_models:
                self.logger.info(f"ğŸ” {len(self.available_models)}å€‹ã®GGUFãƒ¢ãƒ‡ãƒ«ã‚’ç™ºè¦‹: {self.available_models}")
            else:
                self.logger.warning(f"âš ï¸ {self.models_dir}ã«GGUFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ã¸ã‚“")
        else:
            self.logger.warning(f"âš ï¸ modelsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã›ã¸ã‚“: {self.models_dir}")
        
        return self.available_models
    
    def scan_custom_directory(self, directory: str) -> List[str]:
        """æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³"""
        try:
            path = Path(directory)
            if not path.exists():
                self.logger.warning(f"âš ï¸ æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {directory}")
                return []
            
            # .ggufãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢
            gguf_files = list(path.rglob('*.gguf'))
            custom_models = [str(f.relative_to(path)) for f in gguf_files]
            
            if custom_models:
                self.logger.info(f"ğŸ” ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰{len(custom_models)}å€‹ã®GGUFãƒ¢ãƒ‡ãƒ«ã‚’ç™ºè¦‹: {custom_models}")
                # ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ 
                self.available_models.extend(custom_models)
            else:
                self.logger.warning(f"âš ï¸ {directory}ã«GGUFãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            return custom_models
            
        except Exception as e:
            self.logger.error(f"âŒ ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚¹ã‚­ãƒ£ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def get_available_models(self) -> List[str]:
        """åˆ©ç”¨å¯èƒ½ãªGGUFãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—"""
        return self.available_models
    
    def get_model_info(self, model_name: str = None) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—"""
        if not model_name and self.available_models:
            model_name = self.available_models[0]  # æœ€åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        
        if not model_name:
            return {"error": "åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“"}
        
        model_path = self.models_dir / model_name
        if model_path.exists():
            stat = model_path.stat()
            return {
                "name": model_name,
                "path": str(model_path),
                "size_mb": round(stat.st_size / (1024 * 1024), 2),
                "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                "provider": "lmstudio"
            }
        
        return {"error": f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ã¸ã‚“: {model_name}"}
    
    async def load_model(self, model_name: str = None) -> Dict[str, Any]:
        """LM Studioã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        if not model_name and self.available_models:
            model_name = self.available_models[0]
        
        if not model_name:
            return {"success": False, "error": "èª­ã¿è¾¼ã‚€ãƒ¢ãƒ‡ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã¸ã‚“"}
        
        try:
            # LM Studio APIã§ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆå®Ÿéš›ã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«å¿œã˜ã¦èª¿æ•´ï¼‰
            headers = {"Content-Type": "application/json"}
            data = {"model": model_name}
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿APIï¼ˆLM Studioã®å®Ÿéš›ã®APIã«åˆã‚ã›ã¦èª¿æ•´ãŒå¿…è¦ï¼‰
            response = requests.post(f"{self.base_url}/v1/models/load", headers=headers, json=data)
            
            if response.status_code == 200:
                self.current_model = model_name
                self.logger.info(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {model_name}")
                return {"success": True, "model": model_name}
            else:
                return {"success": False, "error": f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {response.text}"}
                
        except Exception as e:
            self.logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}
    
    async def generate_response(self, prompt: str, model: str = None, **kwargs) -> Dict[str, Any]:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆçµ±è¨ˆå­¦å°‚ç”¨æœ€é©åŒ–ï¼‰"""
        try:
            # ãƒ¢ãƒ‡ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ãªã„å ´åˆã¯ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯æœ€åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
            if not model:
                model = self.current_model or (self.available_models[0] if self.available_models else "local-model")
            
            headers = {"Content-Type": "application/json"}
            
            # çµ±è¨ˆå­¦å°‚ç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            system_prompt = """ã‚ãªãŸã¯çµ±è¨ˆå­¦ã®å°‚é–€å®¶ã§ã™ã€‚æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§çµ±è¨ˆå­¦ã®æ¦‚å¿µã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
æ•°å¼ã‚„å…·ä½“ä¾‹ã‚’å«ã‚ã¦ã€å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"""
            
            data = {
                "model": model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                "temperature": kwargs.get('temperature', 0.3),  # çµ±è¨ˆå­¦ã§ã¯æ­£ç¢ºæ€§é‡è¦–
                "max_tokens": kwargs.get('max_tokens', 2048),
                "top_p": kwargs.get('top_p', 0.9),
                "frequency_penalty": kwargs.get('frequency_penalty', 0.1)
            }
            
            response = requests.post(f"{self.base_url}/v1/chat/completions", headers=headers, json=data)
            
            if response.status_code != 200:
                return {"success": False, "error": f"API ã‚¨ãƒ©ãƒ¼: {response.status_code}", "provider": "lmstudio"}
            
            result = response.json()
            
            content = result.get("choices", [{}])[0].get("message", {}).get("content", "")
            tokens = result.get("usage", {}).get("total_tokens", len(content.split()))
            
            return {
                "success": True,
                "content": content,
                "text": content,  # gguf_test_helper.pyã¨ã®äº’æ›æ€§
                "tokens": tokens,
                "tokens_consumed": tokens,  # gguf_test_helper.pyã¨ã®äº’æ›æ€§
                "processing_time": 0.1,  # æ¦‚ç®—å€¤
                "model": model,
                "provider": "lmstudio"
            }
            
        except Exception as e:
            self.logger.error(f"âŒ LM Studioç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e), "provider": "lmstudio"}
    
    def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """åŒæœŸç‰ˆã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆgguf_test_helper.pyã¨ã®äº’æ›æ€§ï¼‰"""
        import asyncio
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(self.generate_response(prompt, **kwargs))
    
    def is_available(self) -> bool:
        """LM Studioã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            response = requests.get(f"{self.base_url}/v1/models", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def _load_statistical_methods_db(self) -> Dict[str, Dict[str, Any]]:
        """çµ±è¨ˆæ‰‹æ³•ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿"""
        return {
            "t_test": {
                "name": "tæ¤œå®š",
                "description": "2ç¾¤ã®å¹³å‡å€¤ã®å·®ã‚’æ¤œå®š",
                "assumptions": ["æ­£è¦åˆ†å¸ƒ", "ç­‰åˆ†æ•£æ€§", "ç‹¬ç«‹æ€§"],
                "use_cases": ["2ç¾¤ã®æ¯”è¼ƒ", "å‰å¾Œæ¯”è¼ƒ"],
                "python_code": "from scipy import stats\nresult = stats.ttest_ind(group1, group2)"
            },
            "chi_square": {
                "name": "ã‚«ã‚¤äºŒä¹—æ¤œå®š",
                "description": "ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ç‹¬ç«‹æ€§ã‚’æ¤œå®š",
                "assumptions": ["ç‹¬ç«‹æ€§", "æœŸå¾…åº¦æ•°"],
                "use_cases": ["åˆ†å‰²è¡¨ã®åˆ†æ", "é©åˆåº¦æ¤œå®š"],
                "python_code": "from scipy import stats\nresult = stats.chi2_contingency(contingency_table)"
            },
            "correlation": {
                "name": "ç›¸é–¢åˆ†æ",
                "description": "2å¤‰æ•°é–“ã®é–¢ä¿‚æ€§ã‚’åˆ†æ",
                "assumptions": ["ç·šå½¢é–¢ä¿‚", "æ­£è¦åˆ†å¸ƒ"],
                "use_cases": ["é–¢ä¿‚æ€§ã®æ¢ç´¢", "äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«"],
                "python_code": "import numpy as np\ncorrelation = np.corrcoef(var1, var2)[0,1]"
            },
            "regression": {
                "name": "å›å¸°åˆ†æ",
                "description": "å¾“å±å¤‰æ•°ã‚’ç‹¬ç«‹å¤‰æ•°ã§äºˆæ¸¬",
                "assumptions": ["ç·šå½¢æ€§", "ç‹¬ç«‹æ€§", "ç­‰åˆ†æ•£æ€§", "æ­£è¦æ€§"],
                "use_cases": ["äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«", "å› æœé–¢ä¿‚ã®æ¢ç´¢"],
                "python_code": "from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X, y)"
            },
            "anova": {
                "name": "åˆ†æ•£åˆ†æ",
                "description": "3ç¾¤ä»¥ä¸Šã®å¹³å‡å€¤ã®å·®ã‚’æ¤œå®š",
                "assumptions": ["æ­£è¦åˆ†å¸ƒ", "ç­‰åˆ†æ•£æ€§", "ç‹¬ç«‹æ€§"],
                "use_cases": ["å¤šç¾¤æ¯”è¼ƒ", "å®Ÿé¨“åŠ¹æœã®æ¤œå®š"],
                "python_code": "from scipy import stats\nresult = stats.f_oneway(*groups)"
            },
            "mann_whitney": {
                "name": "ãƒãƒ³ãƒ›ã‚¤ãƒƒãƒˆãƒ‹ãƒ¼æ¤œå®š",
                "description": "ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãª2ç¾¤æ¯”è¼ƒ",
                "assumptions": ["ç‹¬ç«‹æ€§", "é€£ç¶šãƒ‡ãƒ¼ã‚¿"],
                "use_cases": ["æ­£è¦åˆ†å¸ƒã—ãªã„ãƒ‡ãƒ¼ã‚¿ã®æ¯”è¼ƒ"],
                "python_code": "from scipy import stats\nresult = stats.mannwhitneyu(group1, group2)"
            }
        }
    
    async def analyze_statistical_query(self, query: str, data_info: Optional[Dict[str, Any]] = None, 
                                      user_expertise: str = "intermediate") -> Dict[str, Any]:
        """çµ±è¨ˆã‚¯ã‚¨ãƒªã‚’åˆ†æï¼ˆçµ±è¨ˆè£œåŠ©æ©Ÿèƒ½ï¼‰"""
        try:
            # ã‚¯ã‚¨ãƒªã®æ„å›³ã‚’åˆ†é¡
            intent = self._classify_statistical_intent(query)
            
            # çµ±è¨ˆå­¦å°‚ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            prompt = self._build_statistical_prompt(query, intent, data_info, user_expertise)
            
            # LMStudioã§æ¨è«–å®Ÿè¡Œ
            response = await self.generate_response(prompt)
            
            # å¿œç­”ã‚’è§£æ
            parsed_response = self._parse_statistical_response(response.get('content', ''))
            
            return {
                "success": True,
                "answer": parsed_response.get('answer', response.get('content', '')),
                "confidence": parsed_response.get('confidence', 0.7),
                "suggested_methods": parsed_response.get('suggested_methods', []),
                "educational_content": parsed_response.get('educational_content'),
                "code_example": parsed_response.get('code_example'),
                "intent": intent,
                "processing_time": response.get('processing_time', 0.0),
                "tokens_used": response.get('tokens_consumed', 0)
            }
            
        except Exception as e:
            self.logger.error(f"çµ±è¨ˆã‚¯ã‚¨ãƒªåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "success": False,
                "error": str(e),
                "answer": f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                "confidence": 0.0,
                "suggested_methods": [],
                "processing_time": 0.0,
                "tokens_used": 0
            }
    
    def _classify_statistical_intent(self, query: str) -> str:
        """çµ±è¨ˆã‚¯ã‚¨ãƒªã®æ„å›³ã‚’åˆ†é¡"""
        query_lower = query.lower()
        
        # è¨˜è¿°çµ±è¨ˆ
        if any(word in query_lower for word in ['å¹³å‡', 'ä¸­å¤®å€¤', 'åˆ†æ•£', 'æ¨™æº–åå·®', 'åˆ†å¸ƒ', 'è¦ç´„', 'è¨˜è¿°']):
            return "descriptive"
        
        # æ¨è«–çµ±è¨ˆ
        elif any(word in query_lower for word in ['æ¤œå®š', 'tæ¤œå®š', 'ã‚«ã‚¤äºŒä¹—', 'ç›¸é–¢', 'å›å¸°', 'æœ‰æ„', 'ä»®èª¬']):
            return "inferential"
        
        # äºˆæ¸¬åˆ†æ
        elif any(word in query_lower for word in ['äºˆæ¸¬', 'ãƒ¢ãƒ‡ãƒ«', 'æ©Ÿæ¢°å­¦ç¿’', 'åˆ†é¡', 'å›å¸°']):
            return "predictive"
        
        # æ•™è‚²çš„å†…å®¹
        else:
            return "educational"
    
    def _build_statistical_prompt(self, query: str, intent: str, data_info: Optional[Dict[str, Any]], 
                                 user_expertise: str) -> str:
        """çµ±è¨ˆåˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
        # åŸºæœ¬ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        base_prompts = {
            "descriptive": "ã‚ãªãŸã¯çµ±è¨ˆåˆ†æã®å°‚é–€å®¶ã§ã™ã€‚è¨˜è¿°çµ±è¨ˆã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚",
            "inferential": "ã‚ãªãŸã¯çµ±è¨ˆåˆ†æã®å°‚é–€å®¶ã§ã™ã€‚æ¨è«–çµ±è¨ˆã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚",
            "predictive": "ã‚ãªãŸã¯çµ±è¨ˆåˆ†æã®å°‚é–€å®¶ã§ã™ã€‚äºˆæ¸¬åˆ†æã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚",
            "educational": "ã‚ãªãŸã¯çµ±è¨ˆåˆ†æã®å°‚é–€å®¶ã§ã™ã€‚çµ±è¨ˆå­¦ã®æ•™è‚²çš„å†…å®¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
        }
        
        base_prompt = base_prompts.get(intent, base_prompts["educational"])
        
        # ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã‚’è¿½åŠ 
        data_context = ""
        if data_info:
            data_context = f"""
ãƒ‡ãƒ¼ã‚¿æƒ…å ±:
- è¡Œæ•°: {data_info.get('rows', 'N/A')}
- åˆ—æ•°: {data_info.get('columns', 'N/A')}
- åˆ—å: {data_info.get('column_names', [])}
- ãƒ‡ãƒ¼ã‚¿å‹: {data_info.get('dtypes', {})}
"""
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ãŸèª¬æ˜ãƒ¬ãƒ™ãƒ«ã‚’è¨­å®š
        expertise_levels = {
            "beginner": "åˆå¿ƒè€…å‘ã‘ã«è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚å°‚é–€ç”¨èªã¯é¿ã‘ã€å…·ä½“ä¾‹ã‚’å¤šãå«ã‚ã¦ãã ã•ã„ã€‚",
            "intermediate": "ä¸­ç´šè€…å‘ã‘ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚ç†è«–çš„èƒŒæ™¯ã¨å®Ÿè·µçš„ãªå¿œç”¨ã‚’ãƒãƒ©ãƒ³ã‚¹ã‚ˆãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚",
            "advanced": "ä¸Šç´šè€…å‘ã‘ã«å°‚é–€çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚æœ€æ–°ã®æ‰‹æ³•ã‚„é«˜åº¦ãªæ¦‚å¿µã‚‚å«ã‚ã¦ãã ã•ã„ã€‚"
        }
        
        expertise_level = expertise_levels.get(user_expertise, expertise_levels["intermediate"])
        
        prompt = f"""
{base_prompt}

{data_context}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {query}
{expertise_level}

å›ç­”ã¯ä»¥ä¸‹ã®å½¢å¼ã§æä¾›ã—ã¦ãã ã•ã„:
1. ç›´æ¥çš„ãªå›ç­”
2. æ¨å¥¨ã•ã‚Œã‚‹çµ±è¨ˆæ‰‹æ³•ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
3. æ•™è‚²çš„å†…å®¹ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
4. Pythonã‚³ãƒ¼ãƒ‰ä¾‹ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰

çµ±è¨ˆæ‰‹æ³•ã®é¸æŠæ™‚ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã¨ä»®å®šã‚’è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚
"""
        
        return prompt
    
    def _parse_statistical_response(self, response: str) -> Dict[str, Any]:
        """çµ±è¨ˆå¿œç­”ã‚’è§£æ"""
        try:
            # JSONå½¢å¼ã®å¿œç­”ã‚’è©¦è¡Œ
            if response.strip().startswith('{') and response.strip().endswith('}'):
                return json.loads(response)
            
            # æ§‹é€ åŒ–ã•ã‚ŒãŸå¿œç­”ã‚’è§£æ
            parsed = {
                'answer': response,
                'confidence': 0.7,
                'suggested_methods': [],
                'educational_content': None,
                'code_example': None,
                'tokens_used': len(response.split())
            }
            
            # çµ±è¨ˆæ‰‹æ³•ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
            statistical_keywords = [
                'tæ¤œå®š', 'ã‚«ã‚¤äºŒä¹—æ¤œå®š', 'ç›¸é–¢åˆ†æ', 'å›å¸°åˆ†æ', 'åˆ†æ•£åˆ†æ',
                'ãƒãƒ³ãƒ›ã‚¤ãƒƒãƒˆãƒ‹ãƒ¼æ¤œå®š', 'ã‚¦ã‚£ãƒ«ã‚³ã‚¯ã‚½ãƒ³æ¤œå®š', 'ã‚¯ãƒ©ã‚¹ã‚«ãƒ«ãƒ»ã‚¦ã‚©ãƒªã‚¹æ¤œå®š',
                'ãƒ•ãƒªãƒ¼ãƒ‰ãƒãƒ³æ¤œå®š', 'ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åˆ†æ', 'ä¸»æˆåˆ†åˆ†æ'
            ]
            
            for keyword in statistical_keywords:
                if keyword in response:
                    parsed['suggested_methods'].append(keyword)
            
            return parsed
            
        except Exception as e:
            self.logger.error(f"å¿œç­”è§£æã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'answer': response,
                'confidence': 0.5,
                'suggested_methods': [],
                'tokens_used': len(response.split())
            }
    
    def get_statistical_methods(self) -> Dict[str, Dict[str, Any]]:
        """åˆ©ç”¨å¯èƒ½ãªçµ±è¨ˆæ‰‹æ³•ã‚’å–å¾—"""
        return self.statistical_methods_db
    
    def suggest_statistical_method(self, data_characteristics: Dict[str, Any], 
                                 research_question: str) -> List[Dict[str, Any]]:
        """ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã¨ç ”ç©¶è³ªå•ã«åŸºã¥ã„ã¦çµ±è¨ˆæ‰‹æ³•ã‚’ææ¡ˆ"""
        suggestions = []
        
        for method_id, method_info in self.statistical_methods_db.items():
            # ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã¨ç ”ç©¶è³ªå•ã«åŸºã¥ã„ã¦é©åˆæ€§ã‚’è©•ä¾¡
            compatibility_score = self._calculate_method_compatibility(
                method_info, data_characteristics, research_question
            )
            
            if compatibility_score > 0.3:  # é–¾å€¤
                suggestions.append({
                    "method_id": method_id,
                    "method_name": method_info["name"],
                    "description": method_info["description"],
                    "compatibility_score": compatibility_score,
                    "assumptions": method_info["assumptions"],
                    "use_cases": method_info["use_cases"],
                    "python_code": method_info.get("python_code", "")
                })
        
        # é©åˆæ€§ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        suggestions.sort(key=lambda x: x["compatibility_score"], reverse=True)
        return suggestions
    
    def _calculate_method_compatibility(self, method_info: Dict[str, Any], 
                                     data_characteristics: Dict[str, Any], 
                                     research_question: str) -> float:
        """çµ±è¨ˆæ‰‹æ³•ã®é©åˆæ€§ã‚’è¨ˆç®—"""
        score = 0.0
        
        # ç ”ç©¶è³ªå•ã¨ã®é©åˆæ€§
        question_lower = research_question.lower()
        method_name_lower = method_info["name"].lower()
        
        if any(word in question_lower for word in method_name_lower.split()):
            score += 0.4
        
        # ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã¨ã®é©åˆæ€§
        data_type = data_characteristics.get("data_type", "unknown")
        n_groups = data_characteristics.get("n_groups", 1)
        
        if "tæ¤œå®š" in method_info["name"] and n_groups == 2:
            score += 0.3
        elif "åˆ†æ•£åˆ†æ" in method_info["name"] and n_groups > 2:
            score += 0.3
        elif "ç›¸é–¢" in method_info["name"] and data_type == "continuous":
            score += 0.3
        
        return min(score, 1.0)

class GGUFProvider(LLMProvider):
    """GGUFç›´æ¥èª­ã¿è¾¼ã¿ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆllama-cpp-pythonä½¿ç”¨ï¼‰"""
    def __init__(self, model_path: str = None, n_ctx: int = 4096, n_gpu_layers: int = 0):
        super().__init__("gguf")
        self.model_path = model_path
        self.n_ctx = n_ctx
        self.n_gpu_layers = n_gpu_layers
        self.model = None
        self.loaded = False
        
        # GPUå¯¾å¿œãƒã‚§ãƒƒã‚¯
        self.gpu_available = self._check_gpu_support()
        
        if self.gpu_available and n_gpu_layers == 0:
            self.n_gpu_layers = -1  # å…¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’GPUã«
            self.logger.info("ğŸš€ GPUåŠ é€Ÿã‚’æœ‰åŠ¹åŒ–ã—ãŸã§")
    
    def _check_gpu_support(self) -> bool:
        """GPUå¯¾å¿œã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            try:
                # ROCmå¯¾å¿œãƒã‚§ãƒƒã‚¯
                import os
                return 'ROCM_PATH' in os.environ
            except:
                return False
    
    def load_model(self, model_path: str = None) -> bool:
        """GGUFãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        if not LLAMA_CPP_AVAILABLE:
            self.logger.error("âŒ llama-cpp-pythonãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã¸ã‚“")
            return False
        
        if model_path:
            self.model_path = model_path
        
        if not self.model_path:
            self.logger.error("âŒ ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒæŒ‡å®šã•ã‚Œã¦ã¸ã‚“")
            return False
        
        if not Path(self.model_path).exists():
            self.logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ã¸ã‚“: {self.model_path}")
            return False
        
        try:
            self.logger.info(f"ğŸ”„ GGUFãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­: {self.model_path}")
            
            # llama-cpp-pythonã§ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            self.model = llama_cpp.Llama(
                model_path=self.model_path,
                n_ctx=self.n_ctx,
                n_gpu_layers=self.n_gpu_layers,
                verbose=False,
                n_threads=os.cpu_count() // 2  # CPUã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’æœ€é©åŒ–
            )
            
            self.loaded = True
            self.logger.info(f"âœ… GGUFãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {Path(self.model_path).name}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ GGUFãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
            return False
    
    def generate(self, prompt: str, max_tokens: int = 512, temperature: float = 0.3, **kwargs) -> Dict[str, Any]:
        """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆï¼ˆåŒæœŸç‰ˆï¼‰"""
        if not self.loaded or not self.model:
            return {
                'success': False,
                'error': 'ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã¸ã‚“',
                'text': '',
                'tokens_consumed': 0,
                'processing_time': 0
            }
        
        try:
            start_time = time.time()
            
            # çµ±è¨ˆå­¦å°‚ç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            system_prompt = """ã‚ãªãŸã¯çµ±è¨ˆå­¦ã®å°‚é–€å®¶ã§ã™ã€‚æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§çµ±è¨ˆå­¦ã®æ¦‚å¿µã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
æ•°å¼ã‚„å…·ä½“ä¾‹ã‚’å«ã‚ã¦ã€å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

è³ªå•: """
            
            full_prompt = system_prompt + prompt
            
            # ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            output = self.model(
                full_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=kwargs.get('top_p', 0.9),
                repeat_penalty=kwargs.get('repeat_penalty', 1.1),
                stop=kwargs.get('stop', ["\n\n", "è³ªå•:", "Q:", "A:"]),
                echo=False
            )
            
            processing_time = time.time() - start_time
            
            generated_text = output['choices'][0]['text'].strip()
            tokens_used = output['usage']['total_tokens']
            
            return {
                'success': True,
                'text': generated_text,
                'content': generated_text,  # ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨ã®äº’æ›æ€§
                'tokens_consumed': tokens_used,
                'tokens': tokens_used,  # ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¨ã®äº’æ›æ€§
                'processing_time': processing_time,
                'model_path': self.model_path,
                'provider': 'gguf'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ GGUFç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'error': str(e),
                'text': '',
                'tokens_consumed': 0,
                'processing_time': 0
            }
    
    async def generate_response(self, prompt: str, model: str = None, **kwargs) -> Dict[str, Any]:
        """éåŒæœŸç‰ˆã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ"""
        import asyncio
        
        # åŒæœŸç‰ˆã‚’éåŒæœŸã§å®Ÿè¡Œ
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self.generate, prompt, **kwargs)
    
    def chat(self, messages: List[Dict[str, str]], **kwargs) -> Dict[str, Any]:
        """ãƒãƒ£ãƒƒãƒˆå½¢å¼ã§ã®ç”Ÿæˆï¼ˆgguf_test_helper.pyã¨ã®äº’æ›æ€§ï¼‰"""
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å˜ä¸€ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›
        prompt_parts = []
        for msg in messages:
            role = msg.get('role', 'user')
            content = msg.get('content', '')
            if role == 'system':
                prompt_parts.append(f"ã‚·ã‚¹ãƒ†ãƒ : {content}")
            elif role == 'user':
                prompt_parts.append(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {content}")
            elif role == 'assistant':
                prompt_parts.append(f"ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: {content}")
        
        full_prompt = "\n".join(prompt_parts) + "\nã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ: "
        return self.generate(full_prompt, **kwargs)
    
    def is_available(self) -> bool:
        """GGUFåˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
        return LLAMA_CPP_AVAILABLE and self.loaded
    
    def get_model_info(self) -> Dict[str, Any]:
        """ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—"""
        if not self.model_path:
            return {'provider': 'GGUF', 'loaded': False}
        
        model_path = Path(self.model_path)
        if model_path.exists():
            stat = model_path.stat()
            return {
                'provider': 'GGUF',
                'model_path': str(model_path),
                'model_name': model_path.name,
                'loaded': self.loaded,
                'gpu_enabled': self.n_gpu_layers > 0,
                'context_size': self.n_ctx,
                'size_mb': round(stat.st_size / (1024 * 1024), 2),
                'modified': datetime.fromtimestamp(stat.st_mtime).isoformat()
            }
        
        return {'provider': 'GGUF', 'loaded': False, 'error': 'Model file not found'}

class KoboldCppProvider(LLMProvider):
    """Kobold.cpp (ãƒ­ãƒ¼ã‚«ãƒ«GGUFãƒ¢ãƒ‡ãƒ«) ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼"""
    def __init__(self, base_url: str):
        super().__init__("koboldcpp")
        # Kobold.cppã¯OpenAIäº’æ›APIã‚’æŒã¤ãŸã‚ã€OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’æµç”¨
        if OPENAI_AVAILABLE:
            self.client = openai.AsyncOpenAI(base_url=base_url, api_key="not-needed")
        else:
            self.client = None

    async def generate_response(self, prompt: str, model: str, **kwargs) -> Dict[str, Any]:
        if not self.client:
            return {"success": False, "error": "OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚"}
        try:
            system_prompt = kwargs.get("system_prompt", "")
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ]
            
            response = await self.client.chat.completions.create(
                model=model, # Kobold.cppã§ã¯ãƒ¢ãƒ‡ãƒ«åã¯ "koboldcpp" ãªã©ã®å›ºå®šå€¤ã§ã‚‚è‰¯ã„å ´åˆãŒå¤šã„
                messages=messages,
                temperature=ai_config.temperature,
                max_tokens=ai_config.max_tokens,
            )
            content = response.choices[0].message.content
            return {"success": True, "provider": "koboldcpp", "model": model, "content": content}

        except Exception as e:
            self.logger.error(f"Kobold.cpp API ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e), "provider": "koboldcpp"}

class KnowledgeBase:
    """RAGç”¨ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹"""
    def __init__(self, docs_dir: str = "_docs"):
        self.docs_dir = Path(docs_dir)
        self.embeddings = None
        self.index = None
        self.documents = []
        self.model = None
        
        if RAG_AVAILABLE:
            try:
                self.model = SentenceTransformer('all-MiniLM-L6-v2')
                self._load_documents()
                self._build_index()
            except Exception as e:
                logging.warning(f"RAGåˆæœŸåŒ–å¤±æ•—: {e}")
    
    def _load_documents(self):
        """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        if not self.docs_dir.exists():
            return
            
        for file_path in self.docs_dir.glob("*.md"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²
                    chunks = self._split_text(content)
                    for chunk in chunks:
                        self.documents.append({
                            'content': chunk,
                            'source': str(file_path),
                            'timestamp': datetime.now().isoformat()
                        })
            except Exception as e:
                logging.warning(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿å¤±æ•— {file_path}: {e}")
    
    def _split_text(self, text: str, chunk_size: int = 500) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        words = text.split()
        chunks = []
        current_chunk = []
        current_size = 0
        
        for word in words:
            current_chunk.append(word)
            current_size += len(word) + 1
            
            if current_size >= chunk_size:
                chunks.append(' '.join(current_chunk))
                current_chunk = []
                current_size = 0
        
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
    
    def _build_index(self):
        """FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰"""
        if not self.documents or not self.model:
            return
            
        try:
            texts = [doc['content'] for doc in self.documents]
            self.embeddings = self.model.encode(texts)
            
            if RAG_AVAILABLE:
                import faiss
                dimension = self.embeddings.shape[1]
                self.index = faiss.IndexFlatL2(dimension)
                self.index.add(self.embeddings.astype('float32'))
        except Exception as e:
            logging.warning(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å¤±æ•—: {e}")
    
    def search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢"""
        if not self.model or not self.index or not self.documents:
            return []
            
        try:
            query_embedding = self.model.encode([query])
            distances, indices = self.index.search(query_embedding.astype('float32'), top_k)
            
            results = []
            for i, idx in enumerate(indices[0]):
                if idx < len(self.documents):
                    results.append({
                        'content': self.documents[idx]['content'],
                        'source': self.documents[idx]['source'],
                        'score': float(distances[0][i])
                    })
            return results
        except Exception as e:
            logging.warning(f"æ¤œç´¢å¤±æ•—: {e}")
            return []

class EducationalContentGenerator:
    """æ•™è‚²ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, llm_provider_manager=None):
        self.logger = logging.getLogger(f"{__name__}.EducationalContentGenerator")
        self.llm_provider_manager = llm_provider_manager

    async def generate_explanation(self, concept: str, user_expertise_level: str, language: str = "ja") -> Dict[str, Any]:
        """
        çµ±è¨ˆæ¦‚å¿µã®èª¬æ˜ã‚’ç”Ÿæˆã™ã‚‹ã€‚
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å°‚é–€ãƒ¬ãƒ™ãƒ«ã¨å¸Œæœ›è¨€èªã«å¿œã˜ã¦èª¬æ˜ã®è¤‡é›‘ã•ã‚’èª¿æ•´ã™ã‚‹ã€‚
        """
        try:
            prompt_template = STATISTICAL_ANALYSIS_PROMPTS["educational_explanation"]
            prompt = prompt_template.format(
                concept=concept,
                user_expertise_level=user_expertise_level,
                language=language
            )

            if self.llm_provider_manager:
                response = await self.llm_provider_manager.route_request(
                    LLMRequest(prompt=prompt, task_type="educational", data_sensitivity="none")
                )
                if response.success:
                    return {"success": True, "explanation": response.content}
                else:
                    return {"success": False, "error": response.error}
            else:
                return {"success": False, "error": "LLMProviderManagerãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"}
        except Exception as e:
            self.logger.error(f"æ•™è‚²ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}

    async def generate_visual_aid_description(self, concept: str, data_example: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        çµ±è¨ˆæ¦‚å¿µã‚’èª¬æ˜ã™ã‚‹ãŸã‚ã®è¦–è¦šè£œåŠ©ï¼ˆã‚°ãƒ©ãƒ•ã€å›³ãªã©ï¼‰ã®è¨˜è¿°ã‚’ç”Ÿæˆã™ã‚‹ã€‚
        """
        try:
            prompt_template = STATISTICAL_ANALYSIS_PROMPTS["visual_aid_description"]
            prompt = prompt_template.format(
                concept=concept,
                data_example=json.dumps(data_example) if data_example else ""
            )

            if self.llm_provider_manager:
                response = await self.llm_provider_manager.route_request(
                    LLMRequest(prompt=prompt, task_type="visual_aid", data_sensitivity="none")
                )
                if response.success:
                    return {"success": True, "description": response.content}
                else:
                    return {"success": False, "error": response.error}
            else:
                return {"success": False, "error": "LLMProviderManagerãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"}
        except Exception as e:
            self.logger.error(f"è¦–è¦šè£œåŠ©è¨˜è¿°ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}

    async def generate_interactive_example(self, concept: str, user_expertise_level: str) -> Dict[str, Any]:
        """
        çµ±è¨ˆæ¦‚å¿µã‚’å­¦ã¶ãŸã‚ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªã‚³ãƒ¼ãƒ‰ä¾‹ã‚’ç”Ÿæˆã™ã‚‹ã€‚
        """
        try:
            prompt_template = STATISTICAL_ANALYSIS_PROMPTS["interactive_example"]
            prompt = prompt_template.format(
                concept=concept,
                user_expertise_level=user_expertise_level
            )

            if self.llm_provider_manager:
                response = await self.llm_provider_manager.route_request(
                    LLMRequest(prompt=prompt, task_type="code_example", data_sensitivity="none")
                )
                if response.success:
                    return {"success": True, "code_example": response.content}
                else:
                    return {"success": False, "error": response.error}
            else:
                return {"success": False, "error": "LLMProviderManagerãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"}
        except Exception as e:
            self.logger.error(f"ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªä¾‹ã®ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}

class CodeGenerator:
    """AIé§†å‹•å‹Pythonã‚³ãƒ¼ãƒ‰ç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self, llm_provider_manager=None):
        self.logger = logging.getLogger(f"{__name__}.CodeGenerator")
        self.llm_provider_manager = llm_provider_manager

    async def generate_analysis_code(self, analysis_type: str, data_info: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
        """
        æŒ‡å®šã•ã‚ŒãŸåˆ†æã‚¿ã‚¤ãƒ—ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦Pythonã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹ã€‚
        """
        try:
            prompt_template = STATISTICAL_ANALYSIS_PROMPTS["generate_analysis_code"]
            prompt = prompt_template.format(
                analysis_type=analysis_type,
                data_info=json.dumps(data_info, indent=2),
                params=json.dumps(params, indent=2)
            )

            if self.llm_provider_manager:
                response = await self.llm_provider_manager.route_request(
                    LLMRequest(prompt=prompt, task_type="code_generation", data_sensitivity="high")
                )
                if response.success:
                    return {"success": True, "code": response.content}
                else:
                    return {"success": False, "error": response.error}
            else:
                return {"success": False, "error": "LLMProviderManagerãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"}
        except Exception as e:
            self.logger.error(f"åˆ†æã‚³ãƒ¼ãƒ‰ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}

    async def generate_visualization_code(self, plot_type: str, data_info: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒƒãƒˆã‚¿ã‚¤ãƒ—ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦å¯è¦–åŒ–ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹ã€‚
        """
        try:
            prompt_template = STATISTICAL_ANALYSIS_PROMPTS["generate_visualization_code"]
            prompt = prompt_template.format(
                plot_type=plot_type,
                data_info=json.dumps(data_info, indent=2),
                params=json.dumps(params, indent=2)
            )

            if self.llm_provider_manager:
                response = await self.llm_provider_manager.route_request(
                    LLMRequest(prompt=prompt, task_type="code_generation", data_sensitivity="medium")
                )
                if response.success:
                    return {"success": True, "code": response.content}
                else:
                    return {"success": False, "error": response.error}
            else:
                return {"success": False, "error": "LLMProviderManagerãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"}
        except Exception as e:
            self.logger.error(f"å¯è¦–åŒ–ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}

    async def generate_report_code(self, report_type: str, analysis_results: Dict[str, Any], params: Dict[str, Any]) -> Dict[str, Any]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—ã¨åˆ†æçµæœã«åŸºã¥ã„ã¦ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹ã€‚
        """
        try:
            prompt_template = STATISTICAL_ANALYSIS_PROMPTS["generate_report_code"]
            prompt = prompt_template.format(
                report_type=report_type,
                analysis_results=json.dumps(analysis_results, indent=2),
                params=json.dumps(params, indent=2)
            )

            if self.llm_provider_manager:
                response = await self.llm_provider_manager.route_request(
                    LLMRequest(prompt=prompt, task_type="code_generation", data_sensitivity="low")
                )
                if response.success:
                    return {"success": True, "code": response.content}
                else:
                    return {"success": False, "error": response.error}
            else:
                return {"success": False, "error": "LLMProviderManagerãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"}
        except Exception as e:
            self.logger.error(f"ãƒ¬ãƒãƒ¼ãƒˆã‚³ãƒ¼ãƒ‰ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}

# Enhanced Data Models for AI Orchestrator
from dataclasses import dataclass

@dataclass
class AnalysisContext:
    """åˆ†æã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
    user_id: str
    session_id: str
    data_fingerprint: str
    analysis_history: List[Dict[str, Any]]
    user_expertise_level: str = "intermediate"
    privacy_settings: Dict[str, Any] = None
    timestamp: datetime = None
    
    # æ‹¡å¼µãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼ˆã‚¿ã‚¹ã‚¯1.3å¯¾å¿œï¼‰
    user_preferences: Dict[str, Any] = None
    session_metadata: Dict[str, Any] = None
    context_tags: List[str] = None
    learning_progress: Dict[str, float] = None
    favorite_methods: List[str] = None
    recent_queries: List[str] = None
    
    def __post_init__(self):
        if self.privacy_settings is None:
            self.privacy_settings = {"use_local_llm": False, "anonymize_data": True}
        if self.timestamp is None:
            self.timestamp = datetime.now()
        if self.user_preferences is None:
            self.user_preferences = {
                "preferred_visualization": "plotly",
                "explanation_style": "detailed",
                "language": "ja",
                "auto_save": True
            }
        if self.session_metadata is None:
            self.session_metadata = {
                "platform": platform.system(),
                "start_time": datetime.now().isoformat(),
                "data_sources": [],
                "analysis_goals": []
            }
        if self.context_tags is None:
            self.context_tags = []
        if self.learning_progress is None:
            self.learning_progress = {}
        if self.favorite_methods is None:
            self.favorite_methods = []
        if self.recent_queries is None:
            self.recent_queries = []

@dataclass
class AIResponse:
    """AIå¿œç­”ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«"""
    content: str
    confidence: float
    provider_used: str
    tokens_consumed: int
    processing_time: float
    intent_detected: Optional[IntentType] = None
    educational_content: Optional[str] = None
    follow_up_suggestions: List[str] = None
    
    def __post_init__(self):
        if self.follow_up_suggestions is None:
            self.follow_up_suggestions = []

class QueryProcessor:
    """è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªå‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.QueryProcessor")
        
    def process_query(self, query: str, context: AnalysisContext) -> Dict[str, Any]:
        """ã‚¯ã‚¨ãƒªã‚’å‡¦ç†ã—ã¦æ§‹é€ åŒ–ã•ã‚ŒãŸæƒ…å ±ã‚’è¿”ã™"""
        try:
            # åŸºæœ¬çš„ãªã‚¯ã‚¨ãƒªè§£æ
            processed = {
                'original_query': query,
                'cleaned_query': self._clean_query(query),
                'intent': self._classify_intent(query),
                'statistical_keywords': self._extract_statistical_keywords(query),
                'data_references': self._extract_data_references(query),
                'confidence': 0.8
            }
            
            return processed
            
        except Exception as e:
            self.logger.error(f"ã‚¯ã‚¨ãƒªå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'original_query': query,
                'cleaned_query': query,
                'intent': IntentType.EXPLORATORY,
                'statistical_keywords': [],
                'data_references': [],
                'confidence': 0.3
            }
    
    def _clean_query(self, query: str) -> str:
        """ã‚¯ã‚¨ãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å‡¦ç†
        cleaned = query.strip()
        cleaned = re.sub(r'\s+', ' ', cleaned)
        return cleaned
    
    def _classify_intent(self, query: str) -> IntentType:
        """ã‚¯ã‚¨ãƒªã®æ„å›³ã‚’åˆ†é¡"""
        query_lower = query.lower()
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®åˆ†é¡
        if any(word in query_lower for word in ['å¹³å‡', 'mean', 'æ¨™æº–åå·®', 'std', 'åˆ†å¸ƒ', 'distribution']):
            return IntentType.DESCRIPTIVE
        elif any(word in query_lower for word in ['æ¤œå®š', 'test', 'æœ‰æ„', 'significant', 'på€¤', 'p-value']):
            return IntentType.INFERENTIAL
        elif any(word in query_lower for word in ['äºˆæ¸¬', 'predict', 'æ©Ÿæ¢°å­¦ç¿’', 'ml', 'model']):
            return IntentType.PREDICTIVE
        elif any(word in query_lower for word in ['èª¬æ˜', 'explain', 'æ•™ãˆã¦', 'how to', 'what is']):
            return IntentType.EDUCATIONAL
        else:
            return IntentType.EXPLORATORY
    
    def _extract_statistical_keywords(self, query: str) -> List[str]:
        """çµ±è¨ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºï¼ˆå¤šè¨€èªå¯¾å¿œï¼‰"""
        # æ‹¡å¼µã•ã‚ŒãŸçµ±è¨ˆç”¨èªè¾æ›¸
        statistical_terms = {
            # åŸºæœ¬çµ±è¨ˆ
            'descriptive': ['å¹³å‡', 'mean', 'average', 'ä¸­å¤®å€¤', 'median', 'æœ€é »å€¤', 'mode', 
                          'æ¨™æº–åå·®', 'std', 'standard deviation', 'åˆ†æ•£', 'variance',
                          'å››åˆ†ä½', 'quartile', 'ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«', 'percentile'],
            
            # ä»®èª¬æ¤œå®š
            'inferential': ['tæ¤œå®š', 't-test', 'anova', 'åˆ†æ•£åˆ†æ', 'analysis of variance',
                          'chi-square', 'ã‚«ã‚¤äºŒä¹—', 'fisher', 'ãƒ•ã‚£ãƒƒã‚·ãƒ£ãƒ¼', 'wilcoxon',
                          'ã‚¦ã‚£ãƒ«ã‚³ã‚¯ã‚½ãƒ³', 'mann-whitney', 'ãƒãƒ³ãƒ»ãƒ›ã‚¤ãƒƒãƒˆãƒ‹ãƒ¼',
                          'på€¤', 'p-value', 'æœ‰æ„', 'significant', 'ä¿¡é ¼åŒºé–“', 'confidence interval'],
            
            # ç›¸é–¢ãƒ»å›å¸°
            'correlation': ['ç›¸é–¢', 'correlation', 'pearson', 'ãƒ”ã‚¢ã‚½ãƒ³', 'spearman', 'ã‚¹ãƒ”ã‚¢ãƒãƒ³',
                          'å›å¸°', 'regression', 'linear', 'ç·šå½¢', 'multiple', 'é‡å›å¸°',
                          'logistic', 'ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯', 'æ±ºå®šä¿‚æ•°', 'r-squared'],
            
            # æ©Ÿæ¢°å­¦ç¿’
            'ml': ['æ©Ÿæ¢°å­¦ç¿’', 'machine learning', 'ml', 'äºˆæ¸¬', 'prediction', 'model', 'ãƒ¢ãƒ‡ãƒ«',
                  'classification', 'åˆ†é¡', 'clustering', 'ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°', 'neural', 'ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«',
                  'random forest', 'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ', 'svm', 'support vector'],
            
            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            'data_processing': ['å‰å‡¦ç†', 'preprocessing', 'æ¬ æå€¤', 'missing', 'outlier', 'å¤–ã‚Œå€¤',
                              'æ­£è¦åŒ–', 'normalization', 'standardization', 'æ¨™æº–åŒ–',
                              'encoding', 'ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°', 'scaling', 'ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°'],
            
            # å¯è¦–åŒ–
            'visualization': ['å¯è¦–åŒ–', 'visualization', 'plot', 'ãƒ—ãƒ­ãƒƒãƒˆ', 'graph', 'ã‚°ãƒ©ãƒ•',
                            'histogram', 'ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ', 'scatter', 'æ•£å¸ƒå›³', 'boxplot', 'ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ',
                            'heatmap', 'ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—', 'dashboard', 'ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰']
        }
        
        found_terms = []
        query_lower = query.lower()
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«æ¤œç´¢
        for category, terms in statistical_terms.items():
            for term in terms:
                if term.lower() in query_lower:
                    found_terms.append(term)
        
        # é‡è¤‡ã‚’é™¤å»ã—ã¦è¿”ã™
        return list(set(found_terms))
    
    def _extract_data_references(self, query: str) -> List[str]:
        """ãƒ‡ãƒ¼ã‚¿å‚ç…§ã‚’æŠ½å‡º"""
        # åˆ—åã‚„ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å‚ç…§ã‚’æŠ½å‡º
        data_refs = re.findall(r'[A-Za-z_][A-Za-z0-9_]*(?:\.[A-Za-z_][A-Za-z0-9_]*)?', query)
        return data_refs[:5]  # æœ€å¤§5å€‹ã¾ã§
    
    def generate_clarifying_questions(self, query: str, context: AnalysisContext) -> List[str]:
        """æ›–æ˜§ãªã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹æ˜ç¢ºåŒ–è³ªå•ã‚’ç”Ÿæˆ"""
        questions = []
        
        # çµ±è¨ˆæ‰‹æ³•ãŒæ›–æ˜§ãªå ´åˆ
        if any(word in query.lower() for word in ['åˆ†æ', 'analysis', 'èª¿ã¹ã‚‹', 'examine']):
            if not self._extract_statistical_keywords(query):
                questions.append("ã©ã®ã‚ˆã†ãªçµ±è¨ˆæ‰‹æ³•ã‚’ãŠè€ƒãˆã§ã™ã‹ï¼Ÿï¼ˆä¾‹ï¼šç›¸é–¢åˆ†æã€tæ¤œå®šã€å›å¸°åˆ†æï¼‰")
        
        # ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ãŒä¸æ˜ãªå ´åˆ
        if any(word in query.lower() for word in ['ãƒ‡ãƒ¼ã‚¿', 'data']) and not self._extract_data_references(query):
            questions.append("åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ã¯ã©ã®ã‚ˆã†ãªå¤‰æ•°ã‚’å«ã‚“ã§ã„ã¾ã™ã‹ï¼Ÿ")
        
        # ç›®çš„ãŒä¸æ˜ç¢ºãªå ´åˆ
        if len(query.split()) < 5:  # çŸ­ã™ãã‚‹ã‚¯ã‚¨ãƒª
            questions.append("åˆ†æã®ç›®çš„ã‚„çŸ¥ã‚ŠãŸã„ã“ã¨ã‚’è©³ã—ãæ•™ãˆã¦ãã ã•ã„ã€‚")
        
        # æ¯”è¼ƒå¯¾è±¡ãŒä¸æ˜ãªå ´åˆ
        if any(word in query.lower() for word in ['æ¯”è¼ƒ', 'compare', 'é•ã„', 'difference']):
            if not any(word in query.lower() for word in ['ã¨', 'and', 'vs', 'å¯¾']):
                questions.append("ä½•ã¨ä½•ã‚’æ¯”è¼ƒã—ãŸã„ã§ã™ã‹ï¼Ÿ")
        
        return questions[:3]  # æœ€å¤§3ã¤ã¾ã§
    
    def suggest_statistical_methods(self, query: str, data_info: Optional[Dict[str, Any]] = None) -> List[str]:
        """ã‚¯ã‚¨ãƒªã¨ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã«åŸºã¥ã„ã¦çµ±è¨ˆæ‰‹æ³•ã‚’ææ¡ˆ"""
        suggestions = []
        query_lower = query.lower()
        
        # è¨˜è¿°çµ±è¨ˆã®ææ¡ˆ
        if any(word in query_lower for word in ['è¦ç´„', 'summary', 'æ¦‚è¦', 'overview']):
            suggestions.extend(['è¨˜è¿°çµ±è¨ˆ', 'åŸºæœ¬çµ±è¨ˆé‡', 'ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ', 'ãƒœãƒƒã‚¯ã‚¹ãƒ—ãƒ­ãƒƒãƒˆ'])
        
        # é–¢ä¿‚æ€§ã®åˆ†æ
        if any(word in query_lower for word in ['é–¢ä¿‚', 'relationship', 'ç›¸é–¢', 'correlation']):
            suggestions.extend(['ç›¸é–¢åˆ†æ', 'æ•£å¸ƒå›³', 'å›å¸°åˆ†æ'])
        
        # ç¾¤é–“æ¯”è¼ƒ
        if any(word in query_lower for word in ['æ¯”è¼ƒ', 'compare', 'å·®', 'difference']):
            suggestions.extend(['tæ¤œå®š', 'ANOVA', 'ã‚«ã‚¤äºŒä¹—æ¤œå®š'])
        
        # äºˆæ¸¬ãƒ»åˆ†é¡
        if any(word in query_lower for word in ['äºˆæ¸¬', 'predict', 'åˆ†é¡', 'classify']):
            suggestions.extend(['ç·šå½¢å›å¸°', 'ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°', 'æ©Ÿæ¢°å­¦ç¿’'])
        
        # ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã«åŸºã¥ãææ¡ˆ
        if data_info:
            n_vars = data_info.get('n_variables', 0)
            if n_vars > 5:
                suggestions.append('ä¸»æˆåˆ†åˆ†æ')
            if data_info.get('has_categorical', False):
                suggestions.append('ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ†æ')
        
        return list(set(suggestions))[:5]  # é‡è¤‡é™¤å»ã—ã¦æœ€å¤§5ã¤

class IntentClassifier:
    """æ„å›³åˆ†é¡ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.IntentClassifier")
    
    def classify(self, query: str, context: AnalysisContext) -> IntentType:
        """ã‚ˆã‚Šé«˜åº¦ãªæ„å›³åˆ†é¡"""
        # å±¥æ­´ã‚’è€ƒæ…®ã—ãŸåˆ†é¡
        if context.analysis_history:
            last_analysis = context.analysis_history[-1]
            if 'follow_up' in query.lower() and last_analysis.get('type') == 'descriptive':
                return IntentType.INFERENTIAL
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯QueryProcessorã®åˆ†é¡ã‚’ä½¿ç”¨
        processor = QueryProcessor()
        result = processor.process_query(query, context)
        return result['intent']

class ContextManager:
    """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆæ°¸ç¶šåŒ–å¯¾å¿œï¼‰"""
    
    def __init__(self, checkpoint_dir: str = "checkpoints"):
        self.logger = logging.getLogger(f"{__name__}.ContextManager")
        self.sessions: Dict[str, AnalysisContext] = {}
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
        
        # æ—¢å­˜ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å¾©æ—§
        self._load_existing_sessions()
    
    def get_or_create_context(self, user_id: str, session_id: str, data_fingerprint: str) -> AnalysisContext:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã¾ãŸã¯ä½œæˆ"""
        context_key = f"{user_id}_{session_id}"
        
        if context_key not in self.sessions:
            # æ°¸ç¶šåŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¢ã™
            loaded_context = self._load_context_from_disk(context_key)
            
            if loaded_context:
                self.sessions[context_key] = loaded_context
                self.logger.info(f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå¾©æ—§: {context_key}")
            else:
                # æ–°ã—ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
                self.sessions[context_key] = AnalysisContext(
                    user_id=user_id,
                    session_id=session_id,
                    data_fingerprint=data_fingerprint,
                    analysis_history=[]
                )
                self.logger.info(f"æ–°ã—ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä½œæˆ: {context_key}")
        
        return self.sessions[context_key]
    
    def update_context(self, context: AnalysisContext, analysis_result: Dict[str, Any]):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ›´æ–°"""
        # åˆ†æçµæœã‚’å±¥æ­´ã«è¿½åŠ 
        analysis_entry = {
            'timestamp': datetime.now().isoformat(),
            'analysis_type': analysis_result.get('type', 'unknown'),
            'method_used': analysis_result.get('method', 'unknown'),
            'success': analysis_result.get('success', False),
            'query': analysis_result.get('query', ''),
            'provider': analysis_result.get('provider', 'unknown'),
            'processing_time': analysis_result.get('processing_time', 0.0),
            'tokens_consumed': analysis_result.get('tokens_consumed', 0)
        }
        
        context.analysis_history.append(analysis_entry)
        
        # å±¥æ­´ã®é•·ã•ã‚’åˆ¶é™
        if len(context.analysis_history) > 100:
            context.analysis_history = context.analysis_history[-100:]
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ°¸ç¶šåŒ–
        self._save_context_to_disk(context)
        
        self.logger.debug(f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ›´æ–°: {context.user_id}_{context.session_id}")
    
    def get_user_expertise_level(self, context: AnalysisContext) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å°‚é–€ãƒ¬ãƒ™ãƒ«ã‚’å‹•çš„ã«è©•ä¾¡"""
        if not context.analysis_history:
            return context.user_expertise_level
        
        # å±¥æ­´ã‹ã‚‰å°‚é–€ãƒ¬ãƒ™ãƒ«ã‚’æ¨å®š
        recent_analyses = context.analysis_history[-10:]  # æœ€è¿‘10ä»¶
        
        advanced_methods = ['machine_learning', 'bayesian_analysis', 'survival_analysis']
        basic_methods = ['descriptive_stats', 'basic_visualization']
        
        advanced_count = sum(1 for analysis in recent_analyses 
                           if analysis.get('method_used') in advanced_methods)
        basic_count = sum(1 for analysis in recent_analyses 
                        if analysis.get('method_used') in basic_methods)
        
        if advanced_count >= basic_count and advanced_count > 0:
            return 'expert'
        elif advanced_count > 0:
            return 'intermediate'
        else:
            return 'novice'
    
    def get_analysis_patterns(self, context: AnalysisContext) -> Dict[str, Any]:
        """åˆ†æãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—"""
        if not context.analysis_history:
            return {}
        
        # ä½¿ç”¨é »åº¦ã®é«˜ã„åˆ†æã‚¿ã‚¤ãƒ—
        analysis_types = [entry.get('analysis_type', 'unknown') 
                         for entry in context.analysis_history]
        type_counts = {}
        for analysis_type in analysis_types:
            type_counts[analysis_type] = type_counts.get(analysis_type, 0) + 1
        
        # ä½¿ç”¨é »åº¦ã®é«˜ã„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
        providers = [entry.get('provider', 'unknown') 
                    for entry in context.analysis_history]
        provider_counts = {}
        for provider in providers:
            provider_counts[provider] = provider_counts.get(provider, 0) + 1
        
        # å¹³å‡å‡¦ç†æ™‚é–“
        processing_times = [entry.get('processing_time', 0.0) 
                          for entry in context.analysis_history 
                          if entry.get('processing_time', 0.0) > 0]
        avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0.0
        
        return {
            'most_used_analysis_types': sorted(type_counts.items(), key=lambda x: x[1], reverse=True)[:3],
            'preferred_providers': sorted(provider_counts.items(), key=lambda x: x[1], reverse=True)[:3],
            'average_processing_time': avg_processing_time,
            'total_analyses': len(context.analysis_history),
            'success_rate': sum(1 for entry in context.analysis_history 
                              if entry.get('success', False)) / len(context.analysis_history)
        }
    
    def cleanup_old_sessions(self, days_old: int = 30):
        """å¤ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        cutoff_time = datetime.now().timestamp() - (days_old * 24 * 3600)
        
        sessions_to_remove = []
        for context_key, context in self.sessions.items():
            if context.timestamp.timestamp() < cutoff_time:
                sessions_to_remove.append(context_key)
        
        for context_key in sessions_to_remove:
            del self.sessions[context_key]
            # ãƒ‡ã‚£ã‚¹ã‚¯ã‹ã‚‰ã‚‚å‰Šé™¤
            context_file = self.checkpoint_dir / f"context_{context_key}.json"
            if context_file.exists():
                context_file.unlink()
        
        self.logger.info(f"å¤ã„ã‚»ãƒƒã‚·ãƒ§ãƒ³ {len(sessions_to_remove)} ä»¶ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ")
    
    def _load_existing_sessions(self):
        """æ—¢å­˜ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’èª­ã¿è¾¼ã¿"""
        try:
            for context_file in self.checkpoint_dir.glob("context_*.json"):
                context_key = context_file.stem.replace("context_", "")
                context = self._load_context_from_disk(context_key)
                if context:
                    self.sessions[context_key] = context
            
            self.logger.info(f"æ—¢å­˜ã‚»ãƒƒã‚·ãƒ§ãƒ³ {len(self.sessions)} ä»¶ã‚’å¾©æ—§ã—ã¾ã—ãŸ")
        except Exception as e:
            self.logger.error(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³å¾©æ—§ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _load_context_from_disk(self, context_key: str) -> Optional[AnalysisContext]:
        """ãƒ‡ã‚£ã‚¹ã‚¯ã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
        context_file = self.checkpoint_dir / f"context_{context_key}.json"
        
        if not context_file.exists():
            return None
        
        try:
            with open(context_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # JSONã‹ã‚‰AnalysisContextã‚’å¾©å…ƒï¼ˆæ‹¡å¼µãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å¯¾å¿œï¼‰
            context = AnalysisContext(
                user_id=data['user_id'],
                session_id=data['session_id'],
                data_fingerprint=data['data_fingerprint'],
                analysis_history=data.get('analysis_history', []),
                user_expertise_level=data.get('user_expertise_level', 'intermediate'),
                privacy_settings=data.get('privacy_settings', {}),
                timestamp=datetime.fromisoformat(data['timestamp']),
                # æ‹¡å¼µãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                user_preferences=data.get('user_preferences', {}),
                session_metadata=data.get('session_metadata', {}),
                context_tags=data.get('context_tags', []),
                learning_progress=data.get('learning_progress', {}),
                favorite_methods=data.get('favorite_methods', []),
                recent_queries=data.get('recent_queries', [])
            )
            
            return context
            
        except Exception as e:
            self.logger.error(f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {context_key}: {e}")
            return None
    
    def _save_context_to_disk(self, context: AnalysisContext):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‡ã‚£ã‚¹ã‚¯ã«ä¿å­˜ï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
        context_key = f"{context.user_id}_{context.session_id}"
        context_file = self.checkpoint_dir / f"context_{context_key}.json"
        
        try:
            # AnalysisContextã‚’JSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢å¼ã«å¤‰æ›ï¼ˆæ‹¡å¼µãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å¯¾å¿œï¼‰
            data = {
                'user_id': context.user_id,
                'session_id': context.session_id,
                'data_fingerprint': context.data_fingerprint,
                'analysis_history': context.analysis_history,
                'user_expertise_level': context.user_expertise_level,
                'privacy_settings': context.privacy_settings,
                'timestamp': context.timestamp.isoformat(),
                # æ‹¡å¼µãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
                'user_preferences': context.user_preferences,
                'session_metadata': context.session_metadata,
                'context_tags': context.context_tags,
                'learning_progress': context.learning_progress,
                'favorite_methods': context.favorite_methods,
                'recent_queries': context.recent_queries
            }
            
            with open(context_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            self.logger.error(f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼ {context_key}: {e}")
    
    def get_session_summary(self, context: AnalysisContext) -> Dict[str, Any]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³è¦ç´„ã‚’å–å¾—ï¼ˆæ‹¡å¼µç‰ˆï¼‰"""
        patterns = self.get_analysis_patterns(context)
        expertise_level = self.get_user_expertise_level(context)
        
        return {
            'user_id': context.user_id,
            'session_id': context.session_id,
            'session_start': context.timestamp.isoformat(),
            'current_expertise_level': expertise_level,
            'analysis_patterns': patterns,
            'data_fingerprint': context.data_fingerprint,
            'privacy_settings': context.privacy_settings,
            'user_preferences': context.user_preferences,
            'learning_progress': context.learning_progress,
            'favorite_methods': context.favorite_methods,
            'context_tags': context.context_tags
        }
    
    def update_user_preferences(self, context: AnalysisContext, preferences: Dict[str, Any]):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã‚’æ›´æ–°"""
        context.user_preferences.update(preferences)
        self._save_context_to_disk(context)
        self.logger.info(f"ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šæ›´æ–°: {context.user_id}")
    
    def add_context_tag(self, context: AnalysisContext, tag: str):
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¿ã‚°ã‚’è¿½åŠ """
        if tag not in context.context_tags:
            context.context_tags.append(tag)
            self._save_context_to_disk(context)
    
    def update_learning_progress(self, context: AnalysisContext, concept: str, progress: float):
        """å­¦ç¿’é€²æ—ã‚’æ›´æ–°"""
        context.learning_progress[concept] = progress
        self._save_context_to_disk(context)
        self.logger.debug(f"å­¦ç¿’é€²æ—æ›´æ–°: {concept} -> {progress}")
    
    def add_favorite_method(self, context: AnalysisContext, method: str):
        """ãŠæ°—ã«å…¥ã‚Šæ‰‹æ³•ã‚’è¿½åŠ """
        if method not in context.favorite_methods:
            context.favorite_methods.append(method)
            # æœ€å¤§10å€‹ã¾ã§ä¿æŒ
            if len(context.favorite_methods) > 10:
                context.favorite_methods = context.favorite_methods[-10:]
            self._save_context_to_disk(context)
    
    def add_recent_query(self, context: AnalysisContext, query: str):
        """æœ€è¿‘ã®ã‚¯ã‚¨ãƒªã‚’è¿½åŠ """
        context.recent_queries.insert(0, query)
        # æœ€å¤§20å€‹ã¾ã§ä¿æŒ
        if len(context.recent_queries) > 20:
            context.recent_queries = context.recent_queries[:20]
        self._save_context_to_disk(context)
    
    def get_contextual_recommendations(self, context: AnalysisContext) -> Dict[str, Any]:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ãæ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = {
            'suggested_methods': [],
            'learning_opportunities': [],
            'workflow_improvements': []
        }
        
        # ä½¿ç”¨å±¥æ­´ã«åŸºã¥ãæ¨å¥¨
        patterns = self.get_analysis_patterns(context)
        if patterns.get('most_used_analysis_types'):
            most_used = patterns['most_used_analysis_types'][0][0]
            recommendations['suggested_methods'].append(f"{most_used}ã®é«˜åº¦ãªæ‰‹æ³•")
        
        # å­¦ç¿’é€²æ—ã«åŸºã¥ãæ¨å¥¨
        for concept, progress in context.learning_progress.items():
            if progress < 0.7:  # 70%æœªæº€ã®ç†è§£åº¦
                recommendations['learning_opportunities'].append(f"{concept}ã®å¾©ç¿’")
        
        # å°‚é–€ãƒ¬ãƒ™ãƒ«ã«åŸºã¥ãæ¨å¥¨
        expertise = self.get_user_expertise_level(context)
        if expertise == 'novice':
            recommendations['learning_opportunities'].extend([
                'åŸºæœ¬çµ±è¨ˆã®å­¦ç¿’', 'ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ã®åŸºç¤'
            ])
        elif expertise == 'expert':
            recommendations['suggested_methods'].extend([
                'ãƒ™ã‚¤ã‚ºçµ±è¨ˆ', 'æ©Ÿæ¢°å­¦ç¿’æ‰‹æ³•'
            ])
        
        return recommendations
    
    def generate_context_aware_response(self, context: AnalysisContext, base_response: str) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®ã—ãŸå¿œç­”ã‚’ç”Ÿæˆ"""
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å°‚é–€ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦å¿œç­”ã‚’èª¿æ•´
        expertise = self.get_user_expertise_level(context)
        language = context.user_preferences.get('language', 'ja')
        explanation_style = context.user_preferences.get('explanation_style', 'detailed')
        
        # å¿œç­”ã®èª¿æ•´ãƒ­ã‚¸ãƒƒã‚¯
        if expertise == 'novice' and explanation_style == 'detailed':
            # åˆå¿ƒè€…å‘ã‘ã«è©³ç´°ãªèª¬æ˜ã‚’è¿½åŠ 
            base_response += "\n\nğŸ“š è£œè¶³èª¬æ˜: ã“ã®åˆ†ææ‰‹æ³•ã«ã¤ã„ã¦è©³ã—ãå­¦ã³ãŸã„å ´åˆã¯ã€åŸºæœ¬æ¦‚å¿µã‹ã‚‰å§‹ã‚ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚"
        elif expertise == 'expert' and explanation_style == 'concise':
            # å°‚é–€å®¶å‘ã‘ã«ç°¡æ½”ãªå¿œç­”ã«èª¿æ•´
            base_response = base_response.split('\n')[0]  # æœ€åˆã®è¡Œã®ã¿
        
        # è¨€èªè¨­å®šã«å¿œã˜ãŸèª¿æ•´
        if language == 'en':
            # è‹±èªã§ã®å¿œç­”ã«å¤‰æ›ï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
            base_response = base_response.replace('åˆ†æ', 'analysis').replace('çµæœ', 'result')
        
        return base_response

@dataclass
class DataCharacteristics:
    """ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§æƒ…å ±"""
    n_rows: int
    n_columns: int
    column_types: Dict[str, str]
    missing_data_pattern: Dict[str, float]
    distribution_characteristics: Dict[str, Dict[str, Any]]
    correlation_structure: Optional[np.ndarray] = None
    outlier_information: Dict[str, Any] = None
    data_quality_score: float = 0.0
    
    def __post_init__(self):
        if self.outlier_information is None:
            self.outlier_information = {}

@dataclass
class MethodSuggestion:
    """çµ±è¨ˆæ‰‹æ³•ææ¡ˆ"""
    method_name: str
    confidence_score: float
    rationale: str
    assumptions: List[str]
    prerequisites: List[str]
    estimated_computation_time: float
    educational_content: Optional[str] = None
    alternative_methods: List[str] = None
    
    def __post_init__(self):
        if self.alternative_methods is None:
            self.alternative_methods = []

@dataclass
class AssumptionValidationResult:
    """ä»®å®šæ¤œè¨¼çµæœ"""
    method: str
    assumptions_met: Dict[str, bool]
    violation_severity: Dict[str, str]
    corrective_actions: List[str]
    alternative_methods: List[str]
    confidence_in_results: float
    test_statistics: Dict[str, Any] = None
    p_values: Dict[str, float] = None
    
    def __post_init__(self):
        if self.test_statistics is None:
            self.test_statistics = {}
        if self.p_values is None:
            self.p_values = {}

class AssumptionValidator:
    """çµ±è¨ˆçš„ä»®å®šè‡ªå‹•æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ  - SPSSä»¥ä¸Šã®æ©Ÿèƒ½"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.AssumptionValidator")
        
        # ä»®å®šæ¤œè¨¼ãƒ†ã‚¹ãƒˆã®ãƒãƒƒãƒ”ãƒ³ã‚°
        self.assumption_tests = {
            'normality': self._test_normality,
            'homoscedasticity': self._test_homoscedasticity,
            'independence': self._test_independence,
            'linearity': self._test_linearity,
            'multicollinearity': self._test_multicollinearity,
            'outliers': self._test_outliers,
            'sphericity': self._test_sphericity,
            'equal_variances': self._test_equal_variances
        }
        
        # é•åã®é‡è¦åº¦ãƒ¬ãƒ™ãƒ«
        self.severity_levels = {
            'critical': 'ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ« - çµæœã®ä¿¡é ¼æ€§ã«é‡å¤§ãªå½±éŸ¿',
            'major': 'é‡è¦ - çµæœã®è§£é‡ˆã«æ³¨æ„ãŒå¿…è¦',
            'minor': 'è»½å¾® - çµæœã¸ã®å½±éŸ¿ã¯é™å®šçš„',
            'warning': 'è­¦å‘Š - ç¢ºèªãŒæ¨å¥¨ã•ã‚Œã‚‹'
        }
    
    def validate_assumptions(self, method: str, data: pd.DataFrame, 
                           target_col: str = None, group_col: str = None) -> AssumptionValidationResult:
        """çµ±è¨ˆæ‰‹æ³•ã®ä»®å®šã‚’åŒ…æ‹¬çš„ã«æ¤œè¨¼"""
        try:
            # æ‰‹æ³•åˆ¥ã®å¿…è¦ãªä»®å®šã‚’å–å¾—
            required_assumptions = self._get_method_assumptions(method)
            
            assumptions_met = {}
            violation_severity = {}
            corrective_actions = []
            alternative_methods = []
            
            # å„ä»®å®šã‚’ãƒ†ã‚¹ãƒˆ
            for assumption in required_assumptions:
                if assumption in self.assumption_tests:
                    test_result = self.assumption_tests[assumption](data, target_col, group_col)
                    assumptions_met[assumption] = test_result['passed']
                    
                    if not test_result['passed']:
                        violation_severity[assumption] = test_result['severity']
                        corrective_actions.extend(test_result['corrective_actions'])
                        alternative_methods.extend(test_result['alternative_methods'])
                else:
                    # æœªå®Ÿè£…ã®ä»®å®šãƒ†ã‚¹ãƒˆã¯è­¦å‘Šã¨ã—ã¦æ‰±ã†
                    assumptions_met[assumption] = False
                    violation_severity[assumption] = 'warning'
                    corrective_actions.append(f"{assumption}ã®æ¤œè¨¼ã¯æœªå®Ÿè£…ã§ã™")
            
            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            confidence_score = self._calculate_confidence_score(assumptions_met, violation_severity)
            
            # é‡è¤‡ã‚’é™¤å»
            corrective_actions = list(set(corrective_actions))
            alternative_methods = list(set(alternative_methods))
            
            return AssumptionValidationResult(
                method=method,
                assumptions_met=assumptions_met,
                violation_severity=violation_severity,
                corrective_actions=corrective_actions,
                alternative_methods=alternative_methods,
                confidence_in_results=confidence_score
            )
            
        except Exception as e:
            self.logger.error(f"ä»®å®šæ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
            return AssumptionValidationResult(
                method=method,
                assumptions_met={},
                violation_severity={},
                corrective_actions=[f"æ¤œè¨¼ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"],
                alternative_methods=[],
                confidence_in_results=0.0
            )
    
    def _get_method_assumptions(self, method: str) -> List[str]:
        """çµ±è¨ˆæ‰‹æ³•ã«å¿…è¦ãªä»®å®šã‚’å–å¾—"""
        method_assumptions = {
            't_test': ['normality', 'independence', 'homoscedasticity'],
            'anova': ['normality', 'independence', 'homoscedasticity'],
            'linear_regression': ['linearity', 'independence', 'homoscedasticity', 'normality', 'multicollinearity'],
            'logistic_regression': ['independence', 'linearity', 'multicollinearity'],
            'correlation': ['linearity', 'normality'],
            'chi_square': ['independence', 'expected_frequency'],
            'mann_whitney': ['independence'],
            'wilcoxon': ['independence'],
            'kruskal_wallis': ['independence'],
            'repeated_measures_anova': ['normality', 'sphericity', 'independence']
        }
        
        return method_assumptions.get(method, [])
    
    def _test_normality(self, data: pd.DataFrame, target_col: str = None, group_col: str = None) -> Dict[str, Any]:
        """æ­£è¦æ€§æ¤œå®š"""
        try:
            if target_col is None:
                # å…¨ã¦ã®æ•°å€¤åˆ—ã‚’ãƒ†ã‚¹ãƒˆ
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                test_data = data[numeric_cols].dropna()
            else:
                test_data = data[target_col].dropna()
            
            if len(test_data) < 8:
                return {
                    'passed': False,
                    'severity': 'warning',
                    'p_value': None,
                    'test_statistic': None,
                    'corrective_actions': ['ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã¾ã™ï¼ˆn<8ï¼‰'],
                    'alternative_methods': ['ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®š']
                }
            
            # Shapiro-Wilkæ¤œå®šï¼ˆã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºâ‰¤5000ï¼‰
            if isinstance(test_data, pd.Series):
                test_values = test_data.values
            else:
                # è¤‡æ•°åˆ—ã®å ´åˆã¯æœ€åˆã®åˆ—ã‚’ä½¿ç”¨
                test_values = test_data.iloc[:, 0].values
            
            # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’åˆ¶é™
            if len(test_values) > 5000:
                test_values = np.random.choice(test_values, 5000, replace=False)
            
            stat, p_value = stats.shapiro(test_values)
            
            # æ­£è¦æ€§ã®åˆ¤å®šï¼ˆp > 0.05ã§æ­£è¦åˆ†å¸ƒï¼‰
            is_normal = p_value > 0.05
            
            if is_normal:
                return {
                    'passed': True,
                    'severity': None,
                    'p_value': p_value,
                    'test_statistic': stat,
                    'corrective_actions': [],
                    'alternative_methods': []
                }
            else:
                severity = 'critical' if p_value < 0.001 else 'major' if p_value < 0.01 else 'minor'
                return {
                    'passed': False,
                    'severity': severity,
                    'p_value': p_value,
                    'test_statistic': stat,
                    'corrective_actions': [
                        'ãƒ‡ãƒ¼ã‚¿å¤‰æ›ï¼ˆå¯¾æ•°å¤‰æ›ã€å¹³æ–¹æ ¹å¤‰æ›ï¼‰ã‚’æ¤œè¨',
                        'å¤–ã‚Œå€¤ã®é™¤å»ã‚’æ¤œè¨',
                        'ã‚ˆã‚Šå¤§ããªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’å–å¾—'
                    ],
                    'alternative_methods': [
                        'Mann-Whitney Uæ¤œå®š',
                        'Kruskal-Wallisæ¤œå®š',
                        'Wilcoxonç¬¦å·é †ä½æ¤œå®š'
                    ]
                }
                
        except Exception as e:
            return {
                'passed': False,
                'severity': 'warning',
                'p_value': None,
                'test_statistic': None,
                'corrective_actions': [f'æ­£è¦æ€§æ¤œå®šã§ã‚¨ãƒ©ãƒ¼: {str(e)}'],
                'alternative_methods': ['ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®š']
            }
    
    def _test_homoscedasticity(self, data: pd.DataFrame, target_col: str = None, group_col: str = None) -> Dict[str, Any]:
        """ç­‰åˆ†æ•£æ€§æ¤œå®šï¼ˆLeveneæ¤œå®šï¼‰"""
        try:
            if target_col is None or group_col is None:
                return {
                    'passed': False,
                    'severity': 'warning',
                    'corrective_actions': ['ç­‰åˆ†æ•£æ€§æ¤œå®šã«ã¯ç›®çš„å¤‰æ•°ã¨ã‚°ãƒ«ãƒ¼ãƒ—å¤‰æ•°ãŒå¿…è¦ã§ã™'],
                    'alternative_methods': []
                }
            
            # ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã«ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
            groups = []
            for group_name in data[group_col].unique():
                group_data = data[data[group_col] == group_name][target_col].dropna()
                if len(group_data) > 0:
                    groups.append(group_data)
            
            if len(groups) < 2:
                return {
                    'passed': False,
                    'severity': 'critical',
                    'corrective_actions': ['æ¯”è¼ƒã™ã‚‹ã‚°ãƒ«ãƒ¼ãƒ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™'],
                    'alternative_methods': []
                }
            
            # Leveneæ¤œå®š
            stat, p_value = stats.levene(*groups)
            
            # ç­‰åˆ†æ•£æ€§ã®åˆ¤å®šï¼ˆp > 0.05ã§ç­‰åˆ†æ•£ï¼‰
            is_homoscedastic = p_value > 0.05
            
            if is_homoscedastic:
                return {
                    'passed': True,
                    'severity': None,
                    'p_value': p_value,
                    'test_statistic': stat,
                    'corrective_actions': [],
                    'alternative_methods': []
                }
            else:
                severity = 'critical' if p_value < 0.001 else 'major' if p_value < 0.01 else 'minor'
                return {
                    'passed': False,
                    'severity': severity,
                    'p_value': p_value,
                    'test_statistic': stat,
                    'corrective_actions': [
                        'Welchã®tæ¤œå®šã‚’ä½¿ç”¨ï¼ˆç­‰åˆ†æ•£ã‚’ä»®å®šã—ãªã„ï¼‰',
                        'ãƒ‡ãƒ¼ã‚¿å¤‰æ›ã‚’æ¤œè¨',
                        'å¤–ã‚Œå€¤ã®ç¢ºèªã¨å‡¦ç†'
                    ],
                    'alternative_methods': [
                        'Welch tæ¤œå®š',
                        'Mann-Whitney Uæ¤œå®š',
                        'ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—æ¤œå®š'
                    ]
                }
                
        except Exception as e:
            return {
                'passed': False,
                'severity': 'warning',
                'corrective_actions': [f'ç­‰åˆ†æ•£æ€§æ¤œå®šã§ã‚¨ãƒ©ãƒ¼: {str(e)}'],
                'alternative_methods': ['ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®š']
            }
    
    def _test_independence(self, data: pd.DataFrame, target_col: str = None, group_col: str = None) -> Dict[str, Any]:
        """ç‹¬ç«‹æ€§ã®æ¤œå®šï¼ˆDurbin-Watsonæ¤œå®šãªã©ï¼‰"""
        try:
            # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯Durbin-Watsonæ¤œå®š
            if target_col and target_col in data.columns:
                test_data = data[target_col].dropna()
                
                if len(test_data) < 10:
                    return {
                        'passed': False,
                        'severity': 'warning',
                        'corrective_actions': ['ç‹¬ç«‹æ€§æ¤œå®šã«ã¯ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™'],
                        'alternative_methods': []
                    }
                
                # ç°¡æ˜“çš„ãªè‡ªå·±ç›¸é–¢æ¤œå®š
                from statsmodels.stats.diagnostic import acorr_ljungbox
                
                try:
                    ljung_box_result = acorr_ljungbox(test_data, lags=min(10, len(test_data)//4), return_df=True)
                    p_values = ljung_box_result['lb_pvalue']
                    min_p_value = p_values.min()
                    
                    # ç‹¬ç«‹æ€§ã®åˆ¤å®šï¼ˆp > 0.05ã§ç‹¬ç«‹ï¼‰
                    is_independent = min_p_value > 0.05
                    
                    if is_independent:
                        return {
                            'passed': True,
                            'severity': None,
                            'p_value': min_p_value,
                            'corrective_actions': [],
                            'alternative_methods': []
                        }
                    else:
                        severity = 'major' if min_p_value < 0.01 else 'minor'
                        return {
                            'passed': False,
                            'severity': severity,
                            'p_value': min_p_value,
                            'corrective_actions': [
                                'æ™‚ç³»åˆ—åˆ†ææ‰‹æ³•ã®ä½¿ç”¨ã‚’æ¤œè¨',
                                'ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ç¢ºèª',
                                'ãƒ‡ãƒ¼ã‚¿åé›†æ–¹æ³•ã®è¦‹ç›´ã—'
                            ],
                            'alternative_methods': [
                                'æ™‚ç³»åˆ—åˆ†æ',
                                'æ··åˆåŠ¹æœãƒ¢ãƒ‡ãƒ«',
                                'GEEï¼ˆä¸€èˆ¬åŒ–æ¨å®šæ–¹ç¨‹å¼ï¼‰'
                            ]
                        }
                        
                except ImportError:
                    # statsmodelsãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ç°¡æ˜“ãƒã‚§ãƒƒã‚¯
                    return {
                        'passed': True,  # ä¿å®ˆçš„ã«é€šã™
                        'severity': 'warning',
                        'corrective_actions': ['ç‹¬ç«‹æ€§ã®è©³ç´°æ¤œå®šã«ã¯è¿½åŠ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™'],
                        'alternative_methods': []
                    }
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ç‹¬ç«‹æ€§ã‚’ä»®å®š
            return {
                'passed': True,
                'severity': None,
                'corrective_actions': [],
                'alternative_methods': []
            }
            
        except Exception as e:
            return {
                'passed': True,  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¿å®ˆçš„ã«é€šã™
                'severity': 'warning',
                'corrective_actions': [f'ç‹¬ç«‹æ€§æ¤œå®šã§ã‚¨ãƒ©ãƒ¼: {str(e)}'],
                'alternative_methods': []
            }
    
    def _test_linearity(self, data: pd.DataFrame, target_col: str = None, group_col: str = None) -> Dict[str, Any]:
        """ç·šå½¢æ€§ã®æ¤œå®š"""
        try:
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            
            if len(numeric_cols) < 2:
                return {
                    'passed': False,
                    'severity': 'warning',
                    'corrective_actions': ['ç·šå½¢æ€§æ¤œå®šã«ã¯2ã¤ä»¥ä¸Šã®æ•°å€¤å¤‰æ•°ãŒå¿…è¦ã§ã™'],
                    'alternative_methods': []
                }
            
            # ç›¸é–¢ä¿‚æ•°ã«ã‚ˆã‚‹ç·šå½¢æ€§ã®ç°¡æ˜“ãƒã‚§ãƒƒã‚¯
            correlations = data[numeric_cols].corr()
            
            # å¯¾è§’æˆåˆ†ã‚’é™¤ã„ãŸç›¸é–¢ä¿‚æ•°ã®çµ¶å¯¾å€¤
            corr_values = []
            for i in range(len(correlations)):
                for j in range(i+1, len(correlations)):
                    corr_values.append(abs(correlations.iloc[i, j]))
            
            if not corr_values:
                return {
                    'passed': True,
                    'severity': None,
                    'corrective_actions': [],
                    'alternative_methods': []
                }
            
            max_correlation = max(corr_values)
            
            # ç·šå½¢é–¢ä¿‚ã®å¼·ã•ã§åˆ¤å®š
            if max_correlation > 0.3:  # ä¸­ç¨‹åº¦ä»¥ä¸Šã®ç·šå½¢é–¢ä¿‚ãŒã‚ã‚Œã°é€šã™
                return {
                    'passed': True,
                    'severity': None,
                    'max_correlation': max_correlation,
                    'corrective_actions': [],
                    'alternative_methods': []
                }
            else:
                return {
                    'passed': False,
                    'severity': 'minor',
                    'max_correlation': max_correlation,
                    'corrective_actions': [
                        'æ•£å¸ƒå›³ã§é–¢ä¿‚æ€§ã‚’è¦–è¦šçš„ã«ç¢ºèª',
                        'éç·šå½¢å¤‰æ›ã‚’æ¤œè¨',
                        'ã‚¹ãƒ—ãƒ©ã‚¤ãƒ³å›å¸°ã‚„å¤šé …å¼å›å¸°ã‚’æ¤œè¨'
                    ],
                    'alternative_methods': [
                        'ã‚¹ãƒ”ã‚¢ãƒãƒ³ç›¸é–¢',
                        'éç·šå½¢å›å¸°',
                        'æ±ºå®šæœ¨ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•'
                    ]
                }
                
        except Exception as e:
            return {
                'passed': True,  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¿å®ˆçš„ã«é€šã™
                'severity': 'warning',
                'corrective_actions': [f'ç·šå½¢æ€§æ¤œå®šã§ã‚¨ãƒ©ãƒ¼: {str(e)}'],
                'alternative_methods': []
            }
    
    def _test_multicollinearity(self, data: pd.DataFrame, target_col: str = None, group_col: str = None) -> Dict[str, Any]:
        """å¤šé‡å…±ç·šæ€§ã®æ¤œå®šï¼ˆVIFè¨ˆç®—ï¼‰"""
        try:
            numeric_cols = data.select_dtypes(include=[np.number]).columns
            
            # ç›®çš„å¤‰æ•°ã‚’é™¤å¤–
            if target_col and target_col in numeric_cols:
                predictor_cols = [col for col in numeric_cols if col != target_col]
            else:
                predictor_cols = list(numeric_cols)
            
            if len(predictor_cols) < 2:
                return {
                    'passed': True,
                    'severity': None,
                    'corrective_actions': [],
                    'alternative_methods': []
                }
            
            # ç›¸é–¢è¡Œåˆ—ã«ã‚ˆã‚‹ç°¡æ˜“å¤šé‡å…±ç·šæ€§ãƒã‚§ãƒƒã‚¯
            corr_matrix = data[predictor_cols].corr()
            
            # é«˜ã„ç›¸é–¢ï¼ˆ|r| > 0.8ï¼‰ã‚’ãƒã‚§ãƒƒã‚¯
            high_correlations = []
            for i in range(len(corr_matrix)):
                for j in range(i+1, len(corr_matrix)):
                    corr_value = abs(corr_matrix.iloc[i, j])
                    if corr_value > 0.8:
                        high_correlations.append({
                            'var1': corr_matrix.index[i],
                            'var2': corr_matrix.columns[j],
                            'correlation': corr_value
                        })
            
            if not high_correlations:
                return {
                    'passed': True,
                    'severity': None,
                    'corrective_actions': [],
                    'alternative_methods': []
                }
            else:
                severity = 'major' if any(hc['correlation'] > 0.9 for hc in high_correlations) else 'minor'
                
                corrective_actions = [
                    'é«˜ã„ç›¸é–¢ã‚’æŒã¤å¤‰æ•°ã®ä¸€æ–¹ã‚’é™¤å»',
                    'ä¸»æˆåˆ†åˆ†æã«ã‚ˆã‚‹æ¬¡å…ƒå‰Šæ¸›',
                    'ãƒªãƒƒã‚¸å›å¸°ã‚„Lassoå›å¸°ã®ä½¿ç”¨'
                ]
                
                return {
                    'passed': False,
                    'severity': severity,
                    'high_correlations': high_correlations,
                    'corrective_actions': corrective_actions,
                    'alternative_methods': [
                        'ãƒªãƒƒã‚¸å›å¸°',
                        'Lassoå›å¸°',
                        'ä¸»æˆåˆ†å›å¸°'
                    ]
                }
                
        except Exception as e:
            return {
                'passed': True,  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¿å®ˆçš„ã«é€šã™
                'severity': 'warning',
                'corrective_actions': [f'å¤šé‡å…±ç·šæ€§æ¤œå®šã§ã‚¨ãƒ©ãƒ¼: {str(e)}'],
                'alternative_methods': []
            }
    
    def _test_outliers(self, data: pd.DataFrame, target_col: str = None, group_col: str = None) -> Dict[str, Any]:
        """å¤–ã‚Œå€¤ã®æ¤œå‡º"""
        try:
            if target_col:
                test_data = data[target_col].dropna()
            else:
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) == 0:
                    return {
                        'passed': True,
                        'severity': None,
                        'corrective_actions': [],
                        'alternative_methods': []
                    }
                test_data = data[numeric_cols[0]].dropna()
            
            if len(test_data) < 10:
                return {
                    'passed': True,
                    'severity': 'warning',
                    'corrective_actions': ['å¤–ã‚Œå€¤æ¤œå‡ºã«ã¯ã‚ˆã‚Šå¤šãã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ã§ã™'],
                    'alternative_methods': []
                }
            
            # IQRæ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º
            Q1 = test_data.quantile(0.25)
            Q3 = test_data.quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers = test_data[(test_data < lower_bound) | (test_data > upper_bound)]
            outlier_ratio = len(outliers) / len(test_data)
            
            if outlier_ratio <= 0.05:  # 5%ä»¥ä¸‹ãªã‚‰è¨±å®¹
                return {
                    'passed': True,
                    'severity': None,
                    'outlier_count': len(outliers),
                    'outlier_ratio': outlier_ratio,
                    'corrective_actions': [],
                    'alternative_methods': []
                }
            else:
                severity = 'major' if outlier_ratio > 0.1 else 'minor'
                return {
                    'passed': False,
                    'severity': severity,
                    'outlier_count': len(outliers),
                    'outlier_ratio': outlier_ratio,
                    'corrective_actions': [
                        'å¤–ã‚Œå€¤ã®åŸå› ã‚’èª¿æŸ»',
                        'å¤–ã‚Œå€¤ã®é™¤å»ã¾ãŸã¯å¤‰æ›ã‚’æ¤œè¨',
                        'ãƒ­ãƒã‚¹ãƒˆçµ±è¨ˆæ‰‹æ³•ã®ä½¿ç”¨'
                    ],
                    'alternative_methods': [
                        'ãƒ­ãƒã‚¹ãƒˆå›å¸°',
                        'ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®š',
                        'ãƒ–ãƒ¼ãƒˆã‚¹ãƒˆãƒ©ãƒƒãƒ—æ³•'
                    ]
                }
                
        except Exception as e:
            return {
                'passed': True,  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¿å®ˆçš„ã«é€šã™
                'severity': 'warning',
                'corrective_actions': [f'å¤–ã‚Œå€¤æ¤œå®šã§ã‚¨ãƒ©ãƒ¼: {str(e)}'],
                'alternative_methods': []
            }
    
    def _test_sphericity(self, data: pd.DataFrame, target_col: str = None, group_col: str = None) -> Dict[str, Any]:
        """çƒé¢æ€§ã®æ¤œå®šï¼ˆåå¾©æ¸¬å®šANOVAç”¨ï¼‰"""
        # ç°¡æ˜“å®Ÿè£…ï¼šå®Ÿéš›ã®Mauchlyæ¤œå®šã¯è¤‡é›‘ãªãŸã‚ã€è­¦å‘Šãƒ¬ãƒ™ãƒ«ã§è¿”ã™
        return {
            'passed': True,
            'severity': 'warning',
            'corrective_actions': ['çƒé¢æ€§ã®è©³ç´°æ¤œå®šã¯æ‰‹å‹•ã§ç¢ºèªã—ã¦ãã ã•ã„'],
            'alternative_methods': ['Greenhouse-Geisserè£œæ­£', 'Huynh-Feldtè£œæ­£']
        }
    
    def _test_equal_variances(self, data: pd.DataFrame, target_col: str = None, group_col: str = None) -> Dict[str, Any]:
        """ç­‰åˆ†æ•£æ€§æ¤œå®šï¼ˆhomoscedasticityã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼‰"""
        return self._test_homoscedasticity(data, target_col, group_col)
    
    def _calculate_confidence_score(self, assumptions_met: Dict[str, bool], 
                                  violation_severity: Dict[str, str]) -> float:
        """ä»®å®šæ¤œè¨¼çµæœã«åŸºã¥ãä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        if not assumptions_met:
            return 0.5  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        total_assumptions = len(assumptions_met)
        met_assumptions = sum(assumptions_met.values())
        
        # åŸºæœ¬ã‚¹ã‚³ã‚¢ï¼ˆæº€ãŸã•ã‚ŒãŸä»®å®šã®å‰²åˆï¼‰
        base_score = met_assumptions / total_assumptions
        
        # é•åã®é‡è¦åº¦ã«ã‚ˆã‚‹æ¸›ç‚¹
        severity_penalties = {
            'critical': 0.3,
            'major': 0.2,
            'minor': 0.1,
            'warning': 0.05
        }
        
        penalty = 0.0
        for assumption, severity in violation_severity.items():
            penalty += severity_penalties.get(severity, 0.0)
        
        # æœ€çµ‚ã‚¹ã‚³ã‚¢ï¼ˆ0.0-1.0ã®ç¯„å›²ï¼‰
        final_score = max(0.0, min(1.0, base_score - penalty))
        
        return final_score

class StatisticalMethodAdvisor:
    """çµ±è¨ˆæ‰‹æ³•ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ - çŸ¥çš„æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.StatisticalMethodAdvisor")
        
        # çµ±è¨ˆæ‰‹æ³•ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        self.method_database = self._initialize_method_database()
        
        # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å™¨
        try:
            from data_preprocessing import DataPreprocessor
            self.data_preprocessor = DataPreprocessor()
        except ImportError:
            self.data_preprocessor = None
            self.logger.warning("DataPreprocessorãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        
        # ä»®å®šæ¤œè¨¼å™¨
        self.assumption_validator = AssumptionValidator()
    
    def _initialize_method_database(self) -> Dict[str, Dict[str, Any]]:
        """çµ±è¨ˆæ‰‹æ³•ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
        return {
            'descriptive_stats': {
                'name': 'è¨˜è¿°çµ±è¨ˆ',
                'category': 'descriptive',
                'assumptions': [],
                'data_requirements': {
                    'min_sample_size': 1,
                    'data_types': ['numeric', 'categorical'],
                    'missing_data_tolerance': 0.5
                },
                'use_cases': ['ãƒ‡ãƒ¼ã‚¿æ¦‚è¦', 'åŸºæœ¬çµ±è¨ˆé‡', 'åˆ†å¸ƒç¢ºèª'],
                'computation_complexity': 'low',
                'educational_level': 'beginner'
            },
            't_test': {
                'name': 'tæ¤œå®š',
                'category': 'inferential',
                'assumptions': ['normality', 'independence', 'homoscedasticity'],
                'data_requirements': {
                    'min_sample_size': 30,
                    'data_types': ['numeric'],
                    'missing_data_tolerance': 0.1
                },
                'use_cases': ['å¹³å‡å€¤æ¯”è¼ƒ', 'ç¾¤é–“å·®æ¤œå®š'],
                'computation_complexity': 'low',
                'educational_level': 'intermediate'
            },
            'anova': {
                'name': 'åˆ†æ•£åˆ†æ',
                'category': 'inferential',
                'assumptions': ['normality', 'independence', 'homoscedasticity'],
                'data_requirements': {
                    'min_sample_size': 20,
                    'data_types': ['numeric'],
                    'missing_data_tolerance': 0.1
                },
                'use_cases': ['å¤šç¾¤æ¯”è¼ƒ', 'è¦å› åŠ¹æœæ¤œå®š'],
                'computation_complexity': 'medium',
                'educational_level': 'intermediate'
            },
            'wilcoxon_test': {
                'name': 'Wilcoxonæ¤œå®š',
                'category': 'inferential',
                'assumptions': ['independence'],
                'data_requirements': {
                    'min_sample_size': 6,
                    'data_types': ['numeric'],
                    'missing_data_tolerance': 0.1
                },
                'use_cases': ['å°ã‚µãƒ³ãƒ—ãƒ«æ¯”è¼ƒ', 'ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®š'],
                'computation_complexity': 'low',
                'educational_level': 'intermediate'
            },
            'mann_whitney_test': {
                'name': 'Mann-Whitney Uæ¤œå®š',
                'category': 'inferential',
                'assumptions': ['independence'],
                'data_requirements': {
                    'min_sample_size': 8,
                    'data_types': ['numeric'],
                    'missing_data_tolerance': 0.1
                },
                'use_cases': ['å°ã‚µãƒ³ãƒ—ãƒ«ç¾¤é–“æ¯”è¼ƒ', 'ãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¤œå®š'],
                'computation_complexity': 'low',
                'educational_level': 'intermediate'
            },
            'correlation': {
                'name': 'ç›¸é–¢åˆ†æ',
                'category': 'descriptive',
                'assumptions': ['linearity'],
                'data_requirements': {
                    'min_sample_size': 10,
                    'data_types': ['numeric'],
                    'missing_data_tolerance': 0.2
                },
                'use_cases': ['å¤‰æ•°é–“é–¢ä¿‚', 'é–¢é€£æ€§åˆ†æ'],
                'computation_complexity': 'low',
                'educational_level': 'beginner'
            },
            'linear_regression': {
                'name': 'ç·šå½¢å›å¸°',
                'category': 'predictive',
                'assumptions': ['linearity', 'independence', 'homoscedasticity', 'normality_residuals'],
                'data_requirements': {
                    'min_sample_size': 50,
                    'data_types': ['numeric'],
                    'missing_data_tolerance': 0.05
                },
                'use_cases': ['äºˆæ¸¬', 'é–¢ä¿‚æ€§ãƒ¢ãƒ‡ãƒªãƒ³ã‚°', 'å› æœæ¨è«–'],
                'computation_complexity': 'medium',
                'educational_level': 'intermediate'
            },
            'logistic_regression': {
                'name': 'ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°',
                'category': 'predictive',
                'assumptions': ['independence', 'linearity_logit'],
                'data_requirements': {
                    'min_sample_size': 100,
                    'data_types': ['numeric', 'categorical'],
                    'missing_data_tolerance': 0.05
                },
                'use_cases': ['åˆ†é¡', 'ç¢ºç‡äºˆæ¸¬', 'ã‚ªãƒƒã‚ºæ¯”åˆ†æ'],
                'computation_complexity': 'medium',
                'educational_level': 'advanced'
            },
            'chi_square': {
                'name': 'ã‚«ã‚¤äºŒä¹—æ¤œå®š',
                'category': 'inferential',
                'assumptions': ['independence', 'expected_frequency'],
                'data_requirements': {
                    'min_sample_size': 20,
                    'data_types': ['categorical'],
                    'missing_data_tolerance': 0.1
                },
                'use_cases': ['ç‹¬ç«‹æ€§æ¤œå®š', 'é©åˆåº¦æ¤œå®š'],
                'computation_complexity': 'low',
                'educational_level': 'intermediate'
            },
            'machine_learning': {
                'name': 'æ©Ÿæ¢°å­¦ç¿’',
                'category': 'predictive',
                'assumptions': [],
                'data_requirements': {
                    'min_sample_size': 1000,
                    'data_types': ['numeric', 'categorical'],
                    'missing_data_tolerance': 0.1
                },
                'use_cases': ['è¤‡é›‘ãªäºˆæ¸¬', 'ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜', 'éç·šå½¢é–¢ä¿‚'],
                'computation_complexity': 'high',
                'educational_level': 'advanced'
            }
        }
    
    def analyze_data_characteristics(self, data: pd.DataFrame) -> DataCharacteristics:
        """ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã‚’åˆ†æ"""
        try:
            # åŸºæœ¬æƒ…å ±
            n_rows, n_columns = data.shape
            
            # åˆ—ã‚¿ã‚¤ãƒ—åˆ†æ
            column_types = {}
            for col in data.columns:
                if data[col].dtype in ['int64', 'float64']:
                    column_types[col] = 'numeric'
                elif data[col].dtype in ['object', 'category']:
                    column_types[col] = 'categorical'
                elif data[col].dtype in ['datetime64[ns]']:
                    column_types[col] = 'datetime'
                else:
                    column_types[col] = 'other'
            
            # æ¬ æãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³
            missing_data_pattern = {}
            for col in data.columns:
                missing_ratio = data[col].isnull().sum() / len(data)
                missing_data_pattern[col] = missing_ratio
            
            # åˆ†å¸ƒç‰¹æ€§ï¼ˆæ•°å€¤åˆ—ã®ã¿ï¼‰
            distribution_characteristics = {}
            numeric_cols = [col for col, dtype in column_types.items() if dtype == 'numeric']
            
            for col in numeric_cols:
                col_data = data[col].dropna()
                if len(col_data) > 0:
                    distribution_characteristics[col] = {
                        'mean': float(col_data.mean()),
                        'std': float(col_data.std()),
                        'skewness': float(col_data.skew()),
                        'kurtosis': float(col_data.kurtosis()),
                        'min': float(col_data.min()),
                        'max': float(col_data.max()),
                        'q25': float(col_data.quantile(0.25)),
                        'q50': float(col_data.quantile(0.50)),
                        'q75': float(col_data.quantile(0.75))
                    }
            
            # ç›¸é–¢æ§‹é€ ï¼ˆæ•°å€¤åˆ—ã®ã¿ï¼‰
            correlation_structure = None
            if len(numeric_cols) > 1:
                numeric_data = data[numeric_cols].dropna()
                if len(numeric_data) > 0:
                    correlation_structure = numeric_data.corr().values
            
            # å¤–ã‚Œå€¤æƒ…å ±
            outlier_information = {}
            if self.data_preprocessor:
                outlier_result = self.data_preprocessor.detect_outliers(data)
                if outlier_result.get('success', False):
                    outlier_information = outlier_result
            
            # ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
            data_quality_score = self._calculate_data_quality_score(
                missing_data_pattern, outlier_information, n_rows, n_columns
            )
            
            return DataCharacteristics(
                n_rows=n_rows,
                n_columns=n_columns,
                column_types=column_types,
                missing_data_pattern=missing_data_pattern,
                distribution_characteristics=distribution_characteristics,
                correlation_structure=correlation_structure,
                outlier_information=outlier_information,
                data_quality_score=data_quality_score
            )
            
        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            return DataCharacteristics(
                n_rows=0,
                n_columns=0,
                column_types={},
                missing_data_pattern={},
                distribution_characteristics={}
            )
    
    def _calculate_data_quality_score(self, missing_pattern: Dict[str, float], 
                                    outlier_info: Dict[str, Any], 
                                    n_rows: int, n_columns: int) -> float:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆ0-1ã®ç¯„å›²ï¼‰"""
        score = 1.0
        
        # æ¬ æãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹æ¸›ç‚¹
        avg_missing_ratio = sum(missing_pattern.values()) / len(missing_pattern) if missing_pattern else 0
        score -= avg_missing_ratio * 0.3
        
        # å¤–ã‚Œå€¤ã«ã‚ˆã‚‹æ¸›ç‚¹
        if outlier_info.get('success', False):
            consensus_outliers = outlier_info.get('consensus_outliers', {})
            total_outlier_ratio = 0
            for col_info in consensus_outliers.values():
                total_outlier_ratio += len(col_info.get('indices', [])) / n_rows
            avg_outlier_ratio = total_outlier_ratio / len(consensus_outliers) if consensus_outliers else 0
            score -= avg_outlier_ratio * 0.2
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹èª¿æ•´
        if n_rows < 30:
            score -= 0.2
        elif n_rows < 100:
            score -= 0.1
        
        return max(0.0, min(1.0, score))
    
    def suggest_methods(self, data_chars: DataCharacteristics, 
                       research_question: str = "", 
                       user_expertise: str = "intermediate") -> List[MethodSuggestion]:
        """çµ±è¨ˆæ‰‹æ³•ã‚’ææ¡ˆ"""
        try:
            suggestions = []
            
            # ç ”ç©¶è³ªå•ã®åˆ†æ
            question_intent = self._analyze_research_question(research_question)
            
            # å„æ‰‹æ³•ã®é©åˆæ€§ã‚’è©•ä¾¡
            for method_id, method_info in self.method_database.items():
                compatibility_score = self._calculate_method_compatibility(
                    method_info, data_chars, question_intent, user_expertise
                )
                
                # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã‚ˆã‚Šé«˜ã„é–¾å€¤ã‚’é©ç”¨
                min_sample_size = method_info['data_requirements']['min_sample_size']
                if data_chars.n_rows < min_sample_size:
                    # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯å¤§å¹…ã«é«˜ã„é–¾å€¤
                    threshold = 0.8
                else:
                    threshold = 0.3
                
                if compatibility_score > threshold:  # å‹•çš„é–¾å€¤ã§æ‰‹æ³•ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                    suggestion = MethodSuggestion(
                        method_name=method_id,  # è‹±èªã®ã‚­ãƒ¼ã‚’ä½¿ç”¨
                        confidence_score=compatibility_score,
                        rationale=self._generate_rationale(method_info, data_chars, question_intent),
                        assumptions=method_info['assumptions'],
                        prerequisites=self._generate_prerequisites(method_info, data_chars),
                        estimated_computation_time=self._estimate_computation_time(method_info, data_chars),
                        educational_content=self._generate_educational_content(method_info, user_expertise),
                        alternative_methods=self._find_alternative_methods(method_id, method_info)
                    )
                    suggestions.append(suggestion)
            
            # ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
            suggestions.sort(key=lambda x: x.confidence_score, reverse=True)
            
            return suggestions[:5]  # ä¸Šä½5ã¤ã‚’è¿”ã™
            
        except Exception as e:
            self.logger.error(f"æ‰‹æ³•ææ¡ˆã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _analyze_research_question(self, question: str) -> Dict[str, Any]:
        """ç ”ç©¶è³ªå•ã‚’åˆ†æ"""
        question_lower = question.lower()
        
        intent = {
            'type': 'descriptive',  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            'keywords': [],
            'variables_mentioned': [],
            'analysis_goal': 'exploration'
        }
        
        # åˆ†æã‚¿ã‚¤ãƒ—ã®åˆ¤å®šï¼ˆå„ªå…ˆé †ä½ä»˜ãï¼‰
        if any(word in question_lower for word in ['äºˆæ¸¬', 'predict', 'äºˆæƒ³', 'forecast', 'å°†æ¥', 'future']):
            intent['type'] = 'predictive'
            intent['analysis_goal'] = 'prediction'
        elif any(word in question_lower for word in ['æ¯”è¼ƒ', 'compare', 'å·®', 'difference', 'æ¤œå®š', 'test', 'æœ‰æ„', 'significant', 'ã‚°ãƒ«ãƒ¼ãƒ—é–“', 'å¹³å‡å€¤']):
            intent['type'] = 'inferential'
            intent['analysis_goal'] = 'comparison'
        elif any(word in question_lower for word in ['è¦ç´„', 'summary', 'çµ±è¨ˆé‡', 'statistics', 'åŸºæœ¬', 'basic', 'è¨˜è¿°', 'descriptive']):
            intent['type'] = 'descriptive'
            intent['analysis_goal'] = 'description'
        elif any(word in question_lower for word in ['é–¢ä¿‚', 'relationship', 'ç›¸é–¢', 'correlation', 'é–¢é€£', 'association']):
            intent['type'] = 'descriptive'
            intent['analysis_goal'] = 'relationship'
        elif any(word in question_lower for word in ['ã‚«ãƒ†ã‚´ãƒª', 'category', 'ç‹¬ç«‹æ€§', 'independence', 'ã‚¯ãƒ­ã‚¹', 'cross']):
            intent['type'] = 'inferential'
            intent['analysis_goal'] = 'categorical_analysis'
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        statistical_keywords = [
            'å¹³å‡', 'mean', 'ä¸­å¤®å€¤', 'median', 'åˆ†æ•£', 'variance',
            'ç›¸é–¢', 'correlation', 'å›å¸°', 'regression', 'åˆ†é¡', 'classification',
            'tæ¤œå®š', 't_test', 'anova', 'åˆ†æ•£åˆ†æ', 'ã‚«ã‚¤äºŒä¹—', 'chi_square'
        ]
        
        for keyword in statistical_keywords:
            if keyword in question_lower:
                intent['keywords'].append(keyword)
        
        return intent
    
    def _calculate_method_compatibility(self, method_info: Dict[str, Any], 
                                      data_chars: DataCharacteristics,
                                      question_intent: Dict[str, Any],
                                      user_expertise: str) -> float:
        """æ‰‹æ³•ã®é©åˆæ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        score = 0.0
        
        # 1. ãƒ‡ãƒ¼ã‚¿è¦ä»¶ã¨ã®é©åˆæ€§ï¼ˆ40%ï¼‰
        data_requirements = method_info['data_requirements']
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆå³æ ¼ãªè©•ä¾¡ï¼‰
        if data_chars.n_rows >= data_requirements['min_sample_size']:
            score += 0.15
        else:
            # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯å¤§å¹…æ¸›ç‚¹
            ratio = data_chars.n_rows / data_requirements['min_sample_size']
            if ratio < 0.5:  # å¿…è¦ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®50%æœªæº€ã®å ´åˆ
                score += 0.0  # åŠ ç‚¹ãªã—
            else:
                score += 0.15 * ratio * 0.5  # å¤§å¹…æ¸›ç‚¹
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—
        required_types = data_requirements['data_types']
        available_types = set(data_chars.column_types.values())
        type_match = len(set(required_types) & available_types) / len(required_types)
        score += 0.15 * type_match
        
        # æ¬ æãƒ‡ãƒ¼ã‚¿è¨±å®¹åº¦
        avg_missing = sum(data_chars.missing_data_pattern.values()) / len(data_chars.missing_data_pattern) if data_chars.missing_data_pattern else 0
        if avg_missing <= data_requirements['missing_data_tolerance']:
            score += 0.1
        else:
            score += 0.1 * (1 - (avg_missing - data_requirements['missing_data_tolerance']))
        
        # 2. ç ”ç©¶è³ªå•ã¨ã®é©åˆæ€§ï¼ˆ30%ï¼‰
        if method_info['category'] == question_intent['type']:
            score += 0.2
        elif method_info['category'] == 'descriptive' and question_intent['type'] in ['inferential', 'predictive']:
            score += 0.1  # è¨˜è¿°çµ±è¨ˆã¯å¸¸ã«æœ‰ç”¨
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒ
        method_keywords = method_info['use_cases']
        keyword_match = any(keyword in ' '.join(method_keywords).lower() 
                          for keyword in question_intent['keywords'])
        if keyword_match:
            score += 0.1
        
        # 3. ãƒ¦ãƒ¼ã‚¶ãƒ¼å°‚é–€ãƒ¬ãƒ™ãƒ«ã¨ã®é©åˆæ€§ï¼ˆ20%ï¼‰
        expertise_levels = {'beginner': 1, 'intermediate': 2, 'advanced': 3}
        method_level = expertise_levels.get(method_info['educational_level'], 2)
        user_level = expertise_levels.get(user_expertise, 2)
        
        level_diff = abs(method_level - user_level)
        if level_diff == 0:
            score += 0.2
        elif level_diff == 1:
            score += 0.1
        # level_diff >= 2 ã®å ´åˆã¯åŠ ç‚¹ãªã—
        
        # 4. ãƒ‡ãƒ¼ã‚¿å“è³ªã¨ã®é©åˆæ€§ï¼ˆ10%ï¼‰
        if data_chars.data_quality_score > 0.8:
            score += 0.1
        elif data_chars.data_quality_score > 0.6:
            score += 0.05
        
        return min(1.0, score)
    
    def _generate_rationale(self, method_info: Dict[str, Any], 
                          data_chars: DataCharacteristics,
                          question_intent: Dict[str, Any]) -> str:
        """ææ¡ˆç†ç”±ã‚’ç”Ÿæˆ"""
        rationale_parts = []
        
        # ãƒ‡ãƒ¼ã‚¿é©åˆæ€§
        if data_chars.n_rows >= method_info['data_requirements']['min_sample_size']:
            rationale_parts.append(f"ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆ{data_chars.n_rows}ï¼‰ãŒååˆ†ã§ã™")
        
        # åˆ†æç›®çš„é©åˆæ€§
        if method_info['category'] == question_intent['type']:
            rationale_parts.append(f"{question_intent['type']}åˆ†æã«é©ã—ã¦ã„ã¾ã™")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—é©åˆæ€§
        required_types = method_info['data_requirements']['data_types']
        available_types = list(set(data_chars.column_types.values()))
        if set(required_types) & set(available_types):
            rationale_parts.append(f"ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ï¼ˆ{', '.join(available_types)}ï¼‰ã«å¯¾å¿œã—ã¦ã„ã¾ã™")
        
        return "ã€‚".join(rationale_parts) + "ã€‚"
    
    def _generate_prerequisites(self, method_info: Dict[str, Any], 
                              data_chars: DataCharacteristics) -> List[str]:
        """å‰ææ¡ä»¶ã‚’ç”Ÿæˆ"""
        prerequisites = []
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¦ä»¶
        min_size = method_info['data_requirements']['min_sample_size']
        if data_chars.n_rows < min_size:
            prerequisites.append(f"ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’{min_size}ä»¥ä¸Šã«å¢—ã‚„ã™å¿…è¦ãŒã‚ã‚Šã¾ã™")
        
        # æ¬ æãƒ‡ãƒ¼ã‚¿å‡¦ç†
        avg_missing = sum(data_chars.missing_data_pattern.values()) / len(data_chars.missing_data_pattern) if data_chars.missing_data_pattern else 0
        tolerance = method_info['data_requirements']['missing_data_tolerance']
        if avg_missing > tolerance:
            prerequisites.append("æ¬ æãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ãŒå¿…è¦ã§ã™")
        
        # å¤–ã‚Œå€¤å‡¦ç†
        if data_chars.outlier_information.get('success', False):
            consensus_outliers = data_chars.outlier_information.get('consensus_outliers', {})
            if consensus_outliers:
                prerequisites.append("å¤–ã‚Œå€¤ã®ç¢ºèªãƒ»å‡¦ç†ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        
        return prerequisites
    
    def _estimate_computation_time(self, method_info: Dict[str, Any], 
                                 data_chars: DataCharacteristics) -> float:
        """è¨ˆç®—æ™‚é–“ã‚’æ¨å®šï¼ˆç§’ï¼‰"""
        base_times = {
            'low': 0.1,
            'medium': 1.0,
            'high': 10.0
        }
        
        base_time = base_times.get(method_info['computation_complexity'], 1.0)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹èª¿æ•´ï¼ˆã‚ˆã‚Šæ•æ„Ÿã«ï¼‰
        size_factor = max(1.0, (data_chars.n_rows / 100) ** 0.5)
        column_factor = max(1.0, (data_chars.n_columns / 5) ** 0.5)
        
        return base_time * size_factor * column_factor
    
    def _generate_educational_content(self, method_info: Dict[str, Any], 
                                    user_expertise: str) -> str:
        """æ•™è‚²ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ"""
        if user_expertise == 'beginner':
            # åˆå¿ƒè€…å‘ã‘ï¼šè©³ç´°ã§åˆ†ã‹ã‚Šã‚„ã™ã„èª¬æ˜
            base_content = f"{method_info['name']}ã¯{method_info['category']}åˆ†æã®æ‰‹æ³•ã§ã™ã€‚"
            use_cases = f"ä¸»ãªç”¨é€”ã¯{', '.join(method_info['use_cases'])}ã§ã™ã€‚"
            complexity = f"è¨ˆç®—ã®è¤‡é›‘ã•ã¯{method_info['computation_complexity']}ãƒ¬ãƒ™ãƒ«ã§ã€åˆå¿ƒè€…ã«ã‚‚ç†è§£ã—ã‚„ã™ã„æ‰‹æ³•ã§ã™ã€‚"
            assumptions = f"ã“ã®æ‰‹æ³•ã‚’ä½¿ç”¨ã™ã‚‹éš›ã®å‰ææ¡ä»¶ã¨ã—ã¦ã€{', '.join(method_info['assumptions'])}ãŒå¿…è¦ã§ã™ã€‚" if method_info['assumptions'] else "ç‰¹åˆ¥ãªå‰ææ¡ä»¶ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
            return f"{base_content} {use_cases} {complexity} {assumptions}"
        elif user_expertise == 'intermediate':
            # ä¸­ç´šè€…å‘ã‘ï¼šé©åº¦ãªè©³ç´°
            assumptions_text = f"ä¸»ãªä»®å®š: {', '.join(method_info['assumptions'])}" if method_info['assumptions'] else "ç‰¹åˆ¥ãªä»®å®šã¯ã‚ã‚Šã¾ã›ã‚“"
            use_cases = f"ç”¨é€”: {', '.join(method_info['use_cases'])}"
            return f"{method_info['name']}ã«ã¤ã„ã¦ - {assumptions_text}ã€‚{use_cases}ã€‚"
        else:  # advanced
            # ä¸Šç´šè€…å‘ã‘ï¼šç°¡æ½”ã§æŠ€è¡“çš„
            return f"{method_info['name']} - è¤‡é›‘åº¦: {method_info['computation_complexity']}"
    
    def _find_alternative_methods(self, current_method_id: str, 
                                current_method_info: Dict[str, Any]) -> List[str]:
        """ä»£æ›¿æ‰‹æ³•ã‚’è¦‹ã¤ã‘ã‚‹"""
        alternatives = []
        current_category = current_method_info['category']
        
        for method_id, method_info in self.method_database.items():
            if (method_id != current_method_id and 
                method_info['category'] == current_category):
                alternatives.append(method_info['name'])
        
        return alternatives[:3]  # æœ€å¤§3ã¤
    
    def validate_method_assumptions(self, method: str, data: pd.DataFrame,
                                  target_col: str = None, group_col: str = None) -> AssumptionValidationResult:
        """çµ±è¨ˆæ‰‹æ³•ã®ä»®å®šã‚’æ¤œè¨¼"""
        return self.assumption_validator.validate_method_assumptions(
            method, data, target_col, group_col
        )
    
    def get_method_with_validation(self, data_chars: DataCharacteristics,
                                 research_question: str = "",
                                 user_expertise: str = "intermediate",
                                 data: pd.DataFrame = None,
                                 target_col: str = None,
                                 group_col: str = None) -> List[Dict[str, Any]]:
        """ä»®å®šæ¤œè¨¼ä»˜ãã®æ‰‹æ³•æ¨å¥¨"""
        try:
            # åŸºæœ¬çš„ãªæ‰‹æ³•æ¨å¥¨ã‚’å–å¾—
            suggestions = self.suggest_methods(data_chars, research_question, user_expertise)
            
            enhanced_suggestions = []
            
            for suggestion in suggestions:
                enhanced_suggestion = {
                    'method_suggestion': suggestion,
                    'assumption_validation': None,
                    'overall_confidence': suggestion.confidence_score
                }
                
                # ãƒ‡ãƒ¼ã‚¿ãŒæä¾›ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ä»®å®šæ¤œè¨¼ã‚’å®Ÿè¡Œ
                if data is not None:
                    try:
                        validation_result = self.validate_method_assumptions(
                            suggestion.method_name, data, target_col, group_col
                        )
                        enhanced_suggestion['assumption_validation'] = validation_result
                        
                        # ä»®å®šæ¤œè¨¼çµæœã‚’è€ƒæ…®ã—ã¦å…¨ä½“çš„ãªä¿¡é ¼åº¦ã‚’èª¿æ•´
                        assumption_confidence = validation_result.confidence_in_results
                        enhanced_suggestion['overall_confidence'] = (
                            suggestion.confidence_score * 0.6 + assumption_confidence * 0.4
                        )
                        
                    except Exception as e:
                        self.logger.warning(f"ä»®å®šæ¤œè¨¼ã‚¨ãƒ©ãƒ¼ ({suggestion.method_name}): {e}")
                
                enhanced_suggestions.append(enhanced_suggestion)
            
            # å…¨ä½“çš„ãªä¿¡é ¼åº¦ã§ã‚½ãƒ¼ãƒˆ
            enhanced_suggestions.sort(key=lambda x: x['overall_confidence'], reverse=True)
            
            return enhanced_suggestions
            
        except Exception as e:
            self.logger.error(f"ä»®å®šæ¤œè¨¼ä»˜ãæ¨å¥¨ã‚¨ãƒ©ãƒ¼: {e}")
            return []

class AIOrchestrator:
    """AIçµ±è¨ˆè§£æã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ - ä¸­å¤®èª¿æ•´ã‚·ã‚¹ãƒ†ãƒ ï¼ˆé›»æºæ–­ä¿è­·æ©Ÿèƒ½å¼·åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.AIOrchestrator")
        self.query_processor = QueryProcessor()
        self.intent_classifier = IntentClassifier()
        self.context_manager = ContextManager()
        self.statistical_advisor = StatisticalMethodAdvisor()
        
        # æ—¢å­˜ã®AIStatisticalAnalyzerã‚’çµ±åˆ
        self.statistical_analyzer = AIStatisticalAnalyzer()
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ç®¡ç†
        self.providers = self.statistical_analyzer.providers
        self.knowledge_base = self.statistical_analyzer.knowledge_base
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
        self.session_id = str(uuid.uuid4())
        self.last_checkpoint = time.time()
        self.checkpoint_interval = 300  # 5åˆ†é–“éš”
        
        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç®¡ç†
        self.backup_manager = self._initialize_backup_manager()
        
        # é›»æºæ–­ä¿è­·æ©Ÿèƒ½ã®åˆæœŸåŒ–
        self._setup_power_protection()
        
        # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®è¨­å®š
        self._setup_signal_handlers()
    
    def _setup_power_protection(self):
        """é›»æºæ–­ä¿è­·æ©Ÿèƒ½ã®è¨­å®š"""
        self.checkpoint_dir = "checkpoints"
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # è‡ªå‹•ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã®é–‹å§‹
        self._start_auto_checkpoint()
    
    def _initialize_backup_manager(self):
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
        return {
            'max_backups': 10,
            'backup_interval': 300,  # 5åˆ†
            'backup_dir': 'pss_backups',
            'session_data': {},
            'last_backup': time.time()
        }
    
    def _setup_signal_handlers(self):
        """ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®è¨­å®šï¼ˆãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã§ã®ã¿å®Ÿè¡Œï¼‰"""
        import signal
        import threading
        
        # ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ã§ã®ã¿ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¨­å®š
        if threading.current_thread() is threading.main_thread():
            def signal_handler(signum, frame):
                self.logger.info(f"ã‚·ã‚°ãƒŠãƒ« {signum} ã‚’å—ä¿¡ã—ã¾ã—ãŸã€‚ç·Šæ€¥ä¿å­˜ã‚’å®Ÿè¡Œã—ã¾ã™...")
                self._emergency_save()
                sys.exit(0)
            
            # Windowså¯¾å¿œã®ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
            if hasattr(signal, 'SIGINT'):
                signal.signal(signal.SIGINT, signal_handler)
            if hasattr(signal, 'SIGTERM'):
                signal.signal(signal.SIGTERM, signal_handler)
        else:
            self.logger.warning("ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®è¨­å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰ä»¥å¤–ã§å®Ÿè¡Œï¼‰")
    
    def _start_auto_checkpoint(self):
        """è‡ªå‹•ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã®é–‹å§‹"""
        def auto_checkpoint():
            while True:
                try:
                    time.sleep(self.checkpoint_interval)
                    self._save_checkpoint()
                except Exception as e:
                    self.logger.error(f"è‡ªå‹•ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        
        import threading
        checkpoint_thread = threading.Thread(target=auto_checkpoint, daemon=True)
        checkpoint_thread.start()
    
    def _save_checkpoint(self):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        try:
            checkpoint_data = {
                'session_id': self.session_id,
                'timestamp': datetime.now().isoformat(),
                'providers': list(self.providers.keys()) if self.providers else [],
                'context_manager_state': self.context_manager.get_session_summary(
                    AnalysisContext("checkpoint", self.session_id, "checkpoint", [])
                ) if hasattr(self.context_manager, 'get_session_summary') else {},
                'statistical_analyzer_state': {
                    'available_models': self.statistical_analyzer.get_available_models() if hasattr(self.statistical_analyzer, 'get_available_models') else {}
                }
            }
            
            checkpoint_file = os.path.join(self.checkpoint_dir, f"ai_orchestrator_checkpoint_{self.session_id}.json")
            with open(checkpoint_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
            
            self.last_checkpoint = time.time()
            self.logger.info(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å®Œäº†: {checkpoint_file}")
            
            # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ç®¡ç†
            self._manage_backups()
            
        except Exception as e:
            self.logger.error(f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _emergency_save(self):
        """ç·Šæ€¥ä¿å­˜æ©Ÿèƒ½"""
        try:
            self.logger.info("ç·Šæ€¥ä¿å­˜ã‚’å®Ÿè¡Œä¸­...")
            
            # å³åº§ã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
            self._save_checkpoint()
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
            session_data = {
                'session_id': self.session_id,
                'timestamp': datetime.now().isoformat(),
                'emergency_save': True,
                'providers_status': {name: provider.is_available() if hasattr(provider, 'is_available') else False 
                                   for name, provider in self.providers.items()} if self.providers else {}
            }
            
            emergency_file = os.path.join(self.checkpoint_dir, f"emergency_save_{self.session_id}.json")
            with open(emergency_file, 'w', encoding='utf-8') as f:
                json.dump(session_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"ç·Šæ€¥ä¿å­˜å®Œäº†: {emergency_file}")
            
        except Exception as e:
            self.logger.error(f"ç·Šæ€¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _manage_backups(self):
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç®¡ç†"""
        try:
            backup_dir = self.backup_manager['backup_dir']
            os.makedirs(backup_dir, exist_ok=True)
            
            # æ—¢å­˜ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
            backup_files = glob.glob(os.path.join(backup_dir, "ai_orchestrator_backup_*.json"))
            backup_files.sort(key=os.path.getmtime, reverse=True)
            
            # æœ€å¤§ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ•°ã‚’è¶…ãˆãŸå ´åˆã€å¤ã„ã‚‚ã®ã‚’å‰Šé™¤
            if len(backup_files) >= self.backup_manager['max_backups']:
                for old_backup in backup_files[self.backup_manager['max_backups']:]:
                    try:
                        os.remove(old_backup)
                        self.logger.info(f"å¤ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å‰Šé™¤: {old_backup}")
                    except Exception as e:
                        self.logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
            
            # æ–°ã—ã„ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’ä½œæˆ
            backup_file = os.path.join(backup_dir, f"ai_orchestrator_backup_{self.session_id}_{int(time.time())}.json")
            checkpoint_data = {
                'session_id': self.session_id,
                'timestamp': datetime.now().isoformat(),
                'backup_type': 'scheduled',
                'providers': list(self.providers.keys()) if self.providers else [],
                'context_manager_state': self.context_manager.get_session_summary(
                    AnalysisContext("backup", self.session_id, "backup", [])
                ) if hasattr(self.context_manager, 'get_session_summary') else {}
            }
            
            with open(backup_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
            
            self.backup_manager['last_backup'] = time.time()
            self.logger.info(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆå®Œäº†: {backup_file}")
            
        except Exception as e:
            self.logger.error(f"ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç®¡ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def analyze_query(self, query: str, context: AnalysisContext, data: Optional[pd.DataFrame] = None) -> AIResponse:
        """analyze_queryãƒ¡ã‚½ãƒƒãƒ‰ - process_user_queryã¸ã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰"""
        try:
            self.logger.info(f"analyze_queryå‘¼ã³å‡ºã—: {query[:50]}...")
            return await self.process_user_query(query, context, data)
        except Exception as e:
            self.logger.error(f"analyze_queryã‚¨ãƒ©ãƒ¼: {e}")
            # ç·Šæ€¥ä¿å­˜ã‚’å®Ÿè¡Œ
            self._emergency_save()
            raise
    
    async def process_user_query(self, query: str, context: AnalysisContext, data: Optional[pd.DataFrame] = None) -> AIResponse:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¯ã‚¨ãƒªã®çµ±åˆå‡¦ç†ï¼ˆé›»æºæ–­ä¿è­·æ©Ÿèƒ½å¼·åŒ–ç‰ˆï¼‰"""
        start_time = time.time()
        
        try:
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹æ™‚ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
            if time.time() - self.last_checkpoint > self.checkpoint_interval:
                self._save_checkpoint()
            
            # 1. ã‚¯ã‚¨ãƒªå‡¦ç†
            processed_query = self.query_processor.process_query(query, context)
            
            # 2. æ„å›³åˆ†é¡
            intent = self.intent_classifier.classify(query, context)
            
            # 3. ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰
            try:
                provider = self._select_optimal_provider(intent, context)
            except Exception as e:
                self.logger.warning(f"ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠã‚¨ãƒ©ãƒ¼: {e}ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨")
                provider = self._get_fallback_provider()
            
            # 4. AIå¿œç­”ç”Ÿæˆ
            if data is not None:
                # ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯çµ±è¨ˆè§£æã‚’å®Ÿè¡Œ
                result = await self.statistical_analyzer.analyze_with_custom_llm(
                    query, data, provider=provider, enable_rag=True
                )
            else:
                # ãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯æ•™è‚²çš„ãªå¿œç­”
                result = await self._generate_educational_response(query, intent, provider)
            
            # 5. å¿œç­”æ§‹ç¯‰
            processing_time = time.time() - start_time
            
            response = AIResponse(
                content=result.get('content', ''),
                confidence=result.get('confidence', 0.7),
                provider_used=provider,
                tokens_consumed=result.get('tokens', 0),
                processing_time=processing_time,
                intent_detected=intent,
                educational_content=result.get('educational_content'),
                follow_up_suggestions=self._generate_follow_up_suggestions(intent, result)
            )
            
            # 6. ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ›´æ–°
            self.context_manager.update_context(context, {
                'type': intent.value,
                'method': result.get('method', 'ai_response'),
                'success': result.get('success', True)
            })
            
            # 7. å‡¦ç†å®Œäº†æ™‚ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
            if processing_time > 30:  # 30ç§’ä»¥ä¸Šã®å‡¦ç†ã®å ´åˆã¯å³åº§ã«ä¿å­˜
                self._save_checkpoint()
            
            return response
            
        except Exception as e:
            self.logger.error(f"ã‚¯ã‚¨ãƒªå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ã‚¨ãƒ©ãƒ¼æ™‚ã®ç·Šæ€¥ä¿å­˜
            self._emergency_save()
            
            return AIResponse(
                content=f"ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
                confidence=0.0,
                provider_used="error",
                tokens_consumed=0,
                processing_time=time.time() - start_time,
                intent_detected=IntentType.EXPLORATORY
            )
    
    def _get_fallback_provider(self) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å–å¾—"""
        # åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å„ªå…ˆé †ä½ã§é¸æŠ
        fallback_order = ['ollama', 'lmstudio', 'google', 'openai']
        
        for provider_name in fallback_order:
            if provider_name in self.providers:
                provider = self.providers[provider_name]
                if hasattr(provider, 'is_available') and provider.is_available():
                    return provider_name
        
        # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦æœ€åˆã®åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨
        for name, provider in self.providers.items():
            if hasattr(provider, 'is_available') and provider.is_available():
                return name
        
        return "error"  # åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒãªã„å ´åˆ
    
    def _select_optimal_provider(self, intent: IntentType, context: AnalysisContext) -> str:
        """æœ€é©ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’é¸æŠï¼ˆGUIæŒ‡å®šå¯¾å¿œç‰ˆï¼‰"""
        try:
            # GUIã‹ã‚‰æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å„ªå…ˆ
            preferred_provider = context.privacy_settings.get('preferred_provider')
            if preferred_provider and preferred_provider in self.providers:
                provider = self.providers[preferred_provider]
                if hasattr(provider, 'is_available') and provider.is_available():
                    self.logger.info(f"GUIæŒ‡å®šãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨: {preferred_provider}")
                    return preferred_provider
                else:
                    self.logger.warning(f"GUIæŒ‡å®šãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ '{preferred_provider}' ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            
            # ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼è¨­å®šã‚’è€ƒæ…®
            if context.privacy_settings.get('use_local_llm', False):
                if 'ollama' in self.providers:
                    provider = self.providers['ollama']
                    if hasattr(provider, 'is_available') and provider.is_available():
                        return 'ollama'
                elif 'lmstudio' in self.providers:
                    provider = self.providers['lmstudio']
                    if hasattr(provider, 'is_available') and provider.is_available():
                        return 'lmstudio'
            
            # æ„å›³ã«åŸºã¥ãé¸æŠ
            if intent == IntentType.EDUCATIONAL:
                # æ•™è‚²çš„ãªå†…å®¹ã«ã¯GeminiãŒé©ã—ã¦ã„ã‚‹
                if 'google' in self.providers:
                    provider = self.providers['google']
                    if hasattr(provider, 'is_available') and provider.is_available():
                        return 'google'
            elif intent == IntentType.PREDICTIVE:
                # äºˆæ¸¬ã‚¿ã‚¹ã‚¯ã«ã¯GPT-4ãŒé©ã—ã¦ã„ã‚‹
                if 'openai' in self.providers:
                    provider = self.providers['openai']
                    if hasattr(provider, 'is_available') and provider.is_available():
                        return 'openai'
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠ
            fallback_provider = self._get_fallback_provider()
            self.logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨: {fallback_provider}")
            return fallback_provider
            
        except Exception as e:
            self.logger.error(f"ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠã‚¨ãƒ©ãƒ¼: {e}")
            return self._get_fallback_provider()
    
    async def _generate_educational_response(self, query: str, intent: IntentType, provider: str) -> Dict[str, Any]:
        """æ•™è‚²çš„ãªå¿œç­”ã‚’ç”Ÿæˆ"""
        educational_prompt = f"""
çµ±è¨ˆå­¦ã®è³ªå•ã«å¯¾ã—ã¦æ•™è‚²çš„ã§åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

è³ªå•: {query}

ä»¥ä¸‹ã®ç‚¹ã‚’å«ã‚ã¦å›ç­”ã—ã¦ãã ã•ã„ï¼š
1. æ¦‚å¿µã®èª¬æ˜
2. ä½¿ç”¨å ´é¢
3. å‰ææ¡ä»¶ã‚„æ³¨æ„ç‚¹
4. ç°¡å˜ãªä¾‹
5. é–¢é€£ã™ã‚‹çµ±è¨ˆæ‰‹æ³•

æ—¥æœ¬èªã§åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚
"""
        
        if provider in self.providers:
            result = await self.providers[provider].generate_response(
                educational_prompt, 
                model=self._get_default_model(provider)
            )
            
            if result.get('success'):
                return {
                    'content': result['content'],
                    'success': True,
                    'educational_content': result['content'],
                    'method': 'educational_response',
                    'tokens': result.get('tokens', 0)
                }
        
        return {
            'content': 'ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ç¾åœ¨ã€ã“ã®è³ªå•ã«ãŠç­”ãˆã§ãã¾ã›ã‚“ã€‚',
            'success': False,
            'method': 'fallback'
        }
    
    def _get_default_model(self, provider: str) -> str:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"""
        model_map = {
            'openai': 'gpt-4o',
            'google': 'gemini-1.5-pro-latest',
            'anthropic': 'claude-3-5-sonnet-20240620',
            'ollama': 'llama3.1',
            'lmstudio': 'local-model',
            'koboldcpp': 'local-model'
        }
        return model_map.get(provider, 'gpt-4o')
    
    def _generate_follow_up_suggestions(self, intent: IntentType, result: Dict[str, Any]) -> List[str]:
        """ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ææ¡ˆã‚’ç”Ÿæˆ"""
        suggestions = []
        
        if intent == IntentType.DESCRIPTIVE:
            suggestions = [
                "ã“ã®çµæœã«åŸºã¥ã„ã¦ä»®èª¬æ¤œå®šã‚’è¡Œã„ã¾ã™ã‹ï¼Ÿ",
                "ãƒ‡ãƒ¼ã‚¿ã®å¯è¦–åŒ–ã‚’ä½œæˆã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ",
                "å¤–ã‚Œå€¤ã®è©³ç´°åˆ†æã‚’è¡Œã„ã¾ã™ã‹ï¼Ÿ"
            ]
        elif intent == IntentType.INFERENTIAL:
            suggestions = [
                "åŠ¹æœé‡ã®è¨ˆç®—ã‚’è¡Œã„ã¾ã™ã‹ï¼Ÿ",
                "æ¤œå®šåŠ›åˆ†æã‚’å®Ÿæ–½ã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ",
                "çµæœã®è§£é‡ˆã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¾ã™ã‹ï¼Ÿ"
            ]
        elif intent == IntentType.PREDICTIVE:
            suggestions = [
                "ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡ã‚’è©³ã—ãè¦‹ã¾ã™ã‹ï¼Ÿ",
                "ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’åˆ†æã—ã¾ã—ã‚‡ã†ã‹ï¼Ÿ",
                "ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–ã‚’è¡Œã„ã¾ã™ã‹ï¼Ÿ"
            ]
        
        return suggestions[:3]  # æœ€å¤§3ã¤ã¾ã§

class AIStatisticalAnalyzer:
    """AIçµ±è¨ˆè§£æã‚¨ãƒ³ã‚¸ãƒ³ - 2025å¹´æœ€æ–°ç‰ˆï¼ˆãƒãƒ«ãƒLLMã€è‡ªå·±ä¿®æ­£ã€RAGã€ãƒ­ãƒ¼ã‚«ãƒ«LLMå¯¾å¿œã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼å¯¾å¿œï¼‰"""
    def __init__(self):
        self.analysis_history: List[Dict[str, Any]] = []
        self.logger = logging.getLogger(__name__)
        self.platform_capabilities = self._detect_platform_capabilities()
        self.knowledge_base = KnowledgeBase()
        self.providers = self._initialize_providers()
        self.privacy_selector = PrivacyAwareProviderSelector(self.providers)
    
    def _detect_platform_capabilities(self) -> Dict[str, Any]:
        """ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ©Ÿèƒ½ã‚’æ¤œå‡º"""
        capabilities = {
            'os': platform.system(),
            'python_version': platform.python_version(),
            'gpu_available': False,
            'cuda_available': False,
            'memory_gb': 0
        }
        
        try:
            import psutil
            capabilities['memory_gb'] = psutil.virtual_memory().total / (1024**3)
        except ImportError:
            pass
            
        try:
            import torch
            capabilities['gpu_available'] = torch.cuda.is_available()
            capabilities['cuda_available'] = torch.cuda.is_available()
            if torch.cuda.is_available():
                capabilities['gpu_name'] = torch.cuda.get_device_name(0)
        except ImportError:
            pass
            
        return capabilities
    
    def get_available_models(self) -> Dict[str, List[str]]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—"""
        models = {}
        for provider_name, provider in self.providers.items():
            try:
                if hasattr(provider, 'get_available_models'):
                    models[provider_name] = provider.get_available_models()
                else:
                    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆ
                    default_models = {
                        'openai': ['gpt-4o', 'o3'],
                        'google': ['gemini-2.5-flash-exp', 'gemini-2.5-flash-lite-exp'],
                        'anthropic': ['claude-3-5-sonnet-20240620', 'claude-3-5-haiku-20240307'],
                        'ollama': ['llama3.1', 'llama3.2', 'llama3.3'],
                        'lmstudio': ['local-model'],
                        'koboldcpp': ['local-model']
                    }
                    models[provider_name] = default_models.get(provider_name, ['default'])
            except Exception as e:
                self.logger.warning(f"ãƒ¢ãƒ‡ãƒ«å–å¾—å¤±æ•— {provider_name}: {e}")
                models[provider_name] = ['error']
        
        return models
    
    async def analyze_with_privacy_aware_llm(self, query: str, data: pd.DataFrame, 
                                           task_type: str = "general",
                                           enable_rag: bool = False, 
                                           enable_correction: bool = True) -> Dict[str, Any]:
        """ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼å¯¾å¿œLLMåˆ†æ"""
        try:
            start_time = time.time()
            
            # ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼å¯¾å¿œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠ
            selected_provider_name, selected_provider = self.privacy_selector.select_optimal_provider(
                data, task_type
            )
            
            self.logger.info(f"é¸æŠã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {selected_provider_name}")
            
            # ãƒ‡ãƒ¼ã‚¿ã®æ©Ÿå¯†æ€§ãƒ¬ãƒ™ãƒ«ã‚’è¨˜éŒ²
            sensitivity_level = self.privacy_selector.privacy_manager.classify_data_sensitivity(data)
            
            # åˆ†æå®Ÿè¡Œ
            result = await self._execute_analysis_with_provider(
                query, data, selected_provider, selected_provider_name,
                enable_rag, enable_correction
            )
            
            # ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼æƒ…å ±ã‚’çµæœã«è¿½åŠ 
            result.update({
                "privacy_info": {
                    "selected_provider": selected_provider_name,
                    "data_sensitivity_level": sensitivity_level,
                    "privacy_measures_applied": self._get_privacy_measures_applied(sensitivity_level),
                    "data_anonymized": sensitivity_level in ['high', 'medium']
                }
            })
            
            # åˆ†æå±¥æ­´ã«è¨˜éŒ²
            self.analysis_history.append({
                "timestamp": datetime.now().isoformat(),
                "query": query,
                "provider": selected_provider_name,
                "sensitivity_level": sensitivity_level,
                "processing_time": time.time() - start_time
            })
            
            return result
            
        except Exception as e:
            self.logger.error(f"ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼å¯¾å¿œåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "success": False,
                "error": str(e),
                "privacy_info": {
                    "selected_provider": "unknown",
                    "data_sensitivity_level": "unknown",
                    "privacy_measures_applied": [],
                    "data_anonymized": False
                }
            }
    
    async def analyze_with_custom_llm(self, query: str, data: pd.DataFrame, 
                                    provider: str = "google", model: str = "gemini-pro",
                                    enable_rag: bool = False, enable_correction: bool = True) -> Dict[str, Any]:
        """ã‚«ã‚¹ã‚¿ãƒ LLMã§çµ±è¨ˆè§£æã‚’å®Ÿè¡Œ"""
        try:
            if provider not in self.providers:
                return {"success": False, "error": f"ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ {provider} ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"}
            
            # RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å–å¾—
            rag_context = ""
            if enable_rag and self.knowledge_base:
                relevant_docs = self.knowledge_base.search(query)
                if relevant_docs:
                    rag_context = "\né–¢é€£æƒ…å ±:\n" + "\n".join([doc['content'][:200] for doc in relevant_docs])
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
            prompt = f"""
çµ±è¨ˆãƒ‡ãƒ¼ã‚¿åˆ†æã‚¿ã‚¹ã‚¯:
{query}

ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:
- è¡Œæ•°: {len(data)}
- åˆ—æ•°: {len(data.columns)}
- åˆ—å: {list(data.columns)}
- ãƒ‡ãƒ¼ã‚¿å‹: {data.dtypes.to_dict()}

{rag_context}

Pythonã‚³ãƒ¼ãƒ‰ã§åˆ†æã‚’å®Ÿè¡Œã—ã€çµæœã‚’è§£é‡ˆã—ã¦ãã ã•ã„ã€‚
"""
            
            # LLMå®Ÿè¡Œ
            provider_instance = self.providers[provider]
            result = await provider_instance.generate_response(prompt, model)
            
            if result["success"] and enable_correction:
                # ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œã¨è‡ªå·±ä¿®æ­£
                code_result = self._execute_generated_code(result["content"], data)
                if not code_result["success"]:
                    # ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã‚’è©¦è¡Œ
                    correction_prompt = f"""
å‰å›ã®ã‚³ãƒ¼ãƒ‰ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:
{code_result['error']}

å…ƒã®ã‚³ãƒ¼ãƒ‰:
{result['content']}

ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã—ãŸPythonã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
"""
                    correction_result = await provider_instance.generate_response(correction_prompt, model)
                    if correction_result["success"]:
                        result["content"] = correction_result["content"]
                        result["corrected"] = True
            
            # å±¥æ­´ã«ä¿å­˜
            self.analysis_history.append({
                "timestamp": datetime.now().isoformat(),
                "query": query,
                "provider": provider,
                "model": model,
                "result": result
            })
            
            return result
            
        except Exception as e:
            self.logger.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}
    
    def _execute_generated_code(self, code: str, data: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚’å®‰å…¨ã«å®Ÿè¡Œ"""
        try:
            # ã‚³ãƒ¼ãƒ‰ã‹ã‚‰Pythonéƒ¨åˆ†ã‚’æŠ½å‡º
            import re
            code_blocks = re.findall(r'```python\n(.*?)\n```', code, re.DOTALL)
            if not code_blocks:
                code_blocks = re.findall(r'```\n(.*?)\n```', code, re.DOTALL)
            
            if not code_blocks:
                return {"success": False, "error": "å®Ÿè¡Œå¯èƒ½ãªã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
            
            # æœ€åˆã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’å®Ÿè¡Œ
            exec_code = code_blocks[0]
            
            # å®‰å…¨ãªå®Ÿè¡Œç’°å¢ƒã‚’æº–å‚™
            safe_globals = {
                'pd': pd,
                'np': np,
                'data': data,
                'plt': None,  # matplotlibãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã®ã¿
                'print': print
            }
            
            try:
                import matplotlib.pyplot as plt
                safe_globals['plt'] = plt
            except ImportError:
                pass
            
            # ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ
            output_buffer = io.StringIO()
            with redirect_stdout(output_buffer):
                exec(exec_code, safe_globals)
            
            output = output_buffer.getvalue()
            
            return {
                "success": True,
                "output": output,
                "code": exec_code
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "traceback": traceback.format_exc()
            }
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã‚’å–å¾—"""
        if not self.analysis_history:
            return {"total_analyses": 0, "avg_tokens": 0, "success_rate": 0}
        
        total = len(self.analysis_history)
        successful = sum(1 for h in self.analysis_history if h["result"].get("success", False))
        
        total_tokens = sum(h["result"].get("tokens", 0) for h in self.analysis_history if h["result"].get("success", False))
        avg_tokens = total_tokens / successful if successful > 0 else 0
        
        return {
            "total_analyses": total,
            "successful_analyses": successful,
            "success_rate": successful / total if total > 0 else 0,
            "avg_tokens": avg_tokens,
            "providers_used": list(set(h["provider"] for h in self.analysis_history))
        }
    
    async def _execute_analysis_with_provider(self, query: str, data: pd.DataFrame,
                                            provider: LLMProvider, provider_name: str,
                                            enable_rag: bool, enable_correction: bool) -> Dict[str, Any]:
        """æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§åˆ†æã‚’å®Ÿè¡Œ"""
        try:
            # RAGæ©Ÿèƒ½ãŒæœ‰åŠ¹ãªå ´åˆã€çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰é–¢é€£æƒ…å ±ã‚’å–å¾—
            context_info = ""
            if enable_rag and self.knowledge_base:
                relevant_docs = self.knowledge_base.search(query, top_k=2)
                if relevant_docs:
                    context_info = "\n\né–¢é€£æƒ…å ±:\n" + "\n".join([doc['content'] for doc in relevant_docs])
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
            prompt = self._build_analysis_prompt(query, data, context_info)
            
            # LLMå¿œç­”ç”Ÿæˆ
            response = await provider.generate_response(prompt, model="default")
            
            if not response.get("success", False):
                return {
                    "success": False,
                    "error": response.get("error", "Unknown error"),
                    "provider": provider_name
                }
            
            # å¿œç­”å‡¦ç†
            result = self._process_llm_response(response, data, enable_correction)
            result["provider"] = provider_name
            
            return result
            
        except Exception as e:
            self.logger.error(f"ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "success": False,
                "error": str(e),
                "provider": provider_name
            }
    
    def _build_analysis_prompt(self, query: str, data: pd.DataFrame, context_info: str = "") -> str:
        """åˆ†æç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
        data_info = f"""
ãƒ‡ãƒ¼ã‚¿æƒ…å ±:
- è¡Œæ•°: {len(data)}
- åˆ—æ•°: {len(data.columns)}
- åˆ—å: {list(data.columns)}
- ãƒ‡ãƒ¼ã‚¿å‹: {dict(data.dtypes)}
- æ¬ æå€¤: {data.isnull().sum().to_dict()}
"""
        
        prompt = f"""
çµ±è¨ˆåˆ†æã‚¿ã‚¹ã‚¯: {query}

{data_info}

{context_info}

ä»¥ä¸‹ã®å½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:
1. æ¨å¥¨ã•ã‚Œã‚‹çµ±è¨ˆæ‰‹æ³•
2. å®Ÿè£…æ‰‹é †
3. çµæœã®è§£é‡ˆæ–¹æ³•
4. æ³¨æ„äº‹é …

å›ç­”:
"""
        return prompt
    
    def _process_llm_response(self, response: Dict[str, Any], data: pd.DataFrame, 
                             enable_correction: bool) -> Dict[str, Any]:
        """LLMå¿œç­”ã‚’å‡¦ç†"""
        try:
            content = response.get("content", "")
            
            # å¿œç­”ã®æ§‹é€ åŒ–
            result = {
                "success": True,
                "analysis": content,
                "tokens_used": response.get("tokens", 0),
                "processing_time": response.get("processing_time", 0)
            }
            
            # è‡ªå·±ä¿®æ­£æ©Ÿèƒ½ãŒæœ‰åŠ¹ãªå ´åˆ
            if enable_correction:
                corrected_content = self._apply_self_correction(content, data)
                result["corrected_analysis"] = corrected_content
                result["corrections_applied"] = content != corrected_content
            
            return result
            
        except Exception as e:
            self.logger.error(f"å¿œç­”å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "success": False,
                "error": str(e),
                "analysis": response.get("content", "")
            }
    
    def _apply_self_correction(self, content: str, data: pd.DataFrame) -> str:
        """è‡ªå·±ä¿®æ­£æ©Ÿèƒ½"""
        try:
            # åŸºæœ¬çš„ãªçµ±è¨ˆç”¨èªã®ä¿®æ­£
            corrections = {
                "å¹³å‡å€¤": "å¹³å‡",
                "æ¨™æº–åå·®": "æ¨™æº–åå·®",
                "tæ¤œå®š": "tæ¤œå®š",
                "åˆ†æ•£åˆ†æ": "åˆ†æ•£åˆ†æ",
                "å›å¸°åˆ†æ": "å›å¸°åˆ†æ"
            }
            
            corrected_content = content
            for wrong, correct in corrections.items():
                corrected_content = corrected_content.replace(wrong, correct)
            
            return corrected_content
            
        except Exception as e:
            self.logger.warning(f"è‡ªå·±ä¿®æ­£ã‚¨ãƒ©ãƒ¼: {e}")
            return content
    
    def _get_privacy_measures_applied(self, sensitivity_level: str) -> List[str]:
        """é©ç”¨ã•ã‚ŒãŸãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼å¯¾ç­–ã‚’å–å¾—"""
        measures = []
        
        if sensitivity_level == 'high':
            measures.extend([
                "ãƒ­ãƒ¼ã‚«ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å„ªå…ˆä½¿ç”¨",
                "ãƒ‡ãƒ¼ã‚¿åŒ¿ååŒ–",
                "æ©Ÿå¯†æƒ…å ±æ¤œå‡º",
                "ã‚¢ã‚¯ã‚»ã‚¹åˆ¶å¾¡"
            ])
        elif sensitivity_level == 'medium':
            measures.extend([
                "ãƒ­ãƒ¼ã‚«ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼è©¦è¡Œ",
                "ãƒ‡ãƒ¼ã‚¿åŒ¿ååŒ–",
                "æ©Ÿå¯†æƒ…å ±æ¤œå‡º"
            ])
        else:
            measures.append("æ¨™æº–ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·")
        
        return measures
    
    def get_privacy_stats(self) -> Dict[str, Any]:
        """ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        if not self.analysis_history:
            return {"total_analyses": 0, "privacy_levels": {}}
        
        privacy_levels = {}
        for analysis in self.analysis_history:
            level = analysis.get("sensitivity_level", "unknown")
            privacy_levels[level] = privacy_levels.get(level, 0) + 1
        
        return {
            "total_analyses": len(self.analysis_history),
            "privacy_levels": privacy_levels,
            "providers_used": list(set(h["provider"] for h in self.analysis_history))
        }
    
    def _initialize_providers(self) -> Dict[str, LLMProvider]:
        """åˆ©ç”¨å¯èƒ½ãªLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–"""
        providers = {}
        if ai_config.is_api_configured("openai"):
            providers["openai"] = OpenAIProvider(ai_config.openai_api_key)
        if ai_config.is_api_configured("anthropic"):
            providers["anthropic"] = AnthropicProvider(ai_config.anthropic_api_key)
        if ai_config.is_api_configured("google"):
            providers["google"] = GoogleProvider(ai_config.google_api_key)
        if ai_config.is_api_configured("together"):
            providers["together"] = TogetherProvider(ai_config.together_api_key)
        if ai_config.is_api_configured("ollama"):
            providers["ollama"] = OllamaProvider(ai_config.ollama_base_url)
        if ai_config.is_api_configured("lmstudio"):
            providers["lmstudio"] = LMStudioProvider(ai_config.lmstudio_base_url)
        if ai_config.is_api_configured("koboldcpp"):
            providers["koboldcpp"] = KoboldCppProvider(ai_config.koboldcpp_base_url)
        self.logger.info(f"åˆæœŸåŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {list(providers.keys())}")
        return providers

    # ... (All other methods of AIStatisticalAnalyzer remain the same) ...

# ã‚°ãƒ­ãƒ¼ãƒãƒ«åˆ†æé–¢æ•°
async def analyze_with_ai(query: str, data: pd.DataFrame, provider: str = "google", 
                         model: str = "gemini-pro", **kwargs) -> Dict[str, Any]:
    """AIçµ±è¨ˆè§£æã®ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    analyzer = AIStatisticalAnalyzer()
    return await analyzer.analyze_with_custom_llm(query, data, provider, model, **kwargs)

def get_performance_stats() -> Dict[str, Any]:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆå–å¾—ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«é–¢æ•°"""
    analyzer = AIStatisticalAnalyzer()
    return analyzer.get_performance_stats()

# åŒæœŸç‰ˆã®åˆ†æé–¢æ•°ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
def analyze_with_ai_sync(query: str, data: pd.DataFrame, provider: str = "google", 
                        model: str = "gemini-pro", **kwargs) -> Dict[str, Any]:
    """AIçµ±è¨ˆè§£æã®åŒæœŸç‰ˆé–¢æ•°"""
    import asyncio
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    return loop.run_until_complete(analyze_with_ai(query, data, provider, model, **kwargs))

# Data sensitivity classification and privacy management
class DataPrivacyManager:
    """ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.sensitivity_patterns = {
            'high': [
                r'\b(ssn|social\s+security|credit\s+card|password|secret|private)\b',
                r'\b(patient|medical|health|diagnosis|treatment)\b',
                r'\b(salary|income|bank|account|financial)\b',
                r'\b(address|phone|email|personal)\b',
                r'\b(confidential|proprietary|trade\s+secret)\b'
            ],
            'medium': [
                r'\b(company|business|corporate|employee)\b',
                r'\b(survey|research|study|participant)\b',
                r'\b(performance|metrics|kpi|analytics)\b'
            ],
            'low': [
                r'\b(public|open|published|available)\b',
                r'\b(sample|test|demo|example)\b',
                r'\b(general|common|standard)\b'
            ]
        }
        self.logger = logging.getLogger(f"{__name__}.DataPrivacyManager")
    
    def classify_data_sensitivity(self, data: Union[str, pd.DataFrame, Dict[str, Any]]) -> str:
        """ãƒ‡ãƒ¼ã‚¿ã®æ©Ÿå¯†æ€§ãƒ¬ãƒ™ãƒ«ã‚’åˆ†é¡ã™ã‚‹"""
        try:
            if isinstance(data, pd.DataFrame):
                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å ´åˆã€ã‚«ãƒ©ãƒ åã‚‚å«ã‚ã¦ãƒã‚§ãƒƒã‚¯
                text_content = ' '.join(data.astype(str).values.flatten()) + ' ' + ' '.join(data.columns.astype(str))
            elif isinstance(data, dict):
                text_content = json.dumps(data, default=str)
            elif isinstance(data, str):
                text_content = data
            else:
                text_content = str(data)
            
            # æ©Ÿå¯†æ€§ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒã‚§ãƒƒã‚¯
            sensitivity_score = 0
            for level, patterns in self.sensitivity_patterns.items():
                for pattern in patterns:
                    if re.search(pattern, text_content, re.IGNORECASE):
                        if level == 'high':
                            sensitivity_score += 3
                        elif level == 'medium':
                            sensitivity_score += 2
                        else:
                            sensitivity_score += 1
            
            # ã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦æ©Ÿå¯†æ€§ãƒ¬ãƒ™ãƒ«ã‚’æ±ºå®š
            if sensitivity_score >= 3:
                return 'high'
            elif sensitivity_score >= 1:
                return 'medium'
            else:
                return 'low'
                
        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿æ©Ÿå¯†æ€§åˆ†é¡ã‚¨ãƒ©ãƒ¼: {e}")
            return 'medium'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ä¸­ç¨‹åº¦
    
    def should_use_local_provider(self, sensitivity_level: str) -> bool:
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ä½¿ç”¨ãŒå¿…è¦ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        return sensitivity_level in ['high', 'medium']
    
    def anonymize_data(self, data: Union[str, pd.DataFrame], sensitivity_level: str) -> Union[str, pd.DataFrame]:
        """ãƒ‡ãƒ¼ã‚¿ã®åŒ¿ååŒ–å‡¦ç†"""
        try:
            if sensitivity_level == 'low':
                return data
            
            if isinstance(data, pd.DataFrame):
                # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®åŒ¿ååŒ–
                anonymized_data = data.copy()
                
                # å€‹äººæƒ…å ±ã‚«ãƒ©ãƒ ã®ç‰¹å®šã¨åŒ¿ååŒ–
                personal_info_patterns = [
                    r'name', r'email', r'phone', r'address', r'id',
                    r'ssn', r'credit', r'password', r'secret'
                ]
                
                columns_to_rename = {}
                for col in anonymized_data.columns:
                    col_lower = col.lower()
                    for pattern in personal_info_patterns:
                        if re.search(pattern, col_lower):
                            # ã‚«ãƒ©ãƒ åã‚’å¤‰æ›´ã—ã¦åŒ¿ååŒ–
                            new_col_name = f"anonymized_{col}"
                            columns_to_rename[col] = new_col_name
                            # å€¤ã‚‚åŒ¿ååŒ–
                            anonymized_data[col] = f"anonymized_{col}_{hash(str(anonymized_data[col].iloc[0])) % 1000}"
                            break
                
                # ã‚«ãƒ©ãƒ åã‚’å¤‰æ›´
                if columns_to_rename:
                    anonymized_data = anonymized_data.rename(columns=columns_to_rename)
                
                return anonymized_data
            
            elif isinstance(data, str):
                # æ–‡å­—åˆ—ã®åŒ¿ååŒ–
                anonymized_text = data
                
                # å€‹äººæƒ…å ±ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç½®æ›
                patterns = {
                    r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b': '[EMAIL]',
                    r'\b\d{3}-\d{2}-\d{4}\b': '[SSN]',
                    r'\b\d{4}-\d{4}-\d{4}-\d{4}\b': '[CREDIT_CARD]',
                    r'\b\d{3}-\d{3}-\d{4}\b': '[PHONE]'
                }
                
                for pattern, replacement in patterns.items():
                    anonymized_text = re.sub(pattern, replacement, anonymized_text)
                
                return anonymized_text
            
            return data
            
        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿åŒ¿ååŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return data

class PrivacyAwareProviderSelector:
    """ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼å¯¾å¿œãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, providers: Dict[str, LLMProvider]):
        self.providers = providers
        self.privacy_manager = DataPrivacyManager()
        self.logger = logging.getLogger(f"{__name__}.PrivacyAwareProviderSelector")
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒ¬ãƒ™ãƒ«åˆ†é¡
        self.privacy_levels = {
            'local': ['gguf', 'ollama', 'lmstudio', 'koboldcpp'],  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
            'cloud_private': ['anthropic', 'openai'],  # ã‚¯ãƒ©ã‚¦ãƒ‰ï¼ˆãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼é‡è¦–ï¼‰
            'cloud_public': ['google', 'together']  # ã‚¯ãƒ©ã‚¦ãƒ‰ï¼ˆä¸€èˆ¬ï¼‰
        }
    
    def select_optimal_provider(self, data: Union[str, pd.DataFrame, Dict[str, Any]], 
                              task_type: str = "general") -> Tuple[str, LLMProvider]:
        """ãƒ‡ãƒ¼ã‚¿ã®æ©Ÿå¯†æ€§ã«åŸºã¥ã„ã¦æœ€é©ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’é¸æŠ"""
        try:
            # ãƒ‡ãƒ¼ã‚¿ã®æ©Ÿå¯†æ€§ã‚’åˆ†é¡
            sensitivity_level = self.privacy_manager.classify_data_sensitivity(data)
            self.logger.info(f"ãƒ‡ãƒ¼ã‚¿æ©Ÿå¯†æ€§ãƒ¬ãƒ™ãƒ«: {sensitivity_level}")
            
            # æ©Ÿå¯†æ€§ã«åŸºã¥ããƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠ
            if sensitivity_level == 'high':
                # é«˜æ©Ÿå¯†æ€§ãƒ‡ãƒ¼ã‚¿: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å„ªå…ˆ
                selected_provider = self._select_local_provider()
                if selected_provider:
                    return selected_provider.provider_name, selected_provider
                else:
                    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒåˆ©ç”¨ã§ããªã„å ´åˆã€ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§åŒ¿ååŒ–
                    return self._select_cloud_provider_with_anonymization(data, 'cloud_private')
            
            elif sensitivity_level == 'medium':
                # ä¸­æ©Ÿå¯†æ€§ãƒ‡ãƒ¼ã‚¿: ãƒ­ãƒ¼ã‚«ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è©¦è¡Œã€å¤±æ•—æ™‚ã¯ã‚¯ãƒ©ã‚¦ãƒ‰
                selected_provider = self._select_local_provider()
                if selected_provider:
                    return selected_provider.provider_name, selected_provider
                else:
                    return self._select_cloud_provider_with_anonymization(data, 'cloud_private')
            
            else:
                # ä½æ©Ÿå¯†æ€§ãƒ‡ãƒ¼ã‚¿: ã‚³ã‚¹ãƒˆã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è€ƒæ…®
                return self._select_best_performance_provider()
                
        except Exception as e:
            self.logger.error(f"ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è¿”ã™
            return self._get_default_provider()
    
    def _select_local_provider(self) -> Optional[LLMProvider]:
        """ãƒ­ãƒ¼ã‚«ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’é¸æŠ"""
        for provider_name in self.privacy_levels['local']:
            if provider_name in self.providers:
                provider = self.providers[provider_name]
                # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å¥åº·çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯
                if self._check_provider_health(provider):
                    return provider
        return None
    
    def _select_cloud_provider_with_anonymization(self, data: Union[str, pd.DataFrame, Dict[str, Any]], 
                                                privacy_level: str) -> Tuple[str, LLMProvider]:
        """ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’é¸æŠã—ã€å¿…è¦ã«å¿œã˜ã¦åŒ¿ååŒ–"""
        # ãƒ‡ãƒ¼ã‚¿ã‚’åŒ¿ååŒ–
        anonymized_data = self.privacy_manager.anonymize_data(data, 'high')
        
        # æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒ¬ãƒ™ãƒ«ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‹ã‚‰é¸æŠ
        for provider_name in self.privacy_levels.get(privacy_level, []):
            if provider_name in self.providers:
                provider = self.providers[provider_name]
                if self._check_provider_health(provider):
                    return provider.provider_name, provider
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’è¿”ã™
        return self._get_default_provider()
    
    def _select_best_performance_provider(self) -> Tuple[str, LLMProvider]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è€ƒæ…®ã—ãŸãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠ"""
        # ã‚³ã‚¹ãƒˆã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è€ƒæ…®ã—ãŸé¸æŠãƒ­ã‚¸ãƒƒã‚¯
        provider_priority = ['openai', 'anthropic', 'google', 'gguf', 'ollama']
        
        for provider_name in provider_priority:
            if provider_name in self.providers:
                provider = self.providers[provider_name]
                if self._check_provider_health(provider):
                    return provider.provider_name, provider
        
        return self._get_default_provider()
    
    def _check_provider_health(self, provider: LLMProvider) -> bool:
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å¥åº·çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯"""
        try:
            # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if provider is None:
                return False
            
            # is_healthyå±æ€§ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
            if hasattr(provider, 'is_healthy'):
                return provider.is_healthy
            
            # is_availableãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
            if hasattr(provider, 'is_available'):
                return provider.is_available()
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯åˆ©ç”¨å¯èƒ½ã¨ã¿ãªã™
            return True
        except Exception as e:
            self.logger.warning(f"ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _get_default_provider(self) -> Tuple[str, LLMProvider]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å–å¾—"""
        # åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‹ã‚‰æœ€åˆã®ã‚‚ã®ã‚’è¿”ã™
        for provider_name, provider in self.providers.items():
            return provider_name, provider
        
        raise ValueError("åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒã‚ã‚Šã¾ã›ã‚“")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    async def main_demo():
        print("ğŸ¤– AIçµ±åˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« 2025 - Kobold.cpp ãƒ†ã‚¹ãƒˆ")
        
        test_df = pd.DataFrame({'A': [100, 200, 300], 'B': [150, 250, 350]})
        query = "ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ  df ã®Aåˆ—ã¨Båˆ—ã®çµ±è¨ˆè¦ç´„é‡ã‚’è¨ˆç®—ã—ã¦ãã ã•ã„ã€‚"
        
        print(f"ã‚¯ã‚¨ãƒª: {query}")
        
        if ai_config.is_api_configured("koboldcpp"):
            result = await ai_analyzer.analyze_with_custom_llm(
                query, test_df, provider="koboldcpp", model="local-model/gguf-model", enable_rag=False, enable_correction=False
            )
            
            if result["success"]:
                print("\nâœ… åˆ†ææˆåŠŸï¼")
                print(result.get("content"))
            else:
                print(f"\nâŒ åˆ†æå¤±æ•—: {result['error']}")
        else:
            print("Kobold.cppãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")

    asyncio.run(main_demo())
