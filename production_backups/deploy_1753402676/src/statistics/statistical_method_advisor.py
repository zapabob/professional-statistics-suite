#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Statistical Method Advisor System
çµ±è¨ˆæ‰‹æ³•æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ  - ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã«åŸºã¥ãæœ€é©ãªçµ±è¨ˆæ‰‹æ³•ã®æ¨å¥¨

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™:
- ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã®è‡ªå‹•åˆ†æ
- ç ”ç©¶è³ªå•ã«åŸºã¥ãçµ±è¨ˆæ‰‹æ³•æ¨å¥¨
- æ‰‹æ³•é©ç”¨å¯èƒ½æ€§ã®è©•ä¾¡
- ä»£æ›¿æ‰‹æ³•ã®ææ¡ˆ
"""

import json
import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from enum import Enum

# çµ±è¨ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from scipy.stats import normaltest, levene, shapiro

# è¨­å®šã¨ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
try:
    from config import check_feature_permission
    if not check_feature_permission('advanced_ai'):
        raise ImportError("Advanced AI features require Professional edition or higher")
except ImportError:
    def check_feature_permission(feature):
        return True

# ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
try:
    from data_preprocessing import validate_statistical_data, DataQualityReport
except ImportError:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè£…
    def validate_statistical_data(data):
        return {"quality_score": 0.8, "issues": [], "recommendations": []}
    
    class DataQualityReport:
        def __init__(self, data):
            self.quality_score = 0.8
            self.issues = []
            self.recommendations = []

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataType(Enum):
    """ãƒ‡ãƒ¼ã‚¿å‹ã®åˆ†é¡"""
    CONTINUOUS = "continuous"
    DISCRETE = "discrete"
    CATEGORICAL = "categorical"
    ORDINAL = "ordinal"
    BINARY = "binary"
    COUNT = "count"

class AnalysisGoal(Enum):
    """è§£æç›®æ¨™ã®åˆ†é¡"""
    DESCRIPTIVE = "descriptive"
    COMPARISON = "comparison"
    RELATIONSHIP = "relationship"
    PREDICTION = "prediction"
    CLASSIFICATION = "classification"
    CLUSTERING = "clustering"
    TIME_SERIES = "time_series"

class StatisticalMethod(Enum):
    """çµ±è¨ˆæ‰‹æ³•ã®åˆ†é¡"""
    # è¨˜è¿°çµ±è¨ˆ
    DESCRIPTIVE_STATS = "descriptive_statistics"
    
    # æ¯”è¼ƒæ¤œå®š
    T_TEST_ONE_SAMPLE = "t_test_one_sample"
    T_TEST_TWO_SAMPLE = "t_test_two_sample"
    T_TEST_PAIRED = "t_test_paired"
    MANN_WHITNEY = "mann_whitney"
    WILCOXON = "wilcoxon"
    
    # åˆ†æ•£åˆ†æ
    ANOVA_ONE_WAY = "anova_one_way"
    ANOVA_TWO_WAY = "anova_two_way"
    KRUSKAL_WALLIS = "kruskal_wallis"
    
    # ç›¸é–¢ãƒ»å›å¸°
    PEARSON_CORRELATION = "pearson_correlation"
    SPEARMAN_CORRELATION = "spearman_correlation"
    LINEAR_REGRESSION = "linear_regression"
    LOGISTIC_REGRESSION = "logistic_regression"
    
    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«åˆ†æ
    CHI_SQUARE = "chi_square"
    FISHER_EXACT = "fisher_exact"
    
    # æ©Ÿæ¢°å­¦ç¿’
    RANDOM_FOREST = "random_forest"
    SVM = "svm"
    NEURAL_NETWORK = "neural_network"

@dataclass
class DataCharacteristics:
    """ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã®æƒ…å ±"""
    sample_size: int
    variables: Dict[str, DataType]
    missing_values: Dict[str, float]
    outliers: Dict[str, int]
    normality: Dict[str, bool]
    homoscedasticity: Optional[bool] = None
    independence: bool = True
    multicollinearity: Optional[float] = None
    balance: Optional[Dict[str, float]] = None

@dataclass
class MethodRecommendation:
    """çµ±è¨ˆæ‰‹æ³•æ¨å¥¨ã®çµæœ"""
    method: StatisticalMethod
    confidence: float
    rationale: str
    assumptions_met: Dict[str, bool]
    alternatives: List[StatisticalMethod] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    sample_size_adequate: bool = True
    effect_size_detectable: Optional[float] = None

class StatisticalMethodAdvisor:
    """çµ±è¨ˆæ‰‹æ³•æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ ã®ã‚³ã‚¢ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self):
        self.logger = logging.getLogger(f"{__name__}.StatisticalMethodAdvisor")
        
        # æ‰‹æ³•ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–
        self.method_database = self._initialize_method_database()
        
        # æ¨å¥¨ãƒ«ãƒ¼ãƒ«ã®åˆæœŸåŒ–
        self.recommendation_rules = self._initialize_recommendation_rules()
        
        self.logger.info("StatisticalMethodAdvisoråˆæœŸåŒ–å®Œäº†")
    
    def _initialize_method_database(self) -> Dict[StatisticalMethod, Dict[str, Any]]:
        """çµ±è¨ˆæ‰‹æ³•ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–"""
        return {
            StatisticalMethod.T_TEST_ONE_SAMPLE: {
                "name": "ä¸€æ¨™æœ¬tæ¤œå®š",
                "description": "æ¯å¹³å‡ãŒç‰¹å®šã®å€¤ã¨ç­‰ã—ã„ã‹ã‚’æ¤œå®š",
                "assumptions": ["normality", "independence"],
                "data_types": [DataType.CONTINUOUS],
                "min_sample_size": 30,
                "analysis_goals": [AnalysisGoal.COMPARISON],
                "power_calculation": True
            },
            StatisticalMethod.T_TEST_TWO_SAMPLE: {
                "name": "äºŒæ¨™æœ¬tæ¤œå®š",
                "description": "2ã¤ã®ç¾¤ã®å¹³å‡å€¤ã‚’æ¯”è¼ƒ",
                "assumptions": ["normality", "independence", "homoscedasticity"],
                "data_types": [DataType.CONTINUOUS],
                "min_sample_size": 30,
                "analysis_goals": [AnalysisGoal.COMPARISON],
                "power_calculation": True
            },
            StatisticalMethod.MANN_WHITNEY: {
                "name": "Mann-Whitney Uæ¤œå®š",
                "description": "2ã¤ã®ç¾¤ã®åˆ†å¸ƒã‚’æ¯”è¼ƒï¼ˆãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ï¼‰",
                "assumptions": ["independence"],
                "data_types": [DataType.CONTINUOUS, DataType.ORDINAL],
                "min_sample_size": 10,
                "analysis_goals": [AnalysisGoal.COMPARISON],
                "power_calculation": False
            },
            StatisticalMethod.ANOVA_ONE_WAY: {
                "name": "ä¸€å…ƒé…ç½®åˆ†æ•£åˆ†æ",
                "description": "3ã¤ä»¥ä¸Šã®ç¾¤ã®å¹³å‡å€¤ã‚’æ¯”è¼ƒ",
                "assumptions": ["normality", "independence", "homoscedasticity"],
                "data_types": [DataType.CONTINUOUS],
                "min_sample_size": 30,
                "analysis_goals": [AnalysisGoal.COMPARISON],
                "power_calculation": True
            },
            StatisticalMethod.KRUSKAL_WALLIS: {
                "name": "Kruskal-Wallisæ¤œå®š",
                "description": "3ã¤ä»¥ä¸Šã®ç¾¤ã®åˆ†å¸ƒã‚’æ¯”è¼ƒï¼ˆãƒãƒ³ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ï¼‰",
                "assumptions": ["independence"],
                "data_types": [DataType.CONTINUOUS, DataType.ORDINAL],
                "min_sample_size": 15,
                "analysis_goals": [AnalysisGoal.COMPARISON],
                "power_calculation": False
            },
            StatisticalMethod.PEARSON_CORRELATION: {
                "name": "Pearsonç›¸é–¢ä¿‚æ•°",
                "description": "2ã¤ã®é€£ç¶šå¤‰æ•°é–“ã®ç·šå½¢é–¢ä¿‚ã‚’æ¸¬å®š",
                "assumptions": ["normality", "linearity", "independence"],
                "data_types": [DataType.CONTINUOUS],
                "min_sample_size": 30,
                "analysis_goals": [AnalysisGoal.RELATIONSHIP],
                "power_calculation": True
            },
            StatisticalMethod.SPEARMAN_CORRELATION: {
                "name": "Spearmané †ä½ç›¸é–¢ä¿‚æ•°",
                "description": "2ã¤ã®å¤‰æ•°é–“ã®å˜èª¿é–¢ä¿‚ã‚’æ¸¬å®š",
                "assumptions": ["independence"],
                "data_types": [DataType.CONTINUOUS, DataType.ORDINAL],
                "min_sample_size": 20,
                "analysis_goals": [AnalysisGoal.RELATIONSHIP],
                "power_calculation": False
            },
            StatisticalMethod.LINEAR_REGRESSION: {
                "name": "ç·šå½¢å›å¸°åˆ†æ",
                "description": "é€£ç¶šå¤‰æ•°ã®äºˆæ¸¬ã¨é–¢ä¿‚æ€§ã®åˆ†æ",
                "assumptions": ["linearity", "independence", "homoscedasticity", "normality_residuals"],
                "data_types": [DataType.CONTINUOUS],
                "min_sample_size": 50,
                "analysis_goals": [AnalysisGoal.PREDICTION, AnalysisGoal.RELATIONSHIP],
                "power_calculation": True
            },
            StatisticalMethod.LOGISTIC_REGRESSION: {
                "name": "ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°åˆ†æ",
                "description": "äºŒå€¤çµæœã®äºˆæ¸¬ã¨é–¢ä¿‚æ€§ã®åˆ†æ",
                "assumptions": ["independence", "linearity_logit"],
                "data_types": [DataType.BINARY],
                "min_sample_size": 100,
                "analysis_goals": [AnalysisGoal.PREDICTION, AnalysisGoal.CLASSIFICATION],
                "power_calculation": True
            },
            StatisticalMethod.CHI_SQUARE: {
                "name": "ã‚«ã‚¤äºŒä¹—æ¤œå®š",
                "description": "ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°é–“ã®é–¢é€£æ€§ã‚’æ¤œå®š",
                "assumptions": ["independence", "expected_frequency"],
                "data_types": [DataType.CATEGORICAL],
                "min_sample_size": 50,
                "analysis_goals": [AnalysisGoal.RELATIONSHIP],
                "power_calculation": True
            }
        }
    
    def _initialize_recommendation_rules(self) -> Dict[str, Any]:
        """æ¨å¥¨ãƒ«ãƒ¼ãƒ«ã®åˆæœŸåŒ–"""
        return {
            "sample_size_thresholds": {
                "small": 30,
                "medium": 100,
                "large": 500
            },
            "normality_threshold": 0.05,
            "homoscedasticity_threshold": 0.05,
            "multicollinearity_threshold": 0.8,
            "missing_data_threshold": 0.1,
            "outlier_threshold": 0.05
        }
    
    def analyze_data_characteristics(self, data: pd.DataFrame, 
                                   target_variable: Optional[str] = None) -> DataCharacteristics:
        """ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã®åˆ†æ"""
        self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§åˆ†æé–‹å§‹: {data.shape}")
        
        sample_size = len(data)
        variables = {}
        missing_values = {}
        outliers = {}
        normality = {}
        
        # å„å¤‰æ•°ã®åˆ†æ
        for column in data.columns:
            # ãƒ‡ãƒ¼ã‚¿å‹ã®åˆ¤å®š
            variables[column] = self._determine_data_type(data[column])
            
            # æ¬ æå€¤ã®å‰²åˆ
            missing_values[column] = data[column].isnull().sum() / len(data)
            
            # å¤–ã‚Œå€¤ã®æ¤œå‡º
            if variables[column] in [DataType.CONTINUOUS, DataType.DISCRETE]:
                outliers[column] = self._detect_outliers(data[column])
                
                # æ­£è¦æ€§ã®æ¤œå®š
                if len(data[column].dropna()) >= 8:  # Shapiro-Wilkã®æœ€å°ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
                    normality[column] = self._test_normality(data[column])
                else:
                    normality[column] = False
            else:
                outliers[column] = 0
                normality[column] = False
        
        # ç­‰åˆ†æ•£æ€§ã®æ¤œå®šï¼ˆé€£ç¶šå¤‰æ•°ãŒè¤‡æ•°ã‚ã‚‹å ´åˆï¼‰
        continuous_vars = [col for col, dtype in variables.items() 
                          if dtype == DataType.CONTINUOUS]
        homoscedasticity = None
        if len(continuous_vars) >= 2:
            homoscedasticity = self._test_homoscedasticity(data[continuous_vars])
        
        # å¤šé‡å…±ç·šæ€§ã®æ¤œå‡º
        multicollinearity = None
        if len(continuous_vars) >= 2:
            multicollinearity = self._detect_multicollinearity(data[continuous_vars])
        
        # ã‚¯ãƒ©ã‚¹ãƒãƒ©ãƒ³ã‚¹ã®åˆ†æï¼ˆç›®çš„å¤‰æ•°ãŒã‚ã‚‹å ´åˆï¼‰
        balance = None
        if target_variable and target_variable in data.columns:
            if variables[target_variable] in [DataType.CATEGORICAL, DataType.BINARY]:
                balance = self._analyze_class_balance(data[target_variable])
        
        characteristics = DataCharacteristics(
            sample_size=sample_size,
            variables=variables,
            missing_values=missing_values,
            outliers=outliers,
            normality=normality,
            homoscedasticity=homoscedasticity,
            multicollinearity=multicollinearity,
            balance=balance
        )
        
        self.logger.info("ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§åˆ†æå®Œäº†")
        return characteristics
    
    def _determine_data_type(self, series: pd.Series) -> DataType:
        """ãƒ‡ãƒ¼ã‚¿å‹ã®è‡ªå‹•åˆ¤å®š"""
        # æ¬ æå€¤ã‚’é™¤å¤–
        clean_series = series.dropna()
        
        if len(clean_series) == 0:
            return DataType.CATEGORICAL
        
        # æ•°å€¤å‹ã®å ´åˆ
        if pd.api.types.is_numeric_dtype(clean_series):
            unique_values = clean_series.nunique()
            total_values = len(clean_series)
            
            # äºŒå€¤å¤‰æ•°
            if unique_values == 2:
                return DataType.BINARY
            
            # ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆéè² æ•´æ•°ï¼‰
            if clean_series.dtype in ['int64', 'int32'] and (clean_series >= 0).all():
                if unique_values < 20:  # é›¢æ•£çš„
                    return DataType.COUNT
            
            # é€£ç¶šå¤‰æ•° vs é›¢æ•£å¤‰æ•°ã®åˆ¤å®šã‚’æ”¹å–„
            unique_ratio = unique_values / total_values
            
            # é›¢æ•£ãƒ‡ãƒ¼ã‚¿ã®æ¡ä»¶ã‚’å³ã—ãã™ã‚‹
            if unique_values <= 10 or unique_ratio < 0.02:
                return DataType.DISCRETE
            else:
                return DataType.CONTINUOUS
        
        # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å¤‰æ•°
        else:
            unique_values = clean_series.nunique()
            
            # äºŒå€¤å¤‰æ•°
            if unique_values == 2:
                return DataType.BINARY
            
            # é †åºãŒã‚ã‚‹ã‹ã®åˆ¤å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
            if clean_series.dtype.name == 'category' and clean_series.cat.ordered:
                return DataType.ORDINAL
            
            return DataType.CATEGORICAL
    
    def _detect_outliers(self, series: pd.Series) -> int:
        """å¤–ã‚Œå€¤ã®æ¤œå‡ºï¼ˆIQRæ³•ï¼‰"""
        clean_series = series.dropna()
        if len(clean_series) < 4:
            return 0
        
        Q1 = clean_series.quantile(0.25)
        Q3 = clean_series.quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        outliers = ((clean_series < lower_bound) | (clean_series > upper_bound)).sum()
        return outliers
    
    def _test_normality(self, series: pd.Series) -> bool:
        """æ­£è¦æ€§ã®æ¤œå®š"""
        clean_series = series.dropna()
        if len(clean_series) < 8:
            return False
        
        try:
            # å®šæ•°å€¤ã®å ´åˆã¯æ­£è¦æ€§ãªã—ã¨ã™ã‚‹
            if clean_series.nunique() <= 1:
                return False
            
            if len(clean_series) <= 5000:
                # Shapiro-Wilkæ¤œå®šï¼ˆå°ã‚µãƒ³ãƒ—ãƒ«ï¼‰
                statistic, p_value = shapiro(clean_series)
            else:
                # D'Agostino-Pearsonæ¤œå®šï¼ˆå¤§ã‚µãƒ³ãƒ—ãƒ«ï¼‰
                statistic, p_value = normaltest(clean_series)
            
            # på€¤ãŒæœ‰åŠ¹ãªå€¤ã‹ãƒã‚§ãƒƒã‚¯
            if np.isnan(p_value) or np.isinf(p_value):
                return False
                
            return p_value > self.recommendation_rules["normality_threshold"]
        except Exception as e:
            self.logger.debug(f"æ­£è¦æ€§æ¤œå®šã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _test_homoscedasticity(self, data: pd.DataFrame) -> bool:
        """ç­‰åˆ†æ•£æ€§ã®æ¤œå®š"""
        try:
            # Leveneæ¤œå®š
            groups = [data[col].dropna() for col in data.columns]
            statistic, p_value = levene(*groups)
            return p_value > self.recommendation_rules["homoscedasticity_threshold"]
        except:
            return False
    
    def _detect_multicollinearity(self, data: pd.DataFrame) -> float:
        """å¤šé‡å…±ç·šæ€§ã®æ¤œå‡ºï¼ˆç›¸é–¢è¡Œåˆ—ã®æœ€å¤§å€¤ï¼‰"""
        try:
            corr_matrix = data.corr().abs()
            # å¯¾è§’æˆåˆ†ã‚’é™¤å¤–
            np.fill_diagonal(corr_matrix.values, 0)
            return corr_matrix.max().max()
        except:
            return 0.0
    
    def _analyze_class_balance(self, series: pd.Series) -> Dict[str, float]:
        """ã‚¯ãƒ©ã‚¹ãƒãƒ©ãƒ³ã‚¹ã®åˆ†æ"""
        value_counts = series.value_counts(normalize=True)
        return value_counts.to_dict()
    
    def recommend_methods(self, characteristics: DataCharacteristics,
                         analysis_goal: AnalysisGoal,
                         target_variable: Optional[str] = None,
                         predictor_variables: Optional[List[str]] = None) -> List[MethodRecommendation]:
        """çµ±è¨ˆæ‰‹æ³•ã®æ¨å¥¨"""
        self.logger.info(f"çµ±è¨ˆæ‰‹æ³•æ¨å¥¨é–‹å§‹: {analysis_goal}")
        
        recommendations = []
        
        # å„çµ±è¨ˆæ‰‹æ³•ã«ã¤ã„ã¦é©ç”¨å¯èƒ½æ€§ã‚’è©•ä¾¡
        for method, method_info in self.method_database.items():
            if analysis_goal not in method_info["analysis_goals"]:
                continue
            
            recommendation = self._evaluate_method(
                method, method_info, characteristics, 
                target_variable, predictor_variables
            )
            
            if recommendation.confidence > 0.1:  # æœ€ä½ä¿¡é ¼åº¦
                recommendations.append(recommendation)
        
        # ä¿¡é ¼åº¦ã§ã‚½ãƒ¼ãƒˆ
        recommendations.sort(key=lambda x: x.confidence, reverse=True)
        
        self.logger.info(f"æ¨å¥¨æ‰‹æ³•æ•°: {len(recommendations)}")
        return recommendations
    
    def _evaluate_method(self, method: StatisticalMethod, method_info: Dict[str, Any],
                        characteristics: DataCharacteristics,
                        target_variable: Optional[str] = None,
                        predictor_variables: Optional[List[str]] = None) -> MethodRecommendation:
        """å€‹åˆ¥æ‰‹æ³•ã®è©•ä¾¡"""
        confidence = 1.0
        assumptions_met = {}
        warnings = []
        alternatives = []
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã®ç¢ºèª
        sample_size_adequate = characteristics.sample_size >= method_info["min_sample_size"]
        if not sample_size_adequate:
            confidence *= 0.5
            warnings.append(f"ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒä¸è¶³ï¼ˆå¿…è¦: {method_info['min_sample_size']}, ç¾åœ¨: {characteristics.sample_size}ï¼‰")
        
        # ãƒ‡ãƒ¼ã‚¿å‹ã®ç¢ºèª
        if target_variable:
            target_type = characteristics.variables.get(target_variable)
            if target_type not in method_info["data_types"]:
                confidence *= 0.3
                warnings.append(f"ç›®çš„å¤‰æ•°ã®ãƒ‡ãƒ¼ã‚¿å‹ãŒä¸é©åˆ‡ï¼ˆ{target_type}ï¼‰")
        
        # ä»®å®šã®ç¢ºèª
        for assumption in method_info["assumptions"]:
            met = self._check_assumption(assumption, characteristics, target_variable, predictor_variables)
            assumptions_met[assumption] = met
            
            if not met:
                confidence *= 0.7
                warnings.append(f"ä»®å®šã€Œ{assumption}ã€ãŒæº€ãŸã•ã‚Œã¦ã„ã¾ã›ã‚“")
                
                # ä»£æ›¿æ‰‹æ³•ã®ææ¡ˆ
                alternatives.extend(self._suggest_alternatives(method, assumption))
        
        # æ¬ æå€¤ã®å½±éŸ¿
        if target_variable and characteristics.missing_values.get(target_variable, 0) > self.recommendation_rules["missing_data_threshold"]:
            confidence *= 0.8
            warnings.append("ç›®çš„å¤‰æ•°ã«å¤šãã®æ¬ æå€¤ãŒã‚ã‚Šã¾ã™")
        
        # å¤–ã‚Œå€¤ã®å½±éŸ¿
        if target_variable and characteristics.outliers.get(target_variable, 0) > characteristics.sample_size * self.recommendation_rules["outlier_threshold"]:
            confidence *= 0.9
            warnings.append("å¤šãã®å¤–ã‚Œå€¤ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
        
        # æ ¹æ‹ ã®ç”Ÿæˆ
        rationale = self._generate_rationale(method, method_info, characteristics, assumptions_met)
        
        return MethodRecommendation(
            method=method,
            confidence=confidence,
            rationale=rationale,
            assumptions_met=assumptions_met,
            alternatives=list(set(alternatives)),
            warnings=warnings,
            sample_size_adequate=sample_size_adequate
        )
    
    def _check_assumption(self, assumption: str, characteristics: DataCharacteristics,
                         target_variable: Optional[str] = None,
                         predictor_variables: Optional[List[str]] = None) -> bool:
        """çµ±è¨ˆçš„ä»®å®šã®ç¢ºèª"""
        if assumption == "normality":
            if target_variable:
                return characteristics.normality.get(target_variable, False)
            return any(characteristics.normality.values())
        
        elif assumption == "independence":
            return characteristics.independence
        
        elif assumption == "homoscedasticity":
            return characteristics.homoscedasticity if characteristics.homoscedasticity is not None else True
        
        elif assumption == "linearity":
            # ç°¡æ˜“çš„ãªç·šå½¢æ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ç›¸é–¢ä¿‚æ•°ãªã©ã‚’ä½¿ç”¨ï¼‰
            return True
        
        elif assumption == "expected_frequency":
            # ã‚«ã‚¤äºŒä¹—æ¤œå®šã®æœŸå¾…åº¦æ•°ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
            return characteristics.sample_size >= 50
        
        else:
            return True
    
    def _suggest_alternatives(self, method: StatisticalMethod, violated_assumption: str) -> List[StatisticalMethod]:
        """ä»®å®šé•åæ™‚ã®ä»£æ›¿æ‰‹æ³•ææ¡ˆ"""
        alternatives = []
        
        if violated_assumption == "normality":
            if method == StatisticalMethod.T_TEST_TWO_SAMPLE:
                alternatives.append(StatisticalMethod.MANN_WHITNEY)
            elif method == StatisticalMethod.ANOVA_ONE_WAY:
                alternatives.append(StatisticalMethod.KRUSKAL_WALLIS)
            elif method == StatisticalMethod.PEARSON_CORRELATION:
                alternatives.append(StatisticalMethod.SPEARMAN_CORRELATION)
        
        elif violated_assumption == "homoscedasticity":
            if method == StatisticalMethod.T_TEST_TWO_SAMPLE:
                alternatives.append(StatisticalMethod.MANN_WHITNEY)
            elif method == StatisticalMethod.ANOVA_ONE_WAY:
                alternatives.append(StatisticalMethod.KRUSKAL_WALLIS)
        
        return alternatives
    
    def _generate_rationale(self, method: StatisticalMethod, method_info: Dict[str, Any],
                           characteristics: DataCharacteristics, assumptions_met: Dict[str, bool]) -> str:
        """æ¨å¥¨ç†ç”±ã®ç”Ÿæˆ"""
        rationale_parts = []
        
        # åŸºæœ¬çš„ãªé©ç”¨ç†ç”±
        rationale_parts.append(f"{method_info['name']}ã¯{method_info['description']}ã«é©ã—ã¦ã„ã¾ã™ã€‚")
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
        if characteristics.sample_size >= method_info["min_sample_size"]:
            rationale_parts.append(f"ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºï¼ˆ{characteristics.sample_size}ï¼‰ã¯ååˆ†ã§ã™ã€‚")
        
        # æº€ãŸã•ã‚ŒãŸä»®å®š
        met_assumptions = [assumption for assumption, met in assumptions_met.items() if met]
        if met_assumptions:
            rationale_parts.append(f"å¿…è¦ãªä»®å®šï¼ˆ{', '.join(met_assumptions)}ï¼‰ãŒæº€ãŸã•ã‚Œã¦ã„ã¾ã™ã€‚")
        
        # é•åã—ãŸä»®å®š
        violated_assumptions = [assumption for assumption, met in assumptions_met.items() if not met]
        if violated_assumptions:
            rationale_parts.append(f"ãŸã ã—ã€ä»®å®šï¼ˆ{', '.join(violated_assumptions)}ï¼‰ã«æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚")
        
        return " ".join(rationale_parts)
    
    def get_method_details(self, method: StatisticalMethod) -> Dict[str, Any]:
        """çµ±è¨ˆæ‰‹æ³•ã®è©³ç´°æƒ…å ±ã‚’å–å¾—"""
        return self.method_database.get(method, {})
    
    def export_recommendations(self, recommendations: List[MethodRecommendation], 
                             filepath: str) -> bool:
        """æ¨å¥¨çµæœã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        try:
            export_data = []
            for rec in recommendations:
                # NumPyå‹ã‚’Pythonæ¨™æº–å‹ã«å¤‰æ›
                assumptions_met_converted = {}
                for key, value in rec.assumptions_met.items():
                    if isinstance(value, np.bool_):
                        assumptions_met_converted[key] = bool(value)
                    else:
                        assumptions_met_converted[key] = value
                
                export_data.append({
                    "method": rec.method.value,
                    "confidence": float(rec.confidence),
                    "rationale": rec.rationale,
                    "assumptions_met": assumptions_met_converted,
                    "alternatives": [alt.value for alt in rec.alternatives],
                    "warnings": rec.warnings,
                    "sample_size_adequate": bool(rec.sample_size_adequate)
                })
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"æ¨å¥¨çµæœã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ: {filepath}")
            return True
        except Exception as e:
            self.logger.error(f"ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False

def create_sample_data_for_testing() -> pd.DataFrame:
    """ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ"""
    np.random.seed(42)
    
    n = 200
    data = {
        'age': np.random.normal(35, 10, n),
        'income': np.random.lognormal(10, 0.5, n),
        'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n, p=[0.3, 0.4, 0.2, 0.1]),
        'satisfaction': np.random.randint(1, 6, n),
        'gender': np.random.choice(['Male', 'Female'], n),
        'purchased': np.random.choice([0, 1], n, p=[0.6, 0.4])
    }
    
    return pd.DataFrame(data)

if __name__ == '__main__':
    # ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    print("ğŸ§  Statistical Method Advisor ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("=" * 60)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
    print("\nğŸ“Š ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ...")
    sample_data = create_sample_data_for_testing()
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {sample_data.shape}")
    print(f"å¤‰æ•°: {list(sample_data.columns)}")
    
    # ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
    print("\nğŸ”§ StatisticalMethodAdvisoråˆæœŸåŒ–...")
    advisor = StatisticalMethodAdvisor()
    
    # ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§ã®åˆ†æ
    print("\nğŸ” ãƒ‡ãƒ¼ã‚¿ç‰¹æ€§åˆ†æ...")
    characteristics = advisor.analyze_data_characteristics(sample_data, target_variable='purchased')
    
    print(f"ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {characteristics.sample_size}")
    print(f"å¤‰æ•°å‹: {characteristics.variables}")
    print(f"æ­£è¦æ€§: {characteristics.normality}")
    
    # çµ±è¨ˆæ‰‹æ³•ã®æ¨å¥¨
    print("\nğŸ’¡ çµ±è¨ˆæ‰‹æ³•æ¨å¥¨...")
    
    # æ¯”è¼ƒåˆ†æã®ä¾‹
    comparison_recs = advisor.recommend_methods(
        characteristics, 
        AnalysisGoal.COMPARISON,
        target_variable='income'
    )
    
    print("\nğŸ” æ¯”è¼ƒåˆ†æã®æ¨å¥¨æ‰‹æ³•:")
    for i, rec in enumerate(comparison_recs[:3], 1):
        print(f"{i}. {rec.method.value} (ä¿¡é ¼åº¦: {rec.confidence:.2f})")
        print(f"   ç†ç”±: {rec.rationale}")
        if rec.warnings:
            print(f"   è­¦å‘Š: {', '.join(rec.warnings)}")
    
    # é–¢ä¿‚æ€§åˆ†æã®ä¾‹
    relationship_recs = advisor.recommend_methods(
        characteristics,
        AnalysisGoal.RELATIONSHIP,
        target_variable='satisfaction'
    )
    
    print("\nğŸ”— é–¢ä¿‚æ€§åˆ†æã®æ¨å¥¨æ‰‹æ³•:")
    for i, rec in enumerate(relationship_recs[:3], 1):
        print(f"{i}. {rec.method.value} (ä¿¡é ¼åº¦: {rec.confidence:.2f})")
        print(f"   ç†ç”±: {rec.rationale}")
    
    print("\nâœ… ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†ï¼")